#!/usr/bin/env python3
"""
Stage 5 Data Integration - Integration Tests

æ¸¬è©¦Stage 5 v2.0æ¨¡çµ„åŒ–æ¶æ§‹çš„å®Œæ•´é›†æˆåŠŸèƒ½ï¼š
1. DataIntegrationProcessorä¸»è™•ç†å™¨
2. TimeseriesConverteræ™‚é–“åºåˆ—è½‰æ›
3. AnimationBuilderå‹•ç•«æ•¸æ“šå»ºæ§‹
4. LayeredDataGeneratoråˆ†å±¤æ•¸æ“šç”Ÿæˆ
5. FormatConverterHubå¤šæ ¼å¼è½‰æ›

âš¡ Grade Aæ¸¬è©¦æ¨™æº–ï¼š
- å®Œæ•´çš„åŠŸèƒ½æ¸¬è©¦è¦†è“‹
- çœŸå¯¦æ•¸æ“šæ¨¡æ“¬æ¸¬è©¦
- æ€§èƒ½åŸºæº–æ¸¬è©¦
- éŒ¯èª¤è™•ç†æ¸¬è©¦
"""

import unittest
import json
import time
from datetime import datetime, timezone, timedelta
from typing import Dict, Any, List

# Add project root to path
import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '../../'))

# Stage 5æ¨¡çµ„åŒ–çµ„ä»¶
from src.stages.stage5_data_integration.data_integration_processor import DataIntegrationProcessor
from src.stages.stage5_data_integration.timeseries_converter import TimeseriesConverter
from src.stages.stage5_data_integration.animation_builder import AnimationBuilder
from src.stages.stage5_data_integration.layered_data_generator import LayeredDataGenerator
from src.stages.stage5_data_integration.format_converter_hub import FormatConverterHub

class TestStage5DataIntegration(unittest.TestCase):
    """Stage 5æ•¸æ“šæ•´åˆå®Œæ•´æ¸¬è©¦å¥—ä»¶"""

    def setUp(self):
        """æ¸¬è©¦è¨­ç½®"""
        # v2.0æ¸¬è©¦é…ç½®
        self.test_config = {
            'timeseries': {
                'sampling_frequency': '10S',
                'interpolation_method': 'cubic_spline',
                'compression_enabled': True,
                'compression_level': 6
            },
            'animation': {
                'frame_rate': 30,
                'duration_seconds': 300,
                'keyframe_optimization': True,
                'effect_quality': 'high'
            },
            'layers': {
                'spatial_resolution_levels': 5,
                'temporal_granularity': ['1MIN', '10MIN', '1HOUR'],
                'quality_tiers': ['high', 'medium', 'low'],
                'enable_spatial_indexing': True
            },
            'formats': {
                'output_formats': ['json', 'geojson', 'csv', 'api_package'],
                'schema_version': '2.0',
                'api_version': 'v2',
                'compression_enabled': True
            }
        }

        # å‰µå»ºæ¸¬è©¦ç”¨çš„Stage 4è¼¸å‡ºæ•¸æ“š
        self.test_stage4_data = self._create_test_stage4_data()

        # åˆå§‹åŒ–Stage 5è™•ç†å™¨
        self.processor = DataIntegrationProcessor(self.test_config)

    def _create_test_stage4_data(self) -> Dict[str, Any]:
        """å‰µå»ºæ¸¬è©¦ç”¨çš„Stage 4è¼¸å‡ºæ•¸æ“š"""
        base_time = datetime.now(timezone.utc)

        # æ¨¡æ“¬å„ªåŒ–æ± æ•¸æ“š
        optimal_pool = {
            'satellites': [
                {
                    'satellite_id': 'STARLINK-1001',
                    'name': 'Starlink-1001',
                    'constellation': 'Starlink',
                    'stage1_orbital': {
                        'position_velocity': {
                            'position': {'x': 6678.0, 'y': 0.0, 'z': 0.0, 'latitude': 0.0, 'longitude': 0.0, 'altitude': 550.0},
                            'velocity': {'vx': 0.0, 'vy': 7.66, 'vz': 0.0}
                        }
                    },
                    'position_timeseries': [
                        {'x_km': 6678.0, 'y_km': 0.0, 'z_km': 0.0, 'latitude_deg': 0.0, 'longitude_deg': 0.0, 'altitude_km': 550.0}
                    ],
                    'rsrp': -85.0,
                    'snr': 12.5
                },
                {
                    'satellite_id': 'ONEWEB-2001',
                    'name': 'OneWeb-2001',
                    'constellation': 'OneWeb',
                    'stage1_orbital': {
                        'position_velocity': {
                            'position': {'x': 7178.0, 'y': 0.0, 'z': 0.0, 'latitude': 45.0, 'longitude': 90.0, 'altitude': 1200.0},
                            'velocity': {'vx': 0.0, 'vy': 7.35, 'vz': 0.0}
                        }
                    },
                    'position_timeseries': [
                        {'x_km': 7178.0, 'y_km': 0.0, 'z_km': 0.0, 'latitude_deg': 45.0, 'longitude_deg': 90.0, 'altitude_km': 1200.0}
                    ],
                    'rsrp': -90.0,
                    'snr': 10.0
                },
                {
                    'satellite_id': 'KUIPER-3001',
                    'name': 'Kuiper-3001',
                    'constellation': 'Kuiper',
                    'stage1_orbital': {
                        'position_velocity': {
                            'position': {'x': 6928.0, 'y': 0.0, 'z': 0.0, 'latitude': -30.0, 'longitude': 180.0, 'altitude': 630.0},
                            'velocity': {'vx': 0.0, 'vy': 7.55, 'vz': 0.0}
                        }
                    },
                    'position_timeseries': [
                        {'x_km': 6928.0, 'y_km': 0.0, 'z_km': 0.0, 'latitude_deg': -30.0, 'longitude_deg': 180.0, 'altitude_km': 630.0}
                    ],
                    'rsrp': -75.0,
                    'snr': 15.0
                }
            ],
            'metadata': {
                'processing_timestamp': base_time.isoformat(),
                'start_time': base_time.isoformat(),
                'end_time': (base_time + timedelta(minutes=5)).isoformat(),
                'total_satellites': 3
            }
        }

        # æ¨¡æ“¬å„ªåŒ–çµæœ
        optimization_results = {
            'optimization_summary': {
                'satellites_optimized': 3,
                'optimization_method': 'genetic_algorithm',
                'performance_improvement': 0.85
            }
        }

        return {
            'optimal_pool': optimal_pool,
            'optimization_results': optimization_results,
            'metadata': {
                'processing_timestamp': base_time.isoformat(),
                'stage': 'stage4_optimization',
                'processed_satellites': 3
            }
        }

    def test_complete_data_integration_pipeline(self):
        """æ¸¬è©¦å®Œæ•´çš„æ•¸æ“šæ•´åˆæµæ°´ç·š"""
        print("\nğŸš€ æ¸¬è©¦å®Œæ•´çš„æ•¸æ“šæ•´åˆæµæ°´ç·š...")

        start_time = time.time()

        # åŸ·è¡Œå®Œæ•´çš„Stage 5è™•ç†
        result = self.processor.process(self.test_stage4_data)

        processing_time = time.time() - start_time

        # é©—è­‰åŸºæœ¬çµæ§‹
        self.assertIn('stage', result)
        self.assertEqual(result['stage'], 'stage5_data_integration')

        # é©—è­‰æ•¸æ“šçµ„ä»¶
        self.assertIn('timeseries_data', result)
        self.assertIn('animation_data', result)
        self.assertIn('hierarchical_data', result)
        self.assertIn('formatted_outputs', result)
        self.assertIn('metadata', result)

        # é©—è­‰æ€§èƒ½ç›®æ¨™
        self.assertLess(processing_time, 60.0, "è™•ç†æ™‚é–“æ‡‰å°‘æ–¼60ç§’")

        # é©—è­‰å£“ç¸®æ¯”
        metadata = result.get('metadata', {})
        compression_ratio = metadata.get('compression_ratio', 0.0)
        self.assertGreater(compression_ratio, 0.5, "å£“ç¸®æ¯”æ‡‰å¤§æ–¼50%")

        print(f"âœ… å®Œæ•´æµæ°´ç·šæ¸¬è©¦é€šé ({processing_time:.2f}ç§’)")

    def test_timeseries_converter_functionality(self):
        """æ¸¬è©¦æ™‚é–“åºåˆ—è½‰æ›å™¨åŠŸèƒ½"""
        print("\nâ° æ¸¬è©¦æ™‚é–“åºåˆ—è½‰æ›å™¨...")

        converter = TimeseriesConverter(self.test_config['timeseries'])
        optimal_pool = self.test_stage4_data['optimal_pool']

        # æ¸¬è©¦åŸºæœ¬è½‰æ›
        timeseries_data = converter.convert_to_timeseries(optimal_pool)

        self.assertIn('dataset_id', timeseries_data)
        self.assertIn('satellite_count', timeseries_data)
        self.assertIn('time_index', timeseries_data)
        self.assertIn('satellite_timeseries', timeseries_data)

        # é©—è­‰è¡›æ˜Ÿæ•¸é‡
        self.assertEqual(timeseries_data['satellite_count'], 3)

        # æ¸¬è©¦æ™‚é–“çª—å£ç”Ÿæˆ
        window_data = converter.generate_time_windows(timeseries_data, 600)
        self.assertIn('windows', window_data)
        self.assertIn('total_windows', window_data)

        # æ¸¬è©¦æ’å€¼
        interpolated_data = converter.interpolate_missing_data(timeseries_data)
        self.assertIn('metadata', interpolated_data)
        self.assertTrue(interpolated_data['metadata'].get('interpolation_applied', False))

        # æ¸¬è©¦å£“ç¸®
        compressed_data = converter.compress_timeseries(interpolated_data)
        self.assertIsInstance(compressed_data, bytes)

        # æ¸¬è©¦è§£å£“ç¸®
        decompressed_data = converter.decompress_timeseries(compressed_data)
        self.assertIn('dataset_id', decompressed_data)

        print("âœ… æ™‚é–“åºåˆ—è½‰æ›å™¨æ¸¬è©¦é€šé")

    def test_animation_builder_functionality(self):
        """æ¸¬è©¦å‹•ç•«å»ºæ§‹å™¨åŠŸèƒ½"""
        print("\nğŸ¬ æ¸¬è©¦å‹•ç•«å»ºæ§‹å™¨...")

        # å…ˆç”Ÿæˆæ™‚é–“åºåˆ—æ•¸æ“š
        converter = TimeseriesConverter(self.test_config['timeseries'])
        timeseries_data = converter.convert_to_timeseries(self.test_stage4_data['optimal_pool'])

        # æ¸¬è©¦å‹•ç•«å»ºæ§‹
        builder = AnimationBuilder(self.test_config['animation'])
        animation_data = builder.build_satellite_animation(timeseries_data)

        self.assertIn('animation_id', animation_data)
        self.assertIn('duration', animation_data)
        self.assertIn('frame_rate', animation_data)
        self.assertIn('satellite_trajectories', animation_data)
        self.assertIn('coverage_animation', animation_data)

        # é©—è­‰å‹•ç•«åƒæ•¸
        self.assertEqual(animation_data['frame_rate'], 30)
        self.assertEqual(animation_data['duration'], 300)

        # æ¸¬è©¦è»Œè·¡é—œéµå¹€ç”Ÿæˆ
        satellite_timeseries = timeseries_data.get('satellite_timeseries', {})
        if satellite_timeseries:
            sat_id = list(satellite_timeseries.keys())[0]
            sat_data = satellite_timeseries[sat_id]

            trajectory = builder.generate_trajectory_keyframes(sat_data)
            self.assertIn('satellite_id', trajectory)
            self.assertIn('keyframes', trajectory)
            self.assertIn('orbital_path', trajectory)

        # æ¸¬è©¦è¦†è“‹å‹•ç•«
        coverage_animation = builder.create_coverage_animation(self.test_stage4_data['optimal_pool'])
        self.assertIn('animation_id', coverage_animation)
        self.assertIn('frames', coverage_animation)

        print("âœ… å‹•ç•«å»ºæ§‹å™¨æ¸¬è©¦é€šé")

    def test_layer_data_generator_functionality(self):
        """æ¸¬è©¦åˆ†å±¤æ•¸æ“šç”Ÿæˆå™¨åŠŸèƒ½"""
        print("\nğŸ—‚ï¸ æ¸¬è©¦åˆ†å±¤æ•¸æ“šç”Ÿæˆå™¨...")

        # ç”Ÿæˆæ™‚é–“åºåˆ—æ•¸æ“š
        converter = TimeseriesConverter(self.test_config['timeseries'])
        timeseries_data = converter.convert_to_timeseries(self.test_stage4_data['optimal_pool'])

        # æ¸¬è©¦åˆ†å±¤æ•¸æ“šç”Ÿæˆ
        generator = LayeredDataGenerator(self.test_config['layers'])

        # æ¸¬è©¦éšå±¤å¼æ•¸æ“šé›†
        hierarchical_data = generator.generate_hierarchical_data(timeseries_data)
        self.assertIn('dataset_id', hierarchical_data)
        self.assertIn('constellation_hierarchy', hierarchical_data)
        self.assertIn('quality_hierarchy', hierarchical_data)
        self.assertIn('temporal_hierarchy', hierarchical_data)
        self.assertIn('geographic_hierarchy', hierarchical_data)

        # æ¸¬è©¦ç©ºé–“åˆ†å±¤
        spatial_layers = generator.create_spatial_layers(self.test_stage4_data['optimal_pool'])
        self.assertIsInstance(spatial_layers, dict)
        self.assertGreater(len(spatial_layers), 0)

        # æ¸¬è©¦æ™‚é–“åˆ†å±¤
        temporal_layers = generator.create_temporal_layers(timeseries_data)
        self.assertIsInstance(temporal_layers, dict)
        self.assertGreater(len(temporal_layers), 0)

        # æ¸¬è©¦å¤šå°ºåº¦ç´¢å¼•
        hierarchical_combined = {
            'hierarchical_dataset': hierarchical_data,
            'spatial_layers': spatial_layers,
            'temporal_layers': temporal_layers
        }

        multi_scale_index = generator.build_multi_scale_index(hierarchical_combined)
        self.assertIn('spatial_index', multi_scale_index)
        self.assertIn('temporal_index', multi_scale_index)
        self.assertIn('quality_index', multi_scale_index)
        self.assertIn('composite_index', multi_scale_index)

        print("âœ… åˆ†å±¤æ•¸æ“šç”Ÿæˆå™¨æ¸¬è©¦é€šé")

    def test_format_converter_hub_functionality(self):
        """æ¸¬è©¦æ ¼å¼è½‰æ›ä¸­å¿ƒåŠŸèƒ½"""
        print("\nğŸ“¦ æ¸¬è©¦æ ¼å¼è½‰æ›ä¸­å¿ƒ...")

        converter_hub = FormatConverterHub(self.test_config['formats'])

        # å‰µå»ºæ¸¬è©¦æ•¸æ“š
        test_data = {
            'timeseries_data': {'satellite_count': 3},
            'animation_data': {'frame_count': 100},
            'hierarchical_data': {'spatial_layers': {}},
            'metadata': {'processed_satellites': 3, 'processing_duration_seconds': 45.5}
        }

        # æ¸¬è©¦JSONè½‰æ›
        json_result = converter_hub.convert_to_json(test_data)
        self.assertIn('metadata', json_result)
        self.assertIn('data', json_result)
        self.assertIn('schema_version', json_result['metadata'])

        # æ¸¬è©¦GeoJSONè½‰æ›
        spatial_data = {'test_layer': {'grid_data': {'grid_cells': {}}}}
        geojson_result = converter_hub.convert_to_geojson(spatial_data)
        self.assertEqual(geojson_result['type'], 'FeatureCollection')
        self.assertIn('features', geojson_result)

        # æ¸¬è©¦CSVè½‰æ›
        tabular_data = [
            {'timestamp': '2025-01-01T00:00:00Z', 'satellite_id': 'TEST-1', 'latitude': 0.0, 'longitude': 0.0},
            {'timestamp': '2025-01-01T00:01:00Z', 'satellite_id': 'TEST-2', 'latitude': 45.0, 'longitude': 90.0}
        ]
        csv_result = converter_hub.convert_to_csv(tabular_data)
        self.assertIn('timestamp', csv_result)
        self.assertIn('satellite_id', csv_result)

        # æ¸¬è©¦APIåŒ…è£
        api_result = converter_hub.package_for_api(test_data)
        self.assertIn('api', api_result)
        self.assertIn('status', api_result)
        self.assertIn('data', api_result)
        self.assertEqual(api_result['status']['code'], 200)

        # æ¸¬è©¦æ‰¹é‡æ ¼å¼è½‰æ›
        batch_result = converter_hub.convert_multiple_formats(test_data, ['json', 'csv'])
        self.assertIn('results', batch_result)
        self.assertIn('statistics', batch_result)

        print("âœ… æ ¼å¼è½‰æ›ä¸­å¿ƒæ¸¬è©¦é€šé")

    def test_performance_benchmarks(self):
        """æ¸¬è©¦æ€§èƒ½åŸºæº–"""
        print("\nâš¡ æ¸¬è©¦æ€§èƒ½åŸºæº–...")

        # å‰µå»ºè¼ƒå¤§çš„æ¸¬è©¦æ•¸æ“šé›†
        large_test_data = self._create_large_test_dataset(50)  # 50é¡†è¡›æ˜Ÿ

        start_time = time.time()
        result = self.processor.process(large_test_data)
        processing_time = time.time() - start_time

        # é©—è­‰æ€§èƒ½ç›®æ¨™
        self.assertLess(processing_time, 60.0, f"50é¡†è¡›æ˜Ÿè™•ç†æ™‚é–“æ‡‰å°‘æ–¼60ç§’ï¼Œå¯¦éš›: {processing_time:.2f}ç§’")

        # é©—è­‰è¨˜æ†¶é«”æ•ˆç‡ï¼ˆé€šéæ•¸æ“šå¤§å°ä¼°ç®—ï¼‰
        result_size = len(str(result))
        self.assertLess(result_size, 100_000_000, "çµæœæ•¸æ“šå¤§å°æ‡‰åˆç†")  # < 100MBå­—ç¬¦ä¸²

        # é©—è­‰å£“ç¸®æ¯”
        metadata = result.get('metadata', {})
        compression_ratio = metadata.get('compression_ratio', 0.0)
        self.assertGreater(compression_ratio, 0.7, f"å£“ç¸®æ¯”æ‡‰å¤§æ–¼70%ï¼Œå¯¦éš›: {compression_ratio:.2f}")

        print(f"âœ… æ€§èƒ½åŸºæº–æ¸¬è©¦é€šé ({processing_time:.2f}ç§’, å£“ç¸®æ¯”: {compression_ratio:.2f})")

    def test_error_handling(self):
        """æ¸¬è©¦éŒ¯èª¤è™•ç†"""
        print("\nğŸš¨ æ¸¬è©¦éŒ¯èª¤è™•ç†...")

        # æ¸¬è©¦ç©ºè¼¸å…¥æ•¸æ“š
        with self.assertRaises(Exception):
            self.processor.process({})

        # æ¸¬è©¦ç¼ºå°‘å¿…éœ€æ¬„ä½
        incomplete_data = {
            'optimal_pool': {'satellites': []},  # ç©ºè¡›æ˜Ÿåˆ—è¡¨
            # ç¼ºå°‘ optimization_results
        }

        with self.assertRaises(Exception):
            self.processor.process(incomplete_data)

        # æ¸¬è©¦æ ¼å¼è½‰æ›éŒ¯èª¤è™•ç†
        converter_hub = FormatConverterHub(self.test_config['formats'])

        # æ¸¬è©¦ç„¡æ•ˆæ•¸æ“šçš„CSVè½‰æ›
        csv_result = converter_hub.convert_to_csv([])  # ç©ºåˆ—è¡¨
        self.assertEqual(csv_result, "")

        print("âœ… éŒ¯èª¤è™•ç†æ¸¬è©¦é€šé")

    def test_data_quality_validation(self):
        """æ¸¬è©¦æ•¸æ“šå“è³ªé©—è­‰"""
        print("\nğŸ” æ¸¬è©¦æ•¸æ“šå“è³ªé©—è­‰...")

        result = self.processor.process(self.test_stage4_data)

        # æª¢æŸ¥å…ƒæ•¸æ“šå®Œæ•´æ€§
        metadata = result.get('metadata', {})
        self.assertIn('processing_timestamp', metadata)
        self.assertIn('processed_satellites', metadata)
        self.assertIn('architecture_version', metadata)

        # æª¢æŸ¥çµ±è¨ˆè³‡è¨Š
        statistics = result.get('statistics', {})
        self.assertIn('satellites_processed', statistics)
        self.assertIn('timeseries_datapoints', statistics)
        self.assertIn('processing_duration', statistics)

        # é©—è­‰æ•¸æ“šä¸€è‡´æ€§
        timeseries_data = result.get('timeseries_data', {})
        animation_data = result.get('animation_data', {})

        if timeseries_data and animation_data:
            # è¡›æ˜Ÿæ•¸é‡æ‡‰è©²ä¸€è‡´
            timeseries_satellites = len(timeseries_data.get('satellite_timeseries', {}))
            animation_satellites = len(animation_data.get('satellite_trajectories', {}))

            if timeseries_satellites > 0 and animation_satellites > 0:
                self.assertEqual(timeseries_satellites, animation_satellites,
                               "æ™‚é–“åºåˆ—å’Œå‹•ç•«æ•¸æ“šä¸­çš„è¡›æ˜Ÿæ•¸é‡æ‡‰è©²ä¸€è‡´")

        print("âœ… æ•¸æ“šå“è³ªé©—è­‰æ¸¬è©¦é€šé")

    def _create_large_test_dataset(self, satellite_count: int) -> Dict[str, Any]:
        """å‰µå»ºå¤§å‹æ¸¬è©¦æ•¸æ“šé›†"""
        base_time = datetime.now(timezone.utc)

        satellites = []
        for i in range(satellite_count):
            satellites.append({
                'satellite_id': f'TEST-SAT-{i+1:04d}',
                'name': f'TestSat-{i+1}',
                'constellation': f'TestConstellation{(i % 5) + 1}',
                'stage1_orbital': {
                    'position_velocity': {
                        'position': {
                            'x': 6500.0 + i * 10,
                            'y': 0.0,
                            'z': 0.0,
                            'latitude': -90.0 + (180.0 * i / satellite_count),
                            'longitude': -180.0 + (360.0 * i / satellite_count),
                            'altitude': 500.0 + i * 10
                        },
                        'velocity': {'vx': 0.0, 'vy': 7.5, 'vz': 0.0}
                    }
                },
                'position_timeseries': [{
                    'x_km': 6500.0 + i * 10,
                    'y_km': 0.0,
                    'z_km': 0.0,
                    'latitude_deg': -90.0 + (180.0 * i / satellite_count),
                    'longitude_deg': -180.0 + (360.0 * i / satellite_count),
                    'altitude_km': 500.0 + i * 10
                }],
                'rsrp': -100.0 + (i % 30),
                'snr': 5.0 + (i % 20)
            })

        return {
            'optimal_pool': {
                'satellites': satellites,
                'metadata': {
                    'processing_timestamp': base_time.isoformat(),
                    'start_time': base_time.isoformat(),
                    'end_time': (base_time + timedelta(minutes=5)).isoformat(),
                    'total_satellites': satellite_count
                }
            },
            'optimization_results': {
                'optimization_summary': {
                    'satellites_optimized': satellite_count,
                    'optimization_method': 'genetic_algorithm',
                    'performance_improvement': 0.85
                }
            },
            'metadata': {
                'processing_timestamp': base_time.isoformat(),
                'stage': 'stage4_optimization',
                'processed_satellites': satellite_count
            }
        }

    def tearDown(self):
        """æ¸¬è©¦æ¸…ç†"""
        pass

def run_stage5_tests():
    """é‹è¡ŒStage 5æ¸¬è©¦å¥—ä»¶"""
    print("ğŸš€ é–‹å§‹Stage 5æ•¸æ“šæ•´åˆæ¸¬è©¦å¥—ä»¶...")
    print("="*60)

    # å‰µå»ºæ¸¬è©¦å¥—ä»¶
    test_suite = unittest.TestLoader().loadTestsFromTestCase(TestStage5DataIntegration)

    # é‹è¡Œæ¸¬è©¦
    runner = unittest.TextTestRunner(verbosity=2)
    result = runner.run(test_suite)

    # æ¸¬è©¦çµæœæ‘˜è¦
    print("="*60)
    print(f"ğŸ“Š æ¸¬è©¦çµæœæ‘˜è¦:")
    print(f"   ç¸½æ¸¬è©¦: {result.testsRun}")
    print(f"   æˆåŠŸ: {result.testsRun - len(result.failures) - len(result.errors)}")
    print(f"   å¤±æ•—: {len(result.failures)}")
    print(f"   éŒ¯èª¤: {len(result.errors)}")

    if result.failures:
        print(f"\nâŒ å¤±æ•—çš„æ¸¬è©¦:")
        for test, traceback in result.failures:
            print(f"   - {test}: {traceback}")

    if result.errors:
        print(f"\nğŸš¨ éŒ¯èª¤çš„æ¸¬è©¦:")
        for test, traceback in result.errors:
            print(f"   - {test}: {traceback}")

    success_rate = (result.testsRun - len(result.failures) - len(result.errors)) / result.testsRun * 100
    print(f"\nâœ… ç¸½é«”æˆåŠŸç‡: {success_rate:.1f}%")

    return result.wasSuccessful()

if __name__ == '__main__':
    success = run_stage5_tests()
    exit(0 if success else 1)