"""
è·¨éšæ®µä¸€è‡´æ€§é©—è­‰å™¨ - Stage 5æ¨¡çµ„åŒ–çµ„ä»¶

è·è²¬ï¼š
1. é©—è­‰éšæ®µé–“æ•¸æ“šä¸€è‡´æ€§
2. æ™‚é–“è»¸åŒæ­¥æª¢æŸ¥
3. è¡›æ˜ŸIDæ˜ å°„é©—è­‰
4. æ•¸æ“šå®Œæ•´æ€§æª¢æŸ¥
"""

import logging
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime, timedelta, timezone

logger = logging.getLogger(__name__)

class CrossStageValidator:
    """è·¨éšæ®µä¸€è‡´æ€§é©—è­‰å™¨ - ç¢ºä¿éšæ®µé–“æ•¸æ“šä¸€è‡´æ€§å’Œæ™‚é–“è»¸åŒæ­¥"""
    
    def __init__(self):
        """åˆå§‹åŒ–è·¨éšæ®µä¸€è‡´æ€§é©—è­‰å™¨"""
        self.logger = logging.getLogger(f"{__name__}.CrossStageValidator")
        
        # é©—è­‰çµ±è¨ˆ
        self.validation_statistics = {
            "total_validations": 0,
            "consistency_checks": 0,
            "time_sync_checks": 0,
            "satellite_mapping_checks": 0,
            "validation_errors": 0,
            "validation_warnings": 0
        }
        
        # é©—è­‰é–¾å€¼é…ç½®
        self.validation_thresholds = {
            "time_sync_tolerance_seconds": 300,  # 5åˆ†é˜æ™‚é–“å®¹å¿åº¦
            "satellite_count_variance_threshold": 0.05,  # 5%è¡›æ˜Ÿæ•¸é‡å·®ç•°å®¹å¿
            "data_completeness_threshold": 0.95  # 95%æ•¸æ“šå®Œæ•´æ€§è¦æ±‚
        }
        
        self.logger.info("âœ… è·¨éšæ®µä¸€è‡´æ€§é©—è­‰å™¨åˆå§‹åŒ–å®Œæˆ")
        self.logger.info(f"   æ™‚é–“åŒæ­¥å®¹å¿åº¦: {self.validation_thresholds['time_sync_tolerance_seconds']}ç§’")
        self.logger.info(f"   è¡›æ˜Ÿæ•¸é‡å·®ç•°é–¾å€¼: {self.validation_thresholds['satellite_count_variance_threshold']*100}%")
    
    def validate_cross_stage_consistency(self, stage_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        é©—è­‰è·¨éšæ®µæ•¸æ“šä¸€è‡´æ€§
        
        Args:
            stage_data: åŒ…å«æ‰€æœ‰éšæ®µæ•¸æ“šçš„å­—å…¸
            
        Returns:
            é©—è­‰çµæœå ±å‘Š
        """
        self.logger.info("ğŸ” é–‹å§‹è·¨éšæ®µä¸€è‡´æ€§é©—è­‰...")
        
        validation_results = {
            "overall_valid": True,
            "consistency_checks": {},
            "time_sync_results": {},
            "satellite_mapping_results": {},
            "data_completeness_results": {},
            "validation_summary": {}
        }
        
        self.validation_statistics["total_validations"] += 1
        
        # 1. æ•¸æ“šä¸€è‡´æ€§æª¢æŸ¥
        consistency_results = self._validate_data_consistency(stage_data)
        validation_results["consistency_checks"] = consistency_results
        
        if not consistency_results["valid"]:
            validation_results["overall_valid"] = False
            self.validation_statistics["validation_errors"] += len(consistency_results["errors"])
        
        # 2. æ™‚é–“è»¸åŒæ­¥æª¢æŸ¥
        time_sync_results = self._validate_time_axis_synchronization(stage_data)
        validation_results["time_sync_results"] = time_sync_results
        
        if not time_sync_results["synchronized"]:
            validation_results["overall_valid"] = False
            self.validation_statistics["validation_errors"] += len(time_sync_results["sync_errors"])
        
        # 3. è¡›æ˜Ÿæ˜ å°„é©—è­‰
        mapping_results = self._validate_satellite_mapping(stage_data)
        validation_results["satellite_mapping_results"] = mapping_results
        
        if not mapping_results["mapping_valid"]:
            validation_results["overall_valid"] = False
            self.validation_statistics["validation_errors"] += len(mapping_results["mapping_errors"])
        
        # 4. æ•¸æ“šå®Œæ•´æ€§æª¢æŸ¥
        completeness_results = self._validate_data_completeness(stage_data)
        validation_results["data_completeness_results"] = completeness_results
        
        if not completeness_results["complete"]:
            self.validation_statistics["validation_warnings"] += len(completeness_results["completeness_warnings"])
        
        # ç”Ÿæˆé©—è­‰æ‘˜è¦
        validation_results["validation_summary"] = self._generate_validation_summary(validation_results)
        
        self.validation_statistics["consistency_checks"] += 1
        
        status = "âœ… é€šé" if validation_results["overall_valid"] else "âŒ å¤±æ•—"
        self.logger.info(f"{status} è·¨éšæ®µä¸€è‡´æ€§é©—è­‰å®Œæˆ")
        
        return validation_results
    
    def _validate_data_consistency(self, stage_data: Dict[str, Any]) -> Dict[str, Any]:
        """é©—è­‰æ•¸æ“šä¸€è‡´æ€§"""
        errors = []
        warnings = []
        
        # æª¢æŸ¥éšæ®µæ•¸æ“šå­˜åœ¨æ€§
        required_stages = ["stage1_orbital", "stage2_visibility", "stage3_timeseries"]
        available_stages = []
        
        for stage in required_stages:
            if stage in stage_data and stage_data[stage]:
                available_stages.append(stage)
            else:
                errors.append(f"ç¼ºå°‘å¿…éœ€çš„{stage}æ•¸æ“š")
        
        # æª¢æŸ¥æ•¸æ“šæ ¼å¼ä¸€è‡´æ€§
        for stage in available_stages:
            data = stage_data[stage]
            
            # æª¢æŸ¥åŸºæœ¬çµæ§‹
            if not isinstance(data, dict):
                errors.append(f"{stage}æ•¸æ“šæ ¼å¼éŒ¯èª¤ï¼šéå­—å…¸æ ¼å¼")
                continue
            
            if "data" not in data:
                errors.append(f"{stage}ç¼ºå°‘'data'å­—æ®µ")
                continue
            
            if "metadata" not in data:
                warnings.append(f"{stage}ç¼ºå°‘'metadata'å­—æ®µ")
            
            # æª¢æŸ¥è¡›æ˜Ÿæ•¸æ“šçµæ§‹
            satellites = data.get("data", {}).get("satellites", [])
            if not isinstance(satellites, list):
                errors.append(f"{stage}è¡›æ˜Ÿæ•¸æ“šæ ¼å¼éŒ¯èª¤")
            elif len(satellites) == 0:
                warnings.append(f"{stage}è¡›æ˜Ÿæ•¸æ“šç‚ºç©º")
        
        return {
            "valid": len(errors) == 0,
            "errors": errors,
            "warnings": warnings,
            "available_stages": available_stages
        }
    
    def _validate_time_axis_synchronization(self, stage_data: Dict[str, Any]) -> Dict[str, Any]:
        """é©—è­‰æ™‚é–“è»¸åŒæ­¥"""
        sync_errors = []
        sync_warnings = []
        time_info = {}
        
        self.validation_statistics["time_sync_checks"] += 1
        
        # æ”¶é›†å„éšæ®µæ™‚é–“ä¿¡æ¯
        for stage_name, data in stage_data.items():
            if not data or not isinstance(data, dict):
                continue
            
            metadata = data.get("metadata", {})
            processing_timestamp = metadata.get("processing_timestamp")
            
            if processing_timestamp:
                try:
                    # è§£ææ™‚é–“æˆ³
                    if isinstance(processing_timestamp, str):
                        timestamp = datetime.fromisoformat(processing_timestamp.replace('Z', '+00:00'))
                    else:
                        timestamp = processing_timestamp
                    
                    time_info[stage_name] = {
                        "timestamp": timestamp,
                        "timestamp_str": processing_timestamp
                    }
                except Exception as e:
                    sync_errors.append(f"{stage_name}æ™‚é–“æˆ³è§£æå¤±æ•—: {e}")
        
        # æª¢æŸ¥æ™‚é–“è»¸åŒæ­¥
        if len(time_info) >= 2:
            timestamps = list(time_info.values())
            base_time = timestamps[0]["timestamp"]
            
            for i, time_data in enumerate(timestamps[1:], 1):
                time_diff = abs((time_data["timestamp"] - base_time).total_seconds())
                
                if time_diff > self.validation_thresholds["time_sync_tolerance_seconds"]:
                    stage_names = list(time_info.keys())
                    sync_errors.append(
                        f"{stage_names[0]}èˆ‡{stage_names[i]}æ™‚é–“å·®ç•°éå¤§: {time_diff:.1f}ç§’"
                    )
                elif time_diff > 60:  # 1åˆ†é˜è­¦å‘Šé–¾å€¼
                    stage_names = list(time_info.keys())
                    sync_warnings.append(
                        f"{stage_names[0]}èˆ‡{stage_names[i]}æ™‚é–“å·®ç•°: {time_diff:.1f}ç§’"
                    )
        
        return {
            "synchronized": len(sync_errors) == 0,
            "sync_errors": sync_errors,
            "sync_warnings": sync_warnings,
            "time_info": time_info,
            "max_time_difference": self._calculate_max_time_difference(time_info)
        }
    
    def _calculate_max_time_difference(self, time_info: Dict[str, Any]) -> float:
        """è¨ˆç®—æœ€å¤§æ™‚é–“å·®ç•°"""
        if len(time_info) < 2:
            return 0.0
        
        timestamps = [info["timestamp"] for info in time_info.values()]
        min_time = min(timestamps)
        max_time = max(timestamps)
        
        return (max_time - min_time).total_seconds()
    
    def _validate_satellite_mapping(self, stage_data: Dict[str, Any]) -> Dict[str, Any]:
        """é©—è­‰è¡›æ˜ŸIDæ˜ å°„"""
        mapping_errors = []
        mapping_warnings = []
        satellite_counts = {}
        satellite_sets = {}
        
        self.validation_statistics["satellite_mapping_checks"] += 1
        
        # æ”¶é›†å„éšæ®µè¡›æ˜ŸID
        for stage_name, data in stage_data.items():
            if not data or not isinstance(data, dict):
                continue
            
            # æ”¯æŒå¤šç¨®æ•¸æ“šæ ¼å¼ï¼šdata.satellites æˆ–ç›´æ¥ satellites
            satellites = []
            if "data" in data and isinstance(data["data"], dict) and "satellites" in data["data"]:
                satellites = data["data"]["satellites"]
            elif "satellites" in data:
                satellites = data["satellites"]

            satellite_ids = set()

            for satellite in satellites:
                if not isinstance(satellite, dict):
                    continue
                satellite_id = satellite.get("satellite_id")
                if satellite_id:
                    satellite_ids.add(satellite_id)
            
            satellite_counts[stage_name] = len(satellite_ids)
            satellite_sets[stage_name] = satellite_ids
        
        # æª¢æŸ¥è¡›æ˜Ÿæ•¸é‡ä¸€è‡´æ€§
        if len(satellite_counts) >= 2:
            count_values = list(satellite_counts.values())
            max_count = max(count_values)
            min_count = min(count_values)
            
            if max_count > 0:
                variance = (max_count - min_count) / max_count
                
                if variance > self.validation_thresholds["satellite_count_variance_threshold"]:
                    mapping_errors.append(
                        f"è¡›æ˜Ÿæ•¸é‡å·®ç•°éå¤§: æœ€å¤§{max_count}, æœ€å°{min_count}, å·®ç•°{variance*100:.1f}%"
                    )
        
        # æª¢æŸ¥è¡›æ˜ŸIDé‡ç–Šåº¦
        if len(satellite_sets) >= 2:
            stage_names = list(satellite_sets.keys())
            base_set = satellite_sets[stage_names[0]]
            
            for i, stage_name in enumerate(stage_names[1:], 1):
                current_set = satellite_sets[stage_name]
                
                intersection = base_set & current_set
                union = base_set | current_set
                
                if len(union) > 0:
                    overlap_ratio = len(intersection) / len(union)
                    
                    if overlap_ratio < 0.8:  # 80%é‡ç–Šåº¦é–¾å€¼
                        mapping_warnings.append(
                            f"{stage_names[0]}èˆ‡{stage_name}è¡›æ˜ŸIDé‡ç–Šåº¦ä½: {overlap_ratio*100:.1f}%"
                        )
        
        return {
            "mapping_valid": len(mapping_errors) == 0,
            "mapping_errors": mapping_errors,
            "mapping_warnings": mapping_warnings,
            "satellite_counts": satellite_counts,
            "satellite_overlap_analysis": self._analyze_satellite_overlap(satellite_sets)
        }
    
    def _analyze_satellite_overlap(self, satellite_sets: Dict[str, set]) -> Dict[str, Any]:
        """åˆ†æè¡›æ˜ŸIDé‡ç–Šæƒ…æ³"""
        if len(satellite_sets) < 2:
            return {"analysis": "éœ€è¦è‡³å°‘å…©å€‹éšæ®µæ•¸æ“šé€²è¡Œé‡ç–Šåˆ†æ"}
        
        stage_names = list(satellite_sets.keys())
        overlap_matrix = {}
        
        for i, stage1 in enumerate(stage_names):
            overlap_matrix[stage1] = {}
            for stage2 in stage_names:
                if stage1 != stage2:
                    set1 = satellite_sets[stage1]
                    set2 = satellite_sets[stage2]
                    
                    intersection = set1 & set2
                    union = set1 | set2
                    
                    overlap_ratio = len(intersection) / len(union) if len(union) > 0 else 0
                    overlap_matrix[stage1][stage2] = {
                        "overlap_count": len(intersection),
                        "overlap_ratio": overlap_ratio,
                        "unique_to_stage1": len(set1 - set2),
                        "unique_to_stage2": len(set2 - set1)
                    }
        
        return overlap_matrix
    
    def _validate_data_completeness(self, stage_data: Dict[str, Any]) -> Dict[str, Any]:
        """é©—è­‰æ•¸æ“šå®Œæ•´æ€§"""
        completeness_warnings = []
        completeness_info = {}
        
        for stage_name, data in stage_data.items():
            if not data or not isinstance(data, dict):
                continue
            
            satellites = data.get("data", {}).get("satellites", [])
            completeness_info[stage_name] = {
                "total_satellites": len(satellites),
                "satellites_with_complete_data": 0,
                "completeness_ratio": 0.0
            }
            
            if satellites:
                complete_count = 0
                for satellite in satellites:
                    # æª¢æŸ¥åŸºæœ¬å­—æ®µå®Œæ•´æ€§
                    required_fields = ["satellite_id", "constellation"]
                    has_all_fields = all(
                        field in satellite and satellite[field] is not None 
                        for field in required_fields
                    )
                    
                    if has_all_fields:
                        complete_count += 1
                
                completeness_ratio = complete_count / len(satellites)
                completeness_info[stage_name]["satellites_with_complete_data"] = complete_count
                completeness_info[stage_name]["completeness_ratio"] = completeness_ratio
                
                if completeness_ratio < self.validation_thresholds["data_completeness_threshold"]:
                    completeness_warnings.append(
                        f"{stage_name}æ•¸æ“šå®Œæ•´æ€§ä¸è¶³: {completeness_ratio*100:.1f}%"
                    )
        
        overall_completeness = self._calculate_overall_completeness(completeness_info)
        
        return {
            "complete": len(completeness_warnings) == 0,
            "completeness_warnings": completeness_warnings,
            "completeness_info": completeness_info,
            "overall_completeness": overall_completeness
        }
    
    def _calculate_overall_completeness(self, completeness_info: Dict[str, Any]) -> float:
        """è¨ˆç®—æ•´é«”å®Œæ•´æ€§"""
        if not completeness_info:
            return 0.0
        
        ratios = [info["completeness_ratio"] for info in completeness_info.values()]
        return sum(ratios) / len(ratios) if ratios else 0.0
    
    def _generate_validation_summary(self, validation_results: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆé©—è­‰æ‘˜è¦"""
        summary = {
            "validation_status": "PASS" if validation_results["overall_valid"] else "FAIL",
            "total_errors": 0,
            "total_warnings": 0,
            "key_findings": []
        }
        
        # çµ±è¨ˆéŒ¯èª¤å’Œè­¦å‘Š
        for check_name, check_result in validation_results.items():
            if isinstance(check_result, dict):
                errors = check_result.get("errors", [])
                warnings = check_result.get("warnings", [])
                
                summary["total_errors"] += len(errors)
                summary["total_warnings"] += len(warnings)
        
        # é—œéµç™¼ç¾
        if validation_results["consistency_checks"].get("available_stages"):
            summary["key_findings"].append(
                f"å¯ç”¨éšæ®µ: {len(validation_results['consistency_checks']['available_stages'])}"
            )
        
        if validation_results["time_sync_results"].get("max_time_difference"):
            max_diff = validation_results["time_sync_results"]["max_time_difference"]
            summary["key_findings"].append(f"æœ€å¤§æ™‚é–“å·®ç•°: {max_diff:.1f}ç§’")
        
        if validation_results["satellite_mapping_results"].get("satellite_counts"):
            counts = validation_results["satellite_mapping_results"]["satellite_counts"]
            summary["key_findings"].append(f"è¡›æ˜Ÿæ•¸é‡ç¯„åœ: {min(counts.values())}-{max(counts.values())}")
        
        return summary
    
    def run_comprehensive_validation(self, stage_data: Dict[str, Any]) -> Dict[str, Any]:
        """é‹è¡Œç¶œåˆé©—è­‰æª¢æŸ¥"""
        self.logger.info("ğŸ” é–‹å§‹ç¶œåˆé©—è­‰æª¢æŸ¥...")
        
        # åŸ·è¡Œæ‰€æœ‰é©—è­‰
        validation_result = self.validate_cross_stage_consistency(stage_data)
        
        # æ·»åŠ é¡å¤–æª¢æŸ¥
        additional_checks = self._run_additional_checks(stage_data)
        validation_result["additional_checks"] = additional_checks
        
        # æ›´æ–°æ•´é«”é©—è­‰ç‹€æ…‹
        if additional_checks and not additional_checks.get("all_passed", True):
            validation_result["overall_valid"] = False
        
        return validation_result
    
    def _run_additional_checks(self, stage_data: Dict[str, Any]) -> Dict[str, Any]:
        """é‹è¡Œé¡å¤–æª¢æŸ¥"""
        checks = {
            "all_passed": True,
            "metadata_consistency": self._check_metadata_consistency(stage_data),
            "data_version_compatibility": self._check_data_version_compatibility(stage_data),
            "processing_pipeline_integrity": self._check_pipeline_integrity(stage_data)
        }
        
        # æª¢æŸ¥æ˜¯å¦æœ‰å¤±æ•—é …ç›®
        for check_name, check_result in checks.items():
            if check_name != "all_passed" and isinstance(check_result, dict):
                if not check_result.get("passed", True):
                    checks["all_passed"] = False
                    break
        
        return checks
    
    def _check_metadata_consistency(self, stage_data: Dict[str, Any]) -> Dict[str, Any]:
        """æª¢æŸ¥å…ƒæ•¸æ“šä¸€è‡´æ€§"""
        try:
            metadata_fields = []
            processing_versions = set()
            consistency_errors = []

            # æª¢æŸ¥æ¯å€‹éšæ®µçš„å…ƒæ•¸æ“š
            for stage_name, data in stage_data.items():
                if not data or not isinstance(data, dict):
                    consistency_errors.append(f"{stage_name}: æ•¸æ“šæ ¼å¼éŒ¯èª¤")
                    continue

                metadata = data.get("metadata", {})
                if not metadata:
                    consistency_errors.append(f"{stage_name}: ç¼ºå°‘metadata")
                    continue

                # æª¢æŸ¥åŸºæœ¬å­—æ®µå­˜åœ¨æ€§
                required_fields = ["processing_timestamp", "processor_version"]
                for field in required_fields:
                    if field not in metadata:
                        consistency_errors.append(f"{stage_name}: ç¼ºå°‘{field}å­—æ®µ")

                # æ”¶é›†ç‰ˆæœ¬ä¿¡æ¯
                version = metadata.get("processor_version")
                if version:
                    processing_versions.add(version)

                metadata_fields.append({
                    "stage": stage_name,
                    "fields_count": len(metadata),
                    "has_timestamp": "processing_timestamp" in metadata,
                    "has_version": "processor_version" in metadata
                })

            # æª¢æŸ¥ç‰ˆæœ¬ä¸€è‡´æ€§
            if len(processing_versions) > 1:
                consistency_errors.append(f"ç™¼ç¾å¤šå€‹è™•ç†å™¨ç‰ˆæœ¬: {list(processing_versions)}")

            passed = len(consistency_errors) == 0
            info = f"æª¢æŸ¥äº†{len(metadata_fields)}å€‹éšæ®µçš„å…ƒæ•¸æ“š"
            if not passed:
                info += f"ï¼Œç™¼ç¾{len(consistency_errors)}å€‹ä¸€è‡´æ€§å•é¡Œ"

            return {
                "passed": passed,
                "info": info,
                "metadata_fields": metadata_fields,
                "processing_versions": list(processing_versions),
                "errors": consistency_errors
            }
        except Exception as e:
            return {
                "passed": False,
                "info": f"å…ƒæ•¸æ“šä¸€è‡´æ€§æª¢æŸ¥å¤±æ•—: {str(e)}",
                "errors": [str(e)]
            }
    
    def _check_data_version_compatibility(self, stage_data: Dict[str, Any]) -> Dict[str, Any]:
        """æª¢æŸ¥æ•¸æ“šç‰ˆæœ¬å…¼å®¹æ€§"""
        try:
            version_info = {}
            compatibility_errors = []

            # æ”¶é›†å„éšæ®µç‰ˆæœ¬ä¿¡æ¯
            for stage_name, data in stage_data.items():
                if not data or not isinstance(data, dict):
                    continue

                metadata = data.get("metadata", {})
                data_version = metadata.get("data_version", "unknown")
                processor_version = metadata.get("processor_version", "unknown")
                schema_version = metadata.get("schema_version", "unknown")

                version_info[stage_name] = {
                    "data_version": data_version,
                    "processor_version": processor_version,
                    "schema_version": schema_version
                }

            # æª¢æŸ¥ç‰ˆæœ¬å…¼å®¹æ€§
            unique_data_versions = set(info["data_version"] for info in version_info.values())
            unique_schema_versions = set(info["schema_version"] for info in version_info.values())

            # æ•¸æ“šç‰ˆæœ¬æ‡‰è©²ç›¸è¿‘æˆ–ç›¸åŒ
            if len(unique_data_versions) > 2:
                compatibility_errors.append(f"æ•¸æ“šç‰ˆæœ¬å·®ç•°éå¤§: {list(unique_data_versions)}")

            # Schemaç‰ˆæœ¬æ‡‰è©²å…¼å®¹
            if len(unique_schema_versions) > 1:
                # å…è¨±minor versionå·®ç•°ï¼Œä½†major versionæ‡‰è©²ç›¸åŒ
                major_versions = set()
                for version in unique_schema_versions:
                    if version != "unknown" and "." in str(version):
                        major_version = str(version).split(".")[0]
                        major_versions.add(major_version)

                if len(major_versions) > 1:
                    compatibility_errors.append(f"Schemaä¸»ç‰ˆæœ¬ä¸å…¼å®¹: {list(unique_schema_versions)}")

            passed = len(compatibility_errors) == 0
            info = f"æª¢æŸ¥äº†{len(version_info)}å€‹éšæ®µçš„ç‰ˆæœ¬ä¿¡æ¯"
            if not passed:
                info += f"ï¼Œç™¼ç¾{len(compatibility_errors)}å€‹å…¼å®¹æ€§å•é¡Œ"

            return {
                "passed": passed,
                "info": info,
                "version_info": version_info,
                "unique_data_versions": list(unique_data_versions),
                "unique_schema_versions": list(unique_schema_versions),
                "errors": compatibility_errors
            }
        except Exception as e:
            return {
                "passed": False,
                "info": f"æ•¸æ“šç‰ˆæœ¬å…¼å®¹æ€§æª¢æŸ¥å¤±æ•—: {str(e)}",
                "errors": [str(e)]
            }
    
    def _check_pipeline_integrity(self, stage_data: Dict[str, Any]) -> Dict[str, Any]:
        """æª¢æŸ¥æµæ°´ç·šå®Œæ•´æ€§"""
        try:
            pipeline_info = {}
            integrity_errors = []

            # å®šç¾©æœŸæœ›çš„æµæ°´ç·šéšæ®µé †åº
            expected_stages = ["stage1_orbital", "stage2_visibility", "stage3_signal", "stage4_timeseries", "stage5_integration"]
            available_stages = [stage for stage in expected_stages if stage in stage_data and stage_data[stage]]

            # æª¢æŸ¥éšæ®µå®Œæ•´æ€§
            missing_stages = [stage for stage in expected_stages if stage not in available_stages]
            if missing_stages:
                integrity_errors.append(f"ç¼ºå°‘æµæ°´ç·šéšæ®µ: {missing_stages}")

            # æª¢æŸ¥æ•¸æ“šæµå‘ä¸€è‡´æ€§
            for i, stage in enumerate(available_stages):
                data = stage_data[stage]
                if not isinstance(data, dict):
                    integrity_errors.append(f"{stage}: æ•¸æ“šæ ¼å¼éŒ¯èª¤")
                    continue

                # æª¢æŸ¥è¼¸å…¥ä¾†æº (stage2+æ‡‰è©²å¼•ç”¨å‰ä¸€éšæ®µ)
                metadata = data.get("metadata", {})
                input_source = metadata.get("input_source")

                if i > 0:  # éç¬¬ä¸€éšæ®µ
                    previous_stage = available_stages[i-1]
                    if input_source and previous_stage not in str(input_source):
                        integrity_errors.append(f"{stage}: è¼¸å…¥ä¾†æºä¸åŒ¹é…ï¼ŒæœŸæœ›ä¾†è‡ª{previous_stage}")

                # æª¢æŸ¥è™•ç†ç‹€æ…‹
                processing_status = metadata.get("processing_status", "unknown")
                if processing_status == "failed":
                    integrity_errors.append(f"{stage}: è™•ç†ç‹€æ…‹ç‚ºå¤±æ•—")
                elif processing_status == "unknown":
                    integrity_errors.append(f"{stage}: è™•ç†ç‹€æ…‹æœªçŸ¥")

                pipeline_info[stage] = {
                    "available": True,
                    "input_source": input_source,
                    "processing_status": processing_status,
                    "has_data": "data" in data and bool(data["data"]),
                    "satellite_count": len(data.get("data", {}).get("satellites", []))
                }

            # æª¢æŸ¥æ•¸æ“šé‡ç´šä¸€è‡´æ€§ (ç›¸é„°éšæ®µçš„è¡›æ˜Ÿæ•¸é‡ä¸æ‡‰è©²å·®ç•°éå¤§)
            for i in range(len(available_stages) - 1):
                current_stage = available_stages[i]
                next_stage = available_stages[i + 1]

                current_count = pipeline_info[current_stage]["satellite_count"]
                next_count = pipeline_info[next_stage]["satellite_count"]

                if current_count > 0 and next_count > 0:
                    reduction_ratio = (current_count - next_count) / current_count
                    if reduction_ratio > 0.5:  # æ•¸æ“šæ¸›å°‘è¶…é50%
                        integrity_errors.append(
                            f"{current_stage}åˆ°{next_stage}æ•¸æ“šæ¸›å°‘éå¤š: {current_count}â†’{next_count} ({reduction_ratio*100:.1f}%)"
                        )

            passed = len(integrity_errors) == 0
            info = f"æª¢æŸ¥äº†{len(available_stages)}/{len(expected_stages)}å€‹æµæ°´ç·šéšæ®µ"
            if not passed:
                info += f"ï¼Œç™¼ç¾{len(integrity_errors)}å€‹å®Œæ•´æ€§å•é¡Œ"

            return {
                "passed": passed,
                "info": info,
                "pipeline_info": pipeline_info,
                "available_stages": available_stages,
                "missing_stages": missing_stages,
                "errors": integrity_errors
            }
        except Exception as e:
            return {
                "passed": False,
                "info": f"æµæ°´ç·šå®Œæ•´æ€§æª¢æŸ¥å¤±æ•—: {str(e)}",
                "errors": [str(e)]
            }
    
    def get_validation_statistics(self) -> Dict[str, Any]:
        """ç²å–é©—è­‰çµ±è¨ˆä¿¡æ¯"""
        return self.validation_statistics.copy()