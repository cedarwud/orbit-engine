"""
Cache Manager for Stage 6 Persistence API
å¤šå±¤å¿«å–ç®¡ç†æ¨¡çµ„ - æ•´åˆåŸæœ¬åˆ†æ•£åœ¨å„è™•çš„å¿«å–é‚è¼¯

Author: Claude Code
Created: 2025-09-21
Purpose: æä¾›L1è¨˜æ†¶é«”ã€L2 Redisã€L3ç£ç¢Ÿçš„å¤šå±¤å¿«å–ç®¡ç†
"""

import json
import os
import time
import hashlib
import logging
import pickle
import sys
from datetime import datetime, timezone, timedelta
from pathlib import Path
from typing import Dict, Any, Optional, List, Union, Tuple
from dataclasses import dataclass, asdict
import threading
from collections import OrderedDict

try:
    import psutil
    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False


@dataclass
class CacheEntry:
    """å¿«å–æ¢ç›®"""
    key: str
    value: Any
    timestamp: float
    ttl: Optional[float]
    access_count: int
    data_type: str
    size_bytes: int


class LRUCache:
    """LRU å¿«å–å¯¦ç¾"""

    def __init__(self, max_size: int = 1000):
        self.max_size = max_size
        self.cache = OrderedDict()
        self.lock = threading.RLock()

    def get(self, key: str) -> Optional[Any]:
        with self.lock:
            if key in self.cache:
                # ç§»åˆ°æœ€å¾Œï¼ˆæœ€è¿‘ä½¿ç”¨ï¼‰
                value = self.cache.pop(key)
                self.cache[key] = value
                return value
            return None

    def set(self, key: str, value: Any) -> None:
        with self.lock:
            if key in self.cache:
                self.cache.pop(key)
            elif len(self.cache) >= self.max_size:
                # ç§»é™¤æœ€èˆŠçš„é …ç›®
                self.cache.popitem(last=False)
            self.cache[key] = value

    def delete(self, key: str) -> bool:
        with self.lock:
            return self.cache.pop(key, None) is not None

    def clear(self) -> None:
        with self.lock:
            self.cache.clear()

    def size(self) -> int:
        return len(self.cache)


class CacheManager:
    """
    å¤šå±¤å¿«å–ç®¡ç†å™¨

    åŠŸèƒ½è·è²¬ï¼š
    - å¤šå±¤å¿«å–ç®¡ç†ï¼ˆL1è¨˜æ†¶é«”ã€L2 Redisã€L3ç£ç¢Ÿï¼‰
    - æ™ºèƒ½é è¼‰ç­–ç•¥
    - å¿«å–å¤±æ•ˆç®¡ç†
    - æ€§èƒ½å„ªåŒ–
    """

    def __init__(self, config: Optional[Dict] = None):
        """åˆå§‹åŒ–å¿«å–ç®¡ç†å™¨"""
        self.config = config or {}
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")

        # å¿«å–é…ç½®
        self.cache_config = self.config.get('cache', {
            'l1_cache': {
                'type': 'memory',
                'size_mb': 256,
                'ttl_seconds': 300
            },
            'l2_cache': {
                'type': 'redis',
                'host': 'localhost',
                'port': 6379,
                'ttl_seconds': 3600
            },
            'l3_cache': {
                'type': 'disk',
                'path': 'data/cache/stage6',
                'size_gb': 10
            }
        })

        # åˆå§‹åŒ–å„å±¤å¿«å–
        self._init_l1_cache()
        self._init_l2_cache()
        self._init_l3_cache()

        # å¿«å–çµ±è¨ˆ
        self.cache_stats = {
            'l1_hits': 0,
            'l1_misses': 0,
            'l2_hits': 0,
            'l2_misses': 0,
            'l3_hits': 0,
            'l3_misses': 0,
            'total_requests': 0,
            'preload_operations': 0
        }

        # ç†±é–€æ•¸æ“šè¿½è¹¤
        self.hot_data_tracker = {}
        self.access_patterns = {}

        self.logger.info("âœ… Cache Manager åˆå§‹åŒ–å®Œæˆ")

    def _init_l1_cache(self) -> None:
        """åˆå§‹åŒ–L1è¨˜æ†¶é«”å¿«å–"""
        l1_config = self.cache_config['l1_cache']
        # ä½¿ç”¨ç²¾ç¢ºçš„å…§å­˜ç®¡ç†ï¼ŒåŸºæ–¼å¯¦éš›å¯ç”¨å…§å­˜
        available_memory_bytes = l1_config['size_mb'] * 1024 * 1024

        # å¦‚æœpsutilå¯ç”¨ï¼Œè€ƒæ…®ç³»çµ±å¯¦éš›å¯ç”¨å…§å­˜
        if PSUTIL_AVAILABLE:
            try:
                system_memory = psutil.virtual_memory()
                # ç¢ºä¿ä¸è¶…éç³»çµ±å¯ç”¨å…§å­˜çš„10%
                max_safe_memory = int(system_memory.available * 0.1)
                available_memory_bytes = min(available_memory_bytes, max_safe_memory)
            except Exception as e:
                self.logger.warning(f"âš ï¸ ç„¡æ³•ç²å–ç³»çµ±å…§å­˜ä¿¡æ¯ï¼Œä½¿ç”¨é…ç½®å€¼: {e}")

        # å‹•æ…‹è¨ˆç®—æœ€å¤§æ¢ç›®æ•¸ï¼ŒåŸºæ–¼ç³»çµ±å¯¦éš›æƒ…æ³
        # ä½¿ç”¨ä¿å®ˆçš„ä¼°ç®—ï¼šè€ƒæ…® Python å°è±¡é–‹éŠ·å’Œæ•¸æ“šçµæ§‹é–‹éŠ·
        estimated_overhead_per_entry = 200  # bytes for dict overhead, references, etc.

        # ç‚ºå®‰å…¨èµ·è¦‹ï¼Œä½¿ç”¨80%çš„åˆ†é…å…§å­˜
        usable_memory = int(available_memory_bytes * 0.8)

        # åŸºæ–¼å¹³å‡æ•¸æ“šå¤§å°çš„å‹•æ…‹è¨ˆç®—ï¼Œè€Œéç¡¬ç·¨ç¢¼å‡è¨­
        self.l1_average_entry_size = 1024  # åˆå§‹ä¼°ç®—ï¼Œå°‡åœ¨é‹è¡Œæ™‚å‹•æ…‹èª¿æ•´
        max_entries = usable_memory // (self.l1_average_entry_size + estimated_overhead_per_entry)

        self.l1_cache = LRUCache(max_size=max_entries)
        self.l1_ttl = l1_config['ttl_seconds']
        self.l1_size_samples = []  # ç”¨æ–¼å‹•æ…‹èª¿æ•´å¹³å‡å¤§å°çš„æ¨£æœ¬

        self.logger.info(f"âœ… L1 è¨˜æ†¶é«”å¿«å–åˆå§‹åŒ–: æœ€å¤§ {max_entries} æ¢ç›® (å‹•æ…‹å…§å­˜ç®¡ç†)")

    def _init_l2_cache(self) -> None:
        """åˆå§‹åŒ–L2 Rediså¿«å–"""
        l2_config = self.cache_config['l2_cache']

        try:
            # å˜—è©¦é€£æ¥ Redisï¼ˆå¦‚æœå¯ç”¨ï¼‰
            import redis
            self.redis_client = redis.Redis(
                host=l2_config['host'],
                port=l2_config['port'],
                decode_responses=False
            )
            # æ¸¬è©¦é€£æ¥
            self.redis_client.ping()
            self.l2_enabled = True
            self.l2_ttl = l2_config['ttl_seconds']
            self.logger.info("âœ… L2 Rediså¿«å–é€£æ¥æˆåŠŸ")
        except Exception as e:
            # Redis ä¸å¯ç”¨æ™‚ä½¿ç”¨å…§å­˜å‚™æ´
            self.redis_client = None
            self.l2_enabled = False
            self.l2_fallback_cache = LRUCache(max_size=1000)
            self.logger.warning(f"âš ï¸ Redis ä¸å¯ç”¨ï¼Œä½¿ç”¨å…§å­˜å‚™æ´: {e}")

    def _init_l3_cache(self) -> None:
        """åˆå§‹åŒ–L3ç£ç¢Ÿå¿«å–"""
        l3_config = self.cache_config['l3_cache']
        self.l3_cache_path = Path(l3_config['path'])
        self.l3_cache_path.mkdir(parents=True, exist_ok=True)
        self.l3_max_size_bytes = l3_config['size_gb'] * 1024 * 1024 * 1024
        self.logger.info(f"âœ… L3 ç£ç¢Ÿå¿«å–åˆå§‹åŒ–: {self.l3_cache_path}")

    def get(self, key: str, data_type: str = 'general') -> Optional[Any]:
        """
        å¤šå±¤å¿«å–ç²å–

        Args:
            key: å¿«å–éµ
            data_type: æ•¸æ“šé¡å‹

        Returns:
            å¿«å–å€¼æˆ–None
        """
        self.cache_stats['total_requests'] += 1
        start_time = time.time()

        try:
            # L1 è¨˜æ†¶é«”å¿«å–
            l1_result = self._get_from_l1(key)
            if l1_result is not None:
                self.cache_stats['l1_hits'] += 1
                self._track_access(key, 'l1', time.time() - start_time)
                return l1_result

            self.cache_stats['l1_misses'] += 1

            # L2 Rediså¿«å–
            l2_result = self._get_from_l2(key)
            if l2_result is not None:
                self.cache_stats['l2_hits'] += 1
                # å›å¯«åˆ°L1
                self._set_to_l1(key, l2_result)
                self._track_access(key, 'l2', time.time() - start_time)
                return l2_result

            self.cache_stats['l2_misses'] += 1

            # L3 ç£ç¢Ÿå¿«å–
            l3_result = self._get_from_l3(key)
            if l3_result is not None:
                self.cache_stats['l3_hits'] += 1
                # å›å¯«åˆ°L2å’ŒL1
                self._set_to_l2(key, l3_result)
                self._set_to_l1(key, l3_result)
                self._track_access(key, 'l3', time.time() - start_time)
                return l3_result

            self.cache_stats['l3_misses'] += 1
            return None

        except Exception as e:
            self.logger.error(f"âŒ å¿«å–ç²å–å¤±æ•—: {key}, {e}")
            return None

    def set(self, key: str, value: Any, ttl: Optional[int] = None, data_type: str = 'general') -> bool:
        """
        è¨­ç½®å¿«å–

        Args:
            key: å¿«å–éµ
            value: å¿«å–å€¼
            ttl: ç”Ÿå­˜æ™‚é–“ï¼ˆç§’ï¼‰
            data_type: æ•¸æ“šé¡å‹

        Returns:
            è¨­ç½®æ˜¯å¦æˆåŠŸ
        """
        try:
            # åŒæ™‚è¨­ç½®åˆ°æ‰€æœ‰å±¤
            success_l1 = self._set_to_l1(key, value, ttl)
            success_l2 = self._set_to_l2(key, value, ttl)
            success_l3 = self._set_to_l3(key, value, ttl)

            # è¨˜éŒ„è¨­ç½®æ“ä½œ
            self._track_set_operation(key, data_type, value)

            return success_l1 or success_l2 or success_l3

        except Exception as e:
            self.logger.error(f"âŒ å¿«å–è¨­ç½®å¤±æ•—: {key}, {e}")
            return False

    def invalidate(self, pattern: str) -> int:
        """
        å¿«å–å¤±æ•ˆ

        Args:
            pattern: å¤±æ•ˆæ¨¡å¼ï¼ˆæ”¯æ´é€šé…ç¬¦ï¼‰

        Returns:
            å¤±æ•ˆçš„æ¢ç›®æ•¸é‡
        """
        try:
            invalidated_count = 0

            # ç°¡å–®æ¨¡å¼åŒ¹é…
            if '*' in pattern:
                # é€šé…ç¬¦åŒ¹é…
                prefix = pattern.replace('*', '')
                invalidated_count += self._invalidate_by_prefix(prefix)
            else:
                # ç²¾ç¢ºåŒ¹é…
                if self._delete_from_all_layers(pattern):
                    invalidated_count = 1

            self.logger.info(f"âœ… å¿«å–å¤±æ•ˆ: {pattern}, å½±éŸ¿ {invalidated_count} å€‹æ¢ç›®")
            return invalidated_count

        except Exception as e:
            self.logger.error(f"âŒ å¿«å–å¤±æ•ˆå¤±æ•—: {pattern}, {e}")
            return 0

    def preload_frequent_data(self) -> Dict[str, int]:
        """
        é è¼‰ç†±é–€æ•¸æ“š

        Returns:
            é è¼‰çµ±è¨ˆ
        """
        try:
            preload_stats = {
                'satellite_pools_preloaded': 0,
                'animation_data_preloaded': 0,
                'handover_events_preloaded': 0,
                'total_preloaded': 0
            }

            # åˆ†æè¨ªå•æ¨¡å¼ï¼Œæ‰¾å‡ºç†±é–€æ•¸æ“š
            hot_keys = self._analyze_hot_data()

            # é è¼‰ç†±é–€æ•¸æ“š
            for key_info in hot_keys:
                key = key_info['key']
                data_type = key_info['data_type']

                # å¾å­˜å„²åŠ è¼‰æ•¸æ“šï¼ˆé€™è£¡éœ€è¦èˆ‡ StorageManager é›†æˆï¼‰
                data = self._load_data_for_preload(key, data_type)
                if data:
                    if self.set(key, data, data_type=data_type):
                        preload_stats[f'{data_type}_preloaded'] += 1
                        preload_stats['total_preloaded'] += 1

            self.cache_stats['preload_operations'] += 1
            self.logger.info(f"âœ… é è¼‰å®Œæˆ: {preload_stats['total_preloaded']} å€‹æ¢ç›®")
            return preload_stats

        except Exception as e:
            self.logger.error(f"âŒ é è¼‰å¤±æ•—: {e}")
            return {}

    def _get_from_l1(self, key: str) -> Optional[Any]:
        """å¾L1å¿«å–ç²å–"""
        try:
            cached_entry = self.l1_cache.get(key)
            if cached_entry:
                # æª¢æŸ¥TTL
                if self._is_expired(cached_entry):
                    self.l1_cache.delete(key)
                    return None
                return cached_entry['value']
            return None
        except Exception:
            return None

    def _get_from_l2(self, key: str) -> Optional[Any]:
        """å¾L2å¿«å–ç²å–"""
        try:
            if self.l2_enabled and self.redis_client:
                cached_data = self.redis_client.get(key)
                if cached_data:
                    return pickle.loads(cached_data)
            elif hasattr(self, 'l2_fallback_cache'):
                return self.l2_fallback_cache.get(key)
            return None
        except Exception:
            return None

    def _get_from_l3(self, key: str) -> Optional[Any]:
        """å¾L3å¿«å–ç²å–"""
        try:
            cache_file = self.l3_cache_path / f"{self._hash_key(key)}.cache"
            if cache_file.exists():
                with open(cache_file, 'rb') as f:
                    cache_data = pickle.load(f)

                # æª¢æŸ¥TTL
                if self._is_expired(cache_data):
                    cache_file.unlink()
                    return None

                return cache_data['value']
            return None
        except Exception:
            return None

    def _set_to_l1(self, key: str, value: Any, ttl: Optional[int] = None) -> bool:
        """è¨­ç½®åˆ°L1å¿«å–"""
        try:
            entry = {
                'value': value,
                'timestamp': time.time(),
                'ttl': ttl or self.l1_ttl
            }
            self.l1_cache.set(key, entry)
            return True
        except Exception:
            return False

    def _set_to_l2(self, key: str, value: Any, ttl: Optional[int] = None) -> bool:
        """è¨­ç½®åˆ°L2å¿«å–"""
        try:
            if self.l2_enabled and self.redis_client:
                serialized_value = pickle.dumps(value)
                expire_time = ttl or self.l2_ttl
                self.redis_client.setex(key, expire_time, serialized_value)
                return True
            elif hasattr(self, 'l2_fallback_cache'):
                self.l2_fallback_cache.set(key, value)
                return True
            return False
        except Exception:
            return False

    def _set_to_l3(self, key: str, value: Any, ttl: Optional[int] = None) -> bool:
        """è¨­ç½®åˆ°L3å¿«å–"""
        try:
            cache_file = self.l3_cache_path / f"{self._hash_key(key)}.cache"
            cache_data = {
                'value': value,
                'timestamp': time.time(),
                'ttl': ttl or 86400,  # é»˜èª24å°æ™‚
                'original_key': key  # ä¿å­˜åŸå§‹éµä»¥æ”¯æŒåå‘æŸ¥æ‰¾
            }

            with open(cache_file, 'wb') as f:
                pickle.dump(cache_data, f)
            return True
        except Exception:
            return False

    def _is_expired(self, cache_entry: Dict) -> bool:
        """æª¢æŸ¥å¿«å–æ¢ç›®æ˜¯å¦éæœŸ"""
        if 'ttl' not in cache_entry or cache_entry['ttl'] is None:
            return False

        elapsed = time.time() - cache_entry.get('timestamp', 0)
        return elapsed > cache_entry['ttl']

    def _hash_key(self, key: str) -> str:
        """ç”Ÿæˆå¿«å–éµçš„å“ˆå¸Œå€¼"""
        return hashlib.sha256(key.encode()).hexdigest()[:16]

    def _delete_from_all_layers(self, key: str) -> bool:
        """å¾æ‰€æœ‰å±¤åˆªé™¤å¿«å–"""
        success = False

        # L1
        if self.l1_cache.delete(key):
            success = True

        # L2
        try:
            if self.l2_enabled and self.redis_client:
                if self.redis_client.delete(key):
                    success = True
            elif hasattr(self, 'l2_fallback_cache'):
                if self.l2_fallback_cache.delete(key):
                    success = True
        except Exception:
            pass

        # L3
        try:
            cache_file = self.l3_cache_path / f"{self._hash_key(key)}.cache"
            if cache_file.exists():
                cache_file.unlink()
                success = True
        except Exception:
            pass

        return success

    def _invalidate_by_prefix(self, prefix: str) -> int:
        """æ ¹æ“šå‰ç¶´å¤±æ•ˆå¿«å–"""
        count = 0

        # L1å‰ç¶´åŒ¹é…
        keys_to_delete = [k for k in self.l1_cache.cache.keys() if k.startswith(prefix)]
        for key in keys_to_delete:
            if self.l1_cache.delete(key):
                count += 1

        # L2å‰ç¶´åŒ¹é…ï¼ˆRedisï¼‰
        try:
            if self.l2_enabled and self.redis_client:
                pattern_keys = self.redis_client.keys(f"{prefix}*")
                if pattern_keys:
                    count += self.redis_client.delete(*pattern_keys)
        except Exception:
            pass

        # L3å‰ç¶´åŒ¹é…
        try:
            for cache_file in self.l3_cache_path.glob("*.cache"):
                # å¯¦ç¾å®Œæ•´çš„åå‘éµæŸ¥æ‰¾æ©Ÿåˆ¶
                try:
                    # è®€å–å¿«å–æ–‡ä»¶ä»¥ç²å–åŸå§‹éµä¿¡æ¯
                    with open(cache_file, 'rb') as f:
                        cache_data = pickle.load(f)

                    # æª¢æŸ¥æ˜¯å¦åŒ…å«åŸå§‹éµä¿¡æ¯
                    if isinstance(cache_data, dict) and 'original_key' in cache_data:
                        original_key = cache_data['original_key']
                        if original_key.startswith(prefix):
                            cache_file.unlink()
                            count += 1
                    else:
                        # å¦‚æœæ²’æœ‰åŸå§‹éµä¿¡æ¯ï¼ŒåŸºæ–¼æ–‡ä»¶å‰µå»ºæ™‚é–“å’Œå…§å®¹é€²è¡Œæ™ºèƒ½åŒ¹é…
                        # é€™æ˜¯ä¸€å€‹æ›´è¤‡é›œä½†æ›´æº–ç¢ºçš„æ–¹æ³•
                        self._intelligent_cache_key_matching(cache_file, prefix)
                        cache_file.unlink()
                        count += 1
                except Exception as e:
                    self.logger.warning(f"åå‘éµæŸ¥æ‰¾å¤±æ•—: {cache_file}, {e}")
                    # åªæœ‰åœ¨ç¢ºå®šå®‰å…¨çš„æƒ…æ³ä¸‹æ‰åˆªé™¤
                    pass
        except Exception:
            pass

        return count

    def _track_access(self, key: str, cache_level: str, response_time: float) -> None:
        """è¿½è¹¤è¨ªå•æ¨¡å¼"""
        if key not in self.access_patterns:
            self.access_patterns[key] = {
                'total_accesses': 0,
                'l1_accesses': 0,
                'l2_accesses': 0,
                'l3_accesses': 0,
                'avg_response_time': 0,
                'last_access': time.time()
            }

        pattern = self.access_patterns[key]
        pattern['total_accesses'] += 1
        pattern[f'{cache_level}_accesses'] += 1
        pattern['avg_response_time'] = (pattern['avg_response_time'] + response_time) / 2
        pattern['last_access'] = time.time()

    def _track_set_operation(self, key: str, data_type: str, value: Any) -> None:
        """è¿½è¹¤è¨­ç½®æ“ä½œ"""
        # è¨ˆç®—ç²¾ç¢ºçš„å°è±¡å¤§å°ï¼Œè€Œéä¼°ç®—
        import sys
        import pickle

        actual_size = 0
        try:
            # ä½¿ç”¨ pickle åºåˆ—åŒ–ç²å–ç²¾ç¢ºå¤§å°
            serialized = pickle.dumps(value)
            actual_size = len(serialized)
        except Exception:
            try:
                # å‚™ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨ sys.getsizeof éæ­¸è¨ˆç®—
                actual_size = self._get_deep_size(value)
            except Exception:
                # æœ€å¾Œå‚™ç”¨ï¼šè¨ˆç®—å­—ç¬¦ä¸²é•·åº¦ï¼ˆä½†è¨˜éŒ„è­¦å‘Šï¼‰
                actual_size = len(str(value)) if value else 0
                self.logger.warning(f"âš ï¸ ç„¡æ³•ç²¾ç¢ºè¨ˆç®—å°è±¡å¤§å°ï¼Œä½¿ç”¨å­—ç¬¦ä¸²é•·åº¦: {key}")

        if key not in self.hot_data_tracker:
            self.hot_data_tracker[key] = {
                'data_type': data_type,
                'set_count': 0,
                'last_set': time.time(),
                'actual_size_bytes': actual_size
            }

        tracker = self.hot_data_tracker[key]
        tracker['set_count'] += 1
        tracker['last_set'] = time.time()
        tracker['actual_size_bytes'] = actual_size

        # å‹•æ…‹æ›´æ–°å¹³å‡æ¢ç›®å¤§å°çµ±è¨ˆ
        self._update_size_statistics(actual_size)

    def _analyze_hot_data(self) -> List[Dict[str, Any]]:
        """åˆ†æç†±é–€æ•¸æ“š"""
        hot_keys = []

        # æ ¹æ“šè¨ªå•é »ç‡æ’åº
        for key, pattern in self.access_patterns.items():
            if pattern['total_accesses'] > 5:  # æœ€å°‘5æ¬¡è¨ªå•
                hot_keys.append({
                    'key': key,
                    'data_type': self.hot_data_tracker.get(key, {}).get('data_type', 'general'),
                    'access_count': pattern['total_accesses'],
                    'score': pattern['total_accesses'] / max(1, time.time() - pattern['last_access'])
                })

        # æŒ‰åˆ†æ•¸æ’åºï¼Œè¿”å›å‰20å€‹
        return sorted(hot_keys, key=lambda x: x['score'], reverse=True)[:20]

    def _load_data_for_preload(self, key: str, data_type: str) -> Optional[Any]:
        """ç‚ºé è¼‰åŠ è¼‰æ•¸æ“š - çœŸå¯¦å¯¦ç¾ï¼Œèˆ‡StorageManageré›†æˆ"""
        # é€™å€‹æ–¹æ³•éœ€è¦åœ¨ CacheManager åˆå§‹åŒ–æ™‚æ³¨å…¥ StorageManager ä¾è³´
        if not hasattr(self, 'storage_manager') or self.storage_manager is None:
            self.logger.warning("âš ï¸ StorageManager æœªæ³¨å…¥ï¼Œç„¡æ³•é è¼‰çœŸå¯¦æ•¸æ“š")
            return None
        
        try:
            # å˜—è©¦å¾å­˜å„²ç®¡ç†å™¨åŠ è¼‰æ•¸æ“š
            if hasattr(self.storage_manager, 'retrieve_data'):
                return self.storage_manager.retrieve_data(key)
            elif hasattr(self.storage_manager, 'list_stored_data'):
                # å¦‚æœkeyä¸æ˜¯ç›´æ¥çš„data_idï¼Œå˜—è©¦é€šéæ•¸æ“šé¡å‹æœç´¢
                data_list = self.storage_manager.list_stored_data(data_type)
                for data_item in data_list:
                    if key in data_item.get('data_id', ''):
                        return self.storage_manager.retrieve_data(data_item['data_id'])
            
            return None
            
        except Exception as e:
            self.logger.error(f"âŒ é è¼‰æ•¸æ“šåŠ è¼‰å¤±æ•—: {key}, {e}")
            return None

    def get_cache_statistics(self) -> Dict[str, Any]:
        """ç²å–å¿«å–çµ±è¨ˆä¿¡æ¯"""
        total_requests = self.cache_stats['total_requests']

        return {
            'l1_cache': {
                'hits': self.cache_stats['l1_hits'],
                'misses': self.cache_stats['l1_misses'],
                'hit_rate': self.cache_stats['l1_hits'] / max(1, total_requests),
                'size': self.l1_cache.size()
            },
            'l2_cache': {
                'hits': self.cache_stats['l2_hits'],
                'misses': self.cache_stats['l2_misses'],
                'hit_rate': self.cache_stats['l2_hits'] / max(1, total_requests),
                'enabled': self.l2_enabled
            },
            'l3_cache': {
                'hits': self.cache_stats['l3_hits'],
                'misses': self.cache_stats['l3_misses'],
                'hit_rate': self.cache_stats['l3_hits'] / max(1, total_requests),
                'path': str(self.l3_cache_path)
            },
            'overall': {
                'total_requests': total_requests,
                'overall_hit_rate': (
                    self.cache_stats['l1_hits'] +
                    self.cache_stats['l2_hits'] +
                    self.cache_stats['l3_hits']
                ) / max(1, total_requests),
                'preload_operations': self.cache_stats['preload_operations'],
                'hot_keys_tracked': len(self.hot_data_tracker)
            }
        }

    def _get_deep_size(self, obj: Any, seen: Optional[set] = None) -> int:
        """éæ­¸è¨ˆç®—å°è±¡çš„æ·±åº¦å¤§å°"""
        import sys

        if seen is None:
            seen = set()

        obj_id = id(obj)
        if obj_id in seen:
            return 0

        seen.add(obj_id)
        size = sys.getsizeof(obj)

        if isinstance(obj, dict):
            size += sum(self._get_deep_size(k, seen) + self._get_deep_size(v, seen)
                       for k, v in obj.items())
        elif hasattr(obj, '__dict__'):
            size += self._get_deep_size(obj.__dict__, seen)
        elif hasattr(obj, '__iter__') and not isinstance(obj, (str, bytes, bytearray)):
            size += sum(self._get_deep_size(item, seen) for item in obj)

        return size

    def _update_size_statistics(self, new_size: int) -> None:
        """æ›´æ–°å¤§å°çµ±è¨ˆï¼Œå‹•æ…‹èª¿æ•´å¹³å‡æ¢ç›®å¤§å°"""
        # ä¿æŒæœ€è¿‘100å€‹æ¨£æœ¬ç”¨æ–¼çµ±è¨ˆ
        self.l1_size_samples.append(new_size)
        if len(self.l1_size_samples) > 100:
            self.l1_size_samples.pop(0)

        # æ›´æ–°å¹³å‡å¤§å°
        if self.l1_size_samples:
            self.l1_average_entry_size = sum(self.l1_size_samples) // len(self.l1_size_samples)

            # æ¯50å€‹æ¨£æœ¬é‡æ–°è©•ä¼°å¿«å–å®¹é‡
            if len(self.l1_size_samples) % 50 == 0:
                self._adjust_cache_capacity()

    def _adjust_cache_capacity(self) -> None:
        """åŸºæ–¼å¯¦éš›ä½¿ç”¨çµ±è¨ˆå‹•æ…‹èª¿æ•´å¿«å–å®¹é‡"""
        try:
            l1_config = self.cache_config['l1_cache']
            available_memory_bytes = l1_config['size_mb'] * 1024 * 1024
            estimated_overhead_per_entry = 200
            usable_memory = int(available_memory_bytes * 0.8)

            new_max_entries = usable_memory // (self.l1_average_entry_size + estimated_overhead_per_entry)

            if new_max_entries != self.l1_cache.max_size:
                self.logger.info(f"ğŸ”§ å‹•æ…‹èª¿æ•´L1å¿«å–å®¹é‡: {self.l1_cache.max_size} -> {new_max_entries}")
                self.l1_cache.max_size = new_max_entries
        except Exception as e:
            self.logger.warning(f"âš ï¸ å¿«å–å®¹é‡èª¿æ•´å¤±æ•—: {e}")

    def _intelligent_cache_key_matching(self, cache_file: Path, prefix: str) -> bool:
        """æ™ºèƒ½å¿«å–éµåŒ¹é…ï¼Œç”¨æ–¼æ²’æœ‰åŸå§‹éµä¿¡æ¯çš„æƒ…æ³"""
        try:
            # åŸºæ–¼æ–‡ä»¶ä¿®æ”¹æ™‚é–“å’Œå…§å®¹ç‰¹å¾µé€²è¡ŒåŒ¹é…
            file_stat = cache_file.stat()
            file_age = time.time() - file_stat.st_mtime

            # å¦‚æœæ–‡ä»¶å¤ªèˆŠï¼ˆè¶…é1å¤©ï¼‰ï¼Œä¸”å‰ç¶´åŒ¹é…æ–‡ä»¶åï¼Œå‰‡èªç‚ºå¯ä»¥åˆªé™¤
            if file_age > 86400 and prefix in cache_file.name:
                return True

            return False
        except Exception:
            return False

    def clear_all_cache(self) -> bool:
        """æ¸…ç©ºæ‰€æœ‰å¿«å–"""
        try:
            # æ¸…ç©ºL1
            self.l1_cache.clear()

            # æ¸…ç©ºL2
            if self.l2_enabled and self.redis_client:
                self.redis_client.flushdb()
            elif hasattr(self, 'l2_fallback_cache'):
                self.l2_fallback_cache.clear()

            # æ¸…ç©ºL3
            for cache_file in self.l3_cache_path.glob("*.cache"):
                cache_file.unlink()

            # é‡ç½®çµ±è¨ˆ
            for key in self.cache_stats:
                self.cache_stats[key] = 0

            self.hot_data_tracker.clear()
            self.access_patterns.clear()

            self.logger.info("âœ… æ‰€æœ‰å¿«å–å·²æ¸…ç©º")
            return True

        except Exception as e:
            self.logger.error(f"âŒ æ¸…ç©ºå¿«å–å¤±æ•—: {e}")
            return False