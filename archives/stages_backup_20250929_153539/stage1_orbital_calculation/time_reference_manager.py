#!/usr/bin/env python3
"""
Stage 1: Time Reference Manager Component (v2.0 Architecture)

å°ˆè·è²¬ä»»ï¼š
- TLE Epochæ™‚é–“è§£æå’Œæ¨™æº–åŒ–
- æ™‚é–“åŸºæº–å»ºç«‹å’Œé©—è­‰
- æ™‚é–“ç²¾åº¦ç®¡ç†å’Œæ ¼å¼è½‰æ›
- å­¸è¡“ç´šæ™‚é–“æ¨™æº–åˆè¦

v2.0é‡æ§‹åŸå‰‡ï¼š
- å–®ä¸€è²¬ä»»åŸå‰‡ï¼šå°ˆé–€è² è²¬æ™‚é–“åŸºæº–ç®¡ç†
- å­¸è¡“æ¨™æº–åˆè¦ï¼šæ™‚é–“ç²¾åº¦å’Œæ ¼å¼è¦æ±‚
- çµ±ä¸€æ™‚é–“æ¥å£ï¼šç‚ºå¾ŒçºŒéšæ®µæä¾›æ¨™æº–æ™‚é–“åŸºæº–
"""

import logging
from datetime import datetime, timezone, timedelta
from typing import Dict, Any, List, Optional, Tuple
import math

# å…±äº«æ¨¡çµ„å°å…¥
from shared.utils import TimeUtils
from shared.constants import OrbitEngineConstantsManager

logger = logging.getLogger(__name__)


class TimeReferenceManager:
    """
    Stage 1: æ™‚é–“åŸºæº–ç®¡ç†å™¨ (v2.0æ¶æ§‹)

    å°ˆè·è²¬ä»»ï¼š
    1. TLE Epochæ™‚é–“è§£æå’Œæ¨™æº–åŒ–
    2. æ™‚é–“åŸºæº–å»ºç«‹å’ŒUTCå°é½Š
    3. æ™‚é–“ç²¾åº¦é©—è­‰å’Œå“è³ªä¿è­‰
    4. å¤šæ ¼å¼æ™‚é–“è¼¸å‡ºæ”¯æ´
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}

        # åˆå§‹åŒ–çµ„ä»¶
        self.time_utils = TimeUtils()
        self.system_constants = OrbitEngineConstantsManager()

        # æ™‚é–“ç²¾åº¦é…ç½® (åŸºæ–¼å­¸è¡“æ¨™æº–)
        from shared.constants.tle_constants import TLEConstants
        from shared.constants.academic_standards import AcademicValidationStandards

        self.time_precision = {
            'tle_epoch_precision_seconds': TLEConstants.TLE_REALISTIC_TIME_PRECISION_SECONDS,
            'utc_standard_tolerance_ms': 1000.0,  # 1ç§’å®¹å·® (åˆç†çš„UTCåŒæ­¥è¦æ±‚)
            'max_time_drift_days': TLEConstants.TLE_FRESHNESS_ACCEPTABLE_DAYS,
            'require_utc_alignment': True
        }

        # æ™‚é–“è™•ç†çµ±è¨ˆ
        self.time_stats = {
            'total_epochs_processed': 0,
            'parsing_errors': 0,
            'precision_warnings': 0,
            'time_drift_warnings': 0,
            'utc_alignment_issues': 0
        }

        self.logger = logging.getLogger(f"{__name__}.TimeReferenceManager")
        self.logger.info("Stage 1 æ™‚é–“åŸºæº–ç®¡ç†å™¨å·²åˆå§‹åŒ–")

    def establish_time_reference(self, tle_data_list: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        å»ºç«‹å€‹åˆ¥TLEè¨˜éŒ„çš„æ™‚é–“åŸºæº– (ç¬¦åˆå­¸è¡“æ¨™æº–)

        âš ï¸ é‡è¦ï¼šæ¯ç­†TLEè¨˜éŒ„ä¿æŒç¨ç«‹çš„epochæ™‚é–“ï¼Œä¸å‰µå»ºçµ±ä¸€æ™‚é–“åŸºæº–
        æ ¹æ“šå­¸è¡“æ¨™æº–ï¼ŒTLEæ–‡ä»¶åŒ…å«å¤šå¤©æ•¸æ“šï¼Œæ¯ç­†è¨˜éŒ„æœ‰ä¸åŒepochæ™‚é–“ï¼Œ
        çµ±ä¸€æ™‚é–“åŸºæº–æœƒå°è‡´è»Œé“è¨ˆç®—èª¤å·®ã€‚

        Args:
            tle_data_list: TLEæ•¸æ“šåˆ—è¡¨

        Returns:
            æ™‚é–“åŸºæº–å»ºç«‹çµæœ (ä¿æŒå€‹åˆ¥epochæ™‚é–“)
        """
        self.logger.info(f"â° å»ºç«‹{len(tle_data_list)}ç­†TLEæ•¸æ“šçš„å€‹åˆ¥æ™‚é–“åŸºæº–...")

        time_reference_result = {
            'time_reference_established': False,
            'individual_epoch_processing': True,  # æ¨™è¨˜ä½¿ç”¨å€‹åˆ¥epochè™•ç†
            'epoch_time_range': {},
            'standardized_data': [],
            'time_quality_metrics': {},
            'processing_metadata': {
                'reference_timestamp': datetime.now(timezone.utc).isoformat(),
                'time_standard': 'UTC',
                'precision_level': 'microsecond',
                'manager_version': '2.1.0',
                'academic_compliance': 'individual_epoch_based'
            }
        }

        if not tle_data_list:
            self.logger.warning("æ•¸æ“šé›†ç‚ºç©ºï¼Œç„¡æ³•å»ºç«‹æ™‚é–“åŸºæº–")
            return time_reference_result

        # è§£ææ‰€æœ‰TLE Epochæ™‚é–“
        epoch_times = []
        standardized_data = []

        for idx, tle_data in enumerate(tle_data_list):
            try:
                # è§£æTLE Epochæ™‚é–“
                epoch_result = self._parse_tle_epoch(tle_data)

                if epoch_result['parsing_success']:
                    epoch_times.append(epoch_result['epoch_datetime'])

                    # æ·»åŠ æ¨™æº–åŒ–æ™‚é–“ä¿¡æ¯
                    enhanced_tle = tle_data.copy()
                    enhanced_tle.update({
                        'epoch_datetime': epoch_result['epoch_datetime'].isoformat(),
                        'epoch_year_full': epoch_result['epoch_year_full'],
                        'epoch_day_decimal': epoch_result['epoch_day_decimal'],
                        'epoch_precision_seconds': epoch_result['precision_seconds'],
                        'time_reference_standard': 'tle_epoch_utc',
                        'time_quality_grade': epoch_result['quality_grade']
                    })

                    standardized_data.append(enhanced_tle)
                    self.time_stats['total_epochs_processed'] += 1
                else:
                    # æ¨™è¨˜è§£æéŒ¯èª¤ä½†ä¿ç•™æ•¸æ“š
                    enhanced_tle = tle_data.copy()
                    enhanced_tle.update({
                        'time_reference_error': epoch_result['error_message'],
                        'time_quality_grade': 'F'
                    })
                    standardized_data.append(enhanced_tle)
                    self.time_stats['parsing_errors'] += 1

            except Exception as e:
                self.logger.error(f"è™•ç†ç¬¬{idx}ç­†TLEæ™‚é–“æ•¸æ“šå¤±æ•—: {e}")
                enhanced_tle = tle_data.copy()
                enhanced_tle['time_reference_error'] = str(e)
                standardized_data.append(enhanced_tle)
                self.time_stats['parsing_errors'] += 1

        # å»ºç«‹å€‹åˆ¥æ™‚é–“åŸºæº–è¨˜éŒ„ (ä¸å‰µå»ºçµ±ä¸€åŸºæº–)
        if epoch_times:
            time_reference_result.update({
                'time_reference_established': True,
                'epoch_time_range': {
                    'earliest': min(epoch_times).isoformat(),
                    'latest': max(epoch_times).isoformat(),
                    'span_days': (max(epoch_times) - min(epoch_times)).days,
                    'total_individual_epochs': len(epoch_times)
                },
                'standardized_data': standardized_data,
                'academic_compliance_note': 'æ¯ç­†TLEè¨˜éŒ„ä¿æŒç¨ç«‹epochæ™‚é–“ï¼Œç¬¦åˆå­¸è¡“æ¨™æº–'
            })

            # ç”Ÿæˆæ™‚é–“å“è³ªåº¦é‡
            time_reference_result['time_quality_metrics'] = self._generate_time_quality_metrics(epoch_times)

            self.logger.info(f"âœ… å€‹åˆ¥æ™‚é–“åŸºæº–å»ºç«‹å®Œæˆï¼Œè™•ç†{len(epoch_times)}å€‹ç¨ç«‹epoch (ç„¡çµ±ä¸€åŸºæº–)")
        else:
            self.logger.error("âŒ ç„¡æ³•å»ºç«‹æ™‚é–“åŸºæº–ï¼Œæ²’æœ‰æœ‰æ•ˆçš„epochæ™‚é–“")

        return time_reference_result

    def _parse_tle_epoch(self, tle_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        è§£æå–®å€‹TLEçš„Epochæ™‚é–“

        Args:
            tle_data: TLEæ•¸æ“š

        Returns:
            Epochè§£æçµæœ
        """
        parse_result = {
            'parsing_success': False,
            'epoch_datetime': None,
            'epoch_year_full': None,
            'epoch_day_decimal': None,
            'precision_seconds': None,
            'quality_grade': 'F',
            'error_message': None
        }

        try:
            line1 = tle_data.get('line1', '')
            if len(line1) < 32:
                parse_result['error_message'] = "TLE Line1é•·åº¦ä¸è¶³"
                return parse_result

            # æå–epochå¹´ä»½å’Œå¤©æ•¸
            epoch_year = int(line1[18:20])
            epoch_day_str = line1[20:32]
            epoch_day = float(epoch_day_str)

            # è½‰æ›ç‚ºUTCæ™‚é–“ (ä½¿ç”¨çµ±ä¸€çš„TLEæ¨™æº–)
            epoch_datetime = self.time_utils.parse_tle_epoch(epoch_year, epoch_day)

            # ç²å–å®Œæ•´å¹´ä»½ç”¨æ–¼è¨˜éŒ„
            from shared.constants.tle_constants import convert_tle_year_to_full_year
            full_year = convert_tle_year_to_full_year(epoch_year)

            # è¨ˆç®—å¯¦éš›æ™‚é–“ç²¾åº¦ (åŸºæ–¼TLEæ•¸æ“šç‰¹æ€§å’Œè»Œé“åŠ›å­¸é™åˆ¶)
            from shared.constants.academic_standards import AcademicValidationStandards
            from shared.constants.tle_constants import TLEConstants

            # 1. åˆ†æå°æ•¸ä½æ•¸ (åƒ…ä½œç‚ºåƒè€ƒï¼Œä¸ä½œç‚ºç²¾åº¦æŒ‡æ¨™)
            decimal_places = len(epoch_day_str.split('.')[-1]) if '.' in epoch_day_str else 0

            # 2. åŸºæ–¼å­¸è¡“ç ”ç©¶çš„å¯¦éš›ç²¾åº¦è©•ä¼°
            # TLEç²¾åº¦å—ä»¥ä¸‹å› ç´ é™åˆ¶ï¼š
            # - è»Œé“é æ¸¬æ¨¡å‹èª¤å·® (SGP4/SDP4)
            # - è§€æ¸¬æ•¸æ“šè³ªé‡
            # - å¤§æ°£é˜»åŠ›è®ŠåŒ–çš„ä¸å¯é æ¸¬æ€§
            # - å¤ªé™½è¼»å°„å£“åŠ›è®ŠåŒ–

            # æ ¹æ“šå­¸è¡“æ¨™æº–ï¼ŒTLEçš„å¯¦éš›æ™‚é–“ç²¾åº¦ç´„ç‚º1åˆ†é˜ç´šåˆ¥
            precision_seconds = TLEConstants.TLE_REALISTIC_TIME_PRECISION_SECONDS

            # 3. åŸºæ–¼å¯¦éš›è»Œé“åŠ›å­¸åŸç†çš„ç²¾åº¦è¨ˆç®—
            current_time = datetime.now(timezone.utc)
            data_age_days = (current_time - epoch_datetime).days

            # åŸºæ–¼è»Œé“åŠ›å­¸ç†è«–ï¼Œé æ¸¬èª¤å·®éš¨æ™‚é–“éç·šæ€§å¢é•·
            # ä½¿ç”¨å¯¦éš›ç‰©ç†æ¨¡å‹è€Œéä¼°è¨ˆå€¼
            from shared.constants.physics_constants import PhysicsConstants
            orbit_uncertainty_growth = PhysicsConstants.calculate_orbit_prediction_error_growth(data_age_days)

            # ä½¿ç”¨å¯¦éš›è¨ˆç®—çš„ç²¾åº¦ï¼Œè€Œéé è¨­å› å­
            precision_seconds = max(precision_seconds, orbit_uncertainty_growth)

            # æ™‚é–“å“è³ªè©•ä¼°
            quality_grade = self._assess_time_quality(epoch_datetime, precision_seconds)

            # åŸºæ–¼å­¸è¡“æ¨™æº–è©•ä¼°æ•¸æ“šæ–°é®®åº¦å°å“è³ªçš„å½±éŸ¿
            current_time = datetime.now(timezone.utc)
            age_days = (current_time - epoch_datetime).days

            from shared.constants.academic_standards import assess_tle_data_quality
            freshness_assessment = assess_tle_data_quality(age_days)

            # æ ¹æ“šæ–°é®®åº¦èª¿æ•´å“è³ªç­‰ç´š
            if age_days > self.time_precision['max_time_drift_days']:
                self.time_stats['time_drift_warnings'] += 1

                # åŸºæ–¼å­¸è¡“æ¨™æº–çš„å“è³ªé™ç´š
                if freshness_assessment['quality_level'] in ['poor', 'outdated']:
                    if quality_grade in ['A+', 'A', 'A-']:
                        quality_grade = 'C'  # é¡¯è‘—é™ç´š
                    elif quality_grade in ['B+', 'B']:
                        quality_grade = 'C-'  # é™ç´šåˆ°åŠæ ¼ç·š

            parse_result.update({
                'parsing_success': True,
                'epoch_datetime': epoch_datetime,
                'epoch_year_full': full_year,
                'epoch_day_decimal': epoch_day,
                'precision_seconds': precision_seconds,
                'quality_grade': quality_grade
            })

        except ValueError as e:
            parse_result['error_message'] = f"æ•¸å€¼è§£æéŒ¯èª¤: {e}"
        except Exception as e:
            parse_result['error_message'] = f"æœªçŸ¥éŒ¯èª¤: {e}"

        return parse_result

    def _assess_time_quality(self, epoch_datetime: datetime, precision_seconds: float) -> str:
        """
        è©•ä¼°æ™‚é–“å“è³ªç­‰ç´š (åŸºæ–¼å­¸è¡“æ¨™æº–å’Œè»Œé“åŠ›å­¸åŸç†)
        """
        from shared.constants.academic_standards import AcademicValidationStandards

        # åŸºæ–¼TLEæ™‚é–“ç²¾åº¦æ¨™æº–é€²è¡Œè©•ä¼°
        time_standards = AcademicValidationStandards.TIME_PRECISION_STANDARDS

        if precision_seconds <= time_standards['ultra_high']['precision_seconds']:
            return 'A+'
        elif precision_seconds <= time_standards['high']['precision_seconds']:
            return 'A'
        elif precision_seconds <= time_standards['medium']['precision_seconds']:
            return 'B+'
        elif precision_seconds <= time_standards['low']['precision_seconds']:
            return 'B'
        else:
            return 'C'

    def _generate_time_quality_metrics(self, epoch_times: List[datetime]) -> Dict[str, Any]:
        """
        ç”Ÿæˆæ™‚é–“å“è³ªåº¦é‡
        ğŸ“ Grade Aå­¸è¡“æ¨™æº–ï¼šåŸºæ–¼æ•¸æ“šå…§åœ¨æ™‚é–“åˆ†ä½ˆç‰¹æ€§ï¼Œä¸ä¾è³´åŸ·è¡Œæ™‚é–“
        """
        if not epoch_times:
            return {}

        # åŸºæ–¼æ•¸æ“šå…§åœ¨ç‰¹æ€§çš„åº¦é‡
        time_span = (max(epoch_times) - min(epoch_times))
        
        metrics = {
            'total_epochs': len(epoch_times),
            'time_span_days': time_span.days,
            'time_span_hours': time_span.total_seconds() / 3600,
            'epoch_density': len(epoch_times) / max(1, time_span.days),  # epochs per day
            'temporal_distribution_quality': self._assess_temporal_distribution(epoch_times),
            'time_continuity_score': self._calculate_time_continuity_score(epoch_times),
            'precision_assessment': self._assess_overall_precision(epoch_times)
        }
        
        # è¨ˆç®—åŸºæ–¼æ•¸æ“šå…§åœ¨ç‰¹æ€§çš„å“è³ªåˆ†æ•¸
        distribution_score = metrics['temporal_distribution_quality']
        continuity_score = metrics['time_continuity_score']
        precision_score = metrics['precision_assessment']['overall_score']
        density_score = min(100, metrics['epoch_density'] * 10)  # normalize density
        
        metrics['overall_time_quality_score'] = (
            distribution_score * 0.3 + 
            continuity_score * 0.3 + 
            precision_score * 0.3 + 
            density_score * 0.1
        )

        return metrics

    def _assess_temporal_distribution(self, epoch_times: List[datetime]) -> float:
        """
        è©•ä¼°æ™‚é–“åˆ†ä½ˆå“è³ª
        ğŸ“ å­¸è¡“ç´šæ¨™æº–ï¼šé©æ‡‰çœŸå¯¦TLEæ•¸æ“šçš„æ™‚é–“åˆ†ä½ˆç‰¹æ€§ï¼Œé‡è¦–æ•¸æ“šå®Œæ•´æ€§å’Œæ–°é®®åº¦

        Args:
            epoch_times: epochæ™‚é–“åˆ—è¡¨

        Returns:
            æ™‚é–“åˆ†ä½ˆå“è³ªè©•åˆ† (0-100)
        """
        if len(epoch_times) < 2:
            return 100.0

        sorted_epochs = sorted(epoch_times)

        # ğŸ¯ é‡å°TLEæ•¸æ“šç‰¹æ€§ï¼šæŒ‰æ—¥æœŸåˆ†çµ„çµ±è¨ˆ
        daily_counts = {}
        for epoch in sorted_epochs:
            date_key = epoch.strftime('%Y-%m-%d')
            daily_counts[date_key] = daily_counts.get(date_key, 0) + 1

        total_satellites = len(sorted_epochs)
        dates = sorted(daily_counts.keys())

        # è©•ä¼°æœ€æ–°æ•¸æ“šçš„é›†ä¸­åº¦ï¼ˆé€™å°TLEæ•¸æ“šæ˜¯å¥½äº‹ï¼‰
        latest_2_days = dates[-2:] if len(dates) >= 2 else dates
        recent_count = sum(daily_counts[date] for date in latest_2_days)
        recent_ratio = recent_count / total_satellites

        # è©•ä¼°æ•¸æ“šè¦†è“‹å¤©æ•¸ï¼ˆåˆç†ç¯„åœå…§ï¼‰
        time_span_days = (sorted_epochs[-1] - sorted_epochs[0]).days + 1

        # ğŸ“ å­¸è¡“ç´šè©•åˆ†ï¼šé‡è¦–æ•¸æ“šæ–°é®®åº¦å’Œåˆç†åˆ†ä½ˆ
        if recent_ratio >= 0.8:  # 80%æ•¸æ“šåœ¨æœ€è¿‘2å¤©
            if time_span_days <= 7:  # ä¸”æ™‚é–“è·¨åº¦åˆç†
                distribution_score = 95.0
            else:
                distribution_score = 88.0
        elif recent_ratio >= 0.6:  # 60%æ•¸æ“šåœ¨æœ€è¿‘2å¤©
            distribution_score = 85.0
        elif recent_ratio >= 0.4:  # 40%æ•¸æ“šåœ¨æœ€è¿‘2å¤©
            distribution_score = 80.0
        else:
            # æ•¸æ“šéæ–¼åˆ†æ•£ï¼Œä½†ä»çµ¦äºˆåŸºæœ¬åˆ†æ•¸
            distribution_score = max(70.0, 75.0 - time_span_days * 2)

        return distribution_score

    def _calculate_time_continuity_score(self, epoch_times: List[datetime]) -> float:
        """
        è¨ˆç®—æ™‚é–“é€£çºŒæ€§åˆ†æ•¸
        ğŸ“ å­¸è¡“ç´šæ¨™æº–ï¼šé©æ‡‰TLEæ•¸æ“šçš„å¯¦éš›æ›´æ–°é »ç‡ç‰¹æ€§
        """
        if len(epoch_times) <= 1:
            return 100.0

        sorted_times = sorted(epoch_times)

        # ğŸ¯ æŒ‰æ—¥æœŸåˆ†çµ„è©•ä¼°é€£çºŒæ€§
        daily_counts = {}
        for epoch in sorted_times:
            date_key = epoch.strftime('%Y-%m-%d')
            daily_counts[date_key] = daily_counts.get(date_key, 0) + 1

        dates = sorted(daily_counts.keys())
        time_span_days = (sorted_times[-1] - sorted_times[0]).days + 1

        # è©•ä¼°æ•¸æ“šè¦†è“‹ç‡ï¼ˆæœ‰æ•¸æ“šçš„å¤©æ•¸ / ç¸½æ™‚é–“è·¨åº¦ï¼‰
        data_coverage_ratio = len(dates) / time_span_days

        # ğŸ“ å­¸è¡“ç´šè©•åˆ†ï¼šé‡è¦–æœ€æ–°æ•¸æ“šçš„å®Œæ•´æ€§
        if time_span_days <= 3:
            # 3å¤©å…§æ•¸æ“šï¼šé‡è¦–è¦†è“‹å®Œæ•´æ€§
            if data_coverage_ratio >= 0.8:
                return 95.0
            elif data_coverage_ratio >= 0.6:
                return 90.0
            else:
                return 85.0
        elif time_span_days <= 7:
            # 1é€±å…§æ•¸æ“šï¼šé©åº¦è¦æ±‚è¦†è“‹ç‡
            if data_coverage_ratio >= 0.5:
                return 90.0
            elif data_coverage_ratio >= 0.3:
                return 85.0
            else:
                return 80.0
        else:
            # è¶…é1é€±ï¼šé‡é»è©•ä¼°æœ€æ–°æ•¸æ“šå¯†åº¦
            recent_3_days = dates[-3:] if len(dates) >= 3 else dates
            recent_satellites = sum(daily_counts[date] for date in recent_3_days)
            recent_density = recent_satellites / len(sorted_times)

            if recent_density >= 0.7:
                return 85.0
            elif recent_density >= 0.5:
                return 80.0
            else:
                return 75.0

    def _assess_overall_precision(self, epoch_times: List[datetime]) -> Dict[str, Any]:
        """è©•ä¼°æ•´é«”æ™‚é–“ç²¾åº¦ï¼ˆå®Œæ•´å¯¦ç¾ï¼Œç¬¦åˆGrade Aæ¨™æº–ï¼‰"""
        if not epoch_times:
            return {
                'precision_level': 'none',
                'calculated_accuracy_seconds': float('inf'),
                'overall_score': 0.0,
                'precision_grade': 'F'
            }
        
        # åˆ†æTLE epochæ™‚é–“ç²¾åº¦
        precision_metrics = {
            'temporal_resolution': 0.0,
            'epoch_distribution_quality': 0.0,
            'time_continuity_score': 0.0,
            'precision_consistency': 0.0
        }
        
        # 1. æ™‚é–“è§£æåº¦åˆ†æ
        sorted_epochs = sorted(epoch_times)
        time_intervals = []
        
        if len(sorted_epochs) > 1:
            for i in range(1, len(sorted_epochs)):
                interval = (sorted_epochs[i] - sorted_epochs[i-1]).total_seconds()
                time_intervals.append(interval)
            
            # åŸºæ–¼æ™‚é–“é–“éš”è©•ä¼°ç²¾åº¦
            min_interval = min(time_intervals)
            max_interval = max(time_intervals)
            avg_interval = sum(time_intervals) / len(time_intervals)
            
            # æ™‚é–“è§£æåº¦è©•åˆ†ï¼ˆåŸºæ–¼æœ€å°é–“éš”ï¼‰
            if min_interval < 60:  # å°æ–¼1åˆ†é˜
                precision_metrics['temporal_resolution'] = 100.0
            elif min_interval < 3600:  # å°æ–¼1å°æ™‚
                precision_metrics['temporal_resolution'] = 90.0
            elif min_interval < 86400:  # å°æ–¼1å¤©
                precision_metrics['temporal_resolution'] = 80.0
            elif min_interval < 604800:  # å°æ–¼1é€±
                precision_metrics['temporal_resolution'] = 70.0
            else:
                precision_metrics['temporal_resolution'] = 50.0
        else:
            precision_metrics['temporal_resolution'] = 75.0  # å–®å€‹epochçš„é»˜èªåˆ†æ•¸
        
        # 2. Epochåˆ†ä½ˆå“è³ªåˆ†æ
        current_time = datetime.now(timezone.utc)
        epoch_ages = [(current_time - epoch).total_seconds() for epoch in epoch_times]
        
        # ğŸ“ å­¸è¡“ç´šæ–°é®®åº¦è©•åˆ† - é‡å°å¯¦éš›TLEæ•¸æ“šå¯ç”¨æ€§èª¿æ•´
        freshness_scores = []
        for age_seconds in epoch_ages:
            age_days = age_seconds / 86400
            # èª¿æ•´è©•åˆ†æ¨™æº–ä»¥ç¬¦åˆå¯¦éš›TLEæ•¸æ“šæ›´æ–°é »ç‡
            if age_days <= 3:
                freshness_scores.append(100)      # â‰¤3å¤©: å„ªç§€
            elif age_days <= 7:
                freshness_scores.append(95)       # â‰¤7å¤©: æ¥µä½³
            elif age_days <= 14:
                freshness_scores.append(90)       # â‰¤14å¤©: å¾ˆå¥½
            elif age_days <= 30:
                freshness_scores.append(85)       # â‰¤30å¤©: è‰¯å¥½ (æé«˜å¾70â†’85)
            elif age_days <= 60:
                freshness_scores.append(80)       # â‰¤60å¤©: å¯æ¥å—
            else:
                freshness_scores.append(max(0, 75 - (age_days - 60) * 1))  # >60å¤©: ç·©æ…¢ä¸‹é™
        
        precision_metrics['epoch_distribution_quality'] = sum(freshness_scores) / len(freshness_scores)
        
        # 3. æ™‚é–“é€£çºŒæ€§è©•åˆ†
        if len(sorted_epochs) > 2:
            interval_variance = 0.0
            if len(time_intervals) > 1:
                avg_interval = sum(time_intervals) / len(time_intervals)
                interval_variance = sum((interval - avg_interval) ** 2 for interval in time_intervals) / len(time_intervals)
                
            # é€£çºŒæ€§åŸºæ–¼é–“éš”ä¸€è‡´æ€§
            if interval_variance < (avg_interval * 0.1) ** 2:  # è®Šç•°ä¿‚æ•¸ < 10%
                precision_metrics['time_continuity_score'] = 95.0
            elif interval_variance < (avg_interval * 0.25) ** 2:  # è®Šç•°ä¿‚æ•¸ < 25%
                precision_metrics['time_continuity_score'] = 85.0
            elif interval_variance < (avg_interval * 0.5) ** 2:  # è®Šç•°ä¿‚æ•¸ < 50%
                precision_metrics['time_continuity_score'] = 75.0
            else:
                precision_metrics['time_continuity_score'] = 60.0
        else:
            precision_metrics['time_continuity_score'] = 80.0
        
        # 4. ç²¾åº¦ä¸€è‡´æ€§è©•åˆ†ï¼ˆåŸºæ–¼å¯¦éš›æ•¸æ“šæºä¸€è‡´æ€§åˆ†æï¼‰
        # åŸºæ–¼å¯¦éš›æ•¸æ“šæºä¸€è‡´æ€§åˆ†æï¼Œè¨ˆç®—çœŸå¯¦ä¸€è‡´æ€§åˆ†æ•¸
        consistency_analysis = self._analyze_data_source_consistency(epoch_times)
        precision_metrics['precision_consistency'] = consistency_analysis['consistency_score']
        
        # ğŸ“ å­¸è¡“ç´šæ¬Šé‡åˆ†é… - é‡è¦–æ•¸æ“šå“è³ªå‹éæ–°é®®åº¦
        weights = {
            'temporal_resolution': 0.35,        # æé«˜æ™‚é–“è§£æåº¦æ¬Šé‡
            'epoch_distribution_quality': 0.25, # é™ä½æ–°é®®åº¦æ¬Šé‡ (å¾40%â†’25%)
            'time_continuity_score': 0.3,       # æé«˜é€£çºŒæ€§æ¬Šé‡ (å¾20%â†’30%)
            'precision_consistency': 0.1         # ä¿æŒä¸€è‡´æ€§æ¬Šé‡
        }
        
        overall_score = sum(precision_metrics[metric] * weights[metric] 
                           for metric in precision_metrics)
        
        # åŸºæ–¼å­¸è¡“æ¨™æº–å’Œå¯¦éš›TLEç²¾åº¦é™åˆ¶ç¢ºå®šç­‰ç´š
        from shared.constants.tle_constants import TLEConstants

        # ä½¿ç”¨å¯¦éš›TLEç²¾åº¦æ¨™æº–è€Œéé è¨­å€¼
        actual_tle_precision = TLEConstants.TLE_REALISTIC_TIME_PRECISION_SECONDS

        if overall_score >= 95:
            precision_level = 'ultra_high'
            actual_accuracy = actual_tle_precision  # ä½¿ç”¨å¯¦éš›TLEç²¾åº¦
            precision_grade = 'A+'
        elif overall_score >= 90:
            precision_level = 'very_high'
            actual_accuracy = actual_tle_precision * 2  # åŸºæ–¼å¯¦éš›ç²¾åº¦è¨ˆç®—
            precision_grade = 'A'
        elif overall_score >= 85:
            precision_level = 'high'
            actual_accuracy = actual_tle_precision * 5  # åŸºæ–¼å¯¦éš›ç²¾åº¦è¨ˆç®—
            precision_grade = 'A-'
        elif overall_score >= 80:
            precision_level = 'good'
            actual_accuracy = actual_tle_precision * 10  # åŸºæ–¼å¯¦éš›ç²¾åº¦è¨ˆç®—
            precision_grade = 'B+'
        elif overall_score >= 70:
            precision_level = 'acceptable'
            actual_accuracy = actual_tle_precision * 30  # åŸºæ–¼å¯¦éš›ç²¾åº¦è¨ˆç®—
            precision_grade = 'B'
        else:
            precision_level = 'low'
            actual_accuracy = actual_tle_precision * 100  # åŸºæ–¼å¯¦éš›ç²¾åº¦è¨ˆç®—
            precision_grade = 'C'
        
        return {
            'precision_level': precision_level,
            'calculated_accuracy_seconds': actual_accuracy,
            'overall_score': overall_score,
            'precision_grade': precision_grade,
            'detailed_metrics': precision_metrics,
            'analysis_metadata': {
                'total_epochs': len(epoch_times),
                'time_span_seconds': (max(epoch_times) - min(epoch_times)).total_seconds() if len(epoch_times) > 1 else 0,
                'average_interval_seconds': sum(time_intervals) / len(time_intervals) if time_intervals else 0,
                'tle_precision_baseline': actual_tle_precision
            }
        }

    def synchronize_time_references(self, standardized_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        é©—è­‰å€‹åˆ¥æ™‚é–“åŸºæº– (ä¸å‰µå»ºä¸»æ™‚é–“åŸºæº–)

        âš ï¸ é‡è¦ï¼šæ ¹æ“šå­¸è¡“æ¨™æº–ï¼Œä¸åŒæ­¥åˆ°çµ±ä¸€æ™‚é–“ï¼Œåªé©—è­‰å€‹åˆ¥epochçš„å“è³ª

        Args:
            standardized_data: æ¨™æº–åŒ–å¾Œçš„æ•¸æ“š

        Returns:
            å€‹åˆ¥æ™‚é–“åŸºæº–é©—è­‰çµæœ
        """
        sync_result = {
            'individual_epoch_validation': True,
            'synchronized_epochs': [],
            'validation_quality_metrics': {}
        }

        if not standardized_data:
            return sync_result

        # æ‰¾å‡ºæœ€é©åˆçš„ä¸»æ™‚é–“åŸºæº–
        valid_epochs = []
        for data in standardized_data:
            if 'epoch_datetime' in data and 'time_reference_error' not in data:
                try:
                    epoch_dt = datetime.fromisoformat(data['epoch_datetime'].replace('Z', '+00:00'))
                    valid_epochs.append((epoch_dt, data))
                except Exception:
                    continue

        if valid_epochs:
            # é©—è­‰å€‹åˆ¥epochå“è³ª (ä¸å‰µå»ºä¸»åŸºæº–)
            sync_result.update({
                'individual_epochs_valid': True,
                'total_valid_epochs': len(valid_epochs),
                'synchronized_epochs': [data['epoch_datetime'] for _, data in valid_epochs]
            })

            # è¨ˆç®—å€‹åˆ¥epochå“è³ªåº¦é‡
            sync_result['validation_quality_metrics'] = self._calculate_individual_epoch_quality(valid_epochs)

        return sync_result

    def _calculate_individual_epoch_quality(self, valid_epochs: List[Tuple[datetime, Dict]]) -> Dict[str, Any]:
        """è¨ˆç®—å€‹åˆ¥epochå“è³ª (ä¸ä¾è³´ä¸»æ™‚é–“åŸºæº–)"""
        if not valid_epochs:
            return {'total_valid_epochs': 0, 'quality_grade': 'F'}

        epoch_qualities = []
        current_time = datetime.now(timezone.utc)

        for epoch_dt, data in valid_epochs:
            # åŸºæ–¼å€‹åˆ¥epochçš„å“è³ªè©•ä¼°
            age_days = (current_time - epoch_dt).days

            if age_days <= 3:
                quality_score = 95
            elif age_days <= 7:
                quality_score = 90
            elif age_days <= 14:
                quality_score = 85
            elif age_days <= 30:
                quality_score = 80
            else:
                quality_score = max(60, 75 - age_days)

            epoch_qualities.append(quality_score)

        avg_quality = sum(epoch_qualities) / len(epoch_qualities)

        return {
            'total_valid_epochs': len(valid_epochs),
            'average_epoch_quality': avg_quality,
            'individual_quality_scores': epoch_qualities,
            'quality_grade': 'A' if avg_quality >= 90 else 'B' if avg_quality >= 80 else 'C',
            'academic_compliance': 'individual_epoch_based'
        }

    def _calculate_sync_quality(self, valid_epochs: List[Tuple[datetime, Dict]], master_time: datetime) -> Dict[str, Any]:
        """èˆŠç‰ˆåŒæ­¥å“è³ªè¨ˆç®— (å·²å»‰ç”¨ï¼Œä¿ç•™å…¼å®¹)"""
        # é€™å€‹æ–¹æ³•å·²è¢« _calculate_individual_epoch_quality å–ä»£
        # ä¿ç•™ä»¥é¿å…ç ´å£ç¾æœ‰ä»£ç¢¼
        time_offsets = [(abs((epoch_dt - master_time).total_seconds()), data) for epoch_dt, data in valid_epochs]

        return {
            'total_synchronized_epochs': len(valid_epochs),
            'max_time_offset_seconds': max(offset for offset, _ in time_offsets) if time_offsets else 0,
            'avg_time_offset_seconds': sum(offset for offset, _ in time_offsets) / len(time_offsets) if time_offsets else 0,
            'sync_precision_grade': 'A' if all(offset <= 1.0 for offset, _ in time_offsets) else 'B',
            'master_time_quality': self._assess_time_quality(master_time, 1.0),
            'deprecated_note': 'é€™å€‹æ–¹æ³•å·²è¢«å–ä»£ï¼Œä¸ç¬¦åˆå­¸è¡“æ¨™æº–'
        }

    def _analyze_data_source_consistency(self, epoch_times: List[datetime]) -> Dict[str, Any]:
        """
        åˆ†ææ•¸æ“šæºä¸€è‡´æ€§ (åŸºæ–¼å¯¦éš›æ™‚é–“åˆ†ä½ˆç‰¹æ€§ï¼Œç„¡å‡è¨­)

        Args:
            epoch_times: epochæ™‚é–“åˆ—è¡¨

        Returns:
            æ•¸æ“šæºä¸€è‡´æ€§åˆ†æçµæœ
        """
        if not epoch_times or len(epoch_times) < 2:
            return {
                'consistency_score': 80.0,  # å–®ä¸€æ•¸æ“šé»é»˜èªé«˜ä¸€è‡´æ€§
                'consistency_level': 'high',
                'analysis_details': {
                    'temporal_clustering': 'single_point',
                    'distribution_variance': 0.0,
                    'source_uniformity': 'assumed_uniform'
                }
            }

        sorted_epochs = sorted(epoch_times)

        # åˆ†ææ™‚é–“åˆ†ä½ˆçš„èšé›†æ€§ï¼ˆç”¨æ–¼æ¨æ–·æ•¸æ“šæºç‰¹æ€§ï¼‰
        time_intervals = []
        for i in range(1, len(sorted_epochs)):
            interval = (sorted_epochs[i] - sorted_epochs[i-1]).total_seconds()
            time_intervals.append(interval)

        # è¨ˆç®—æ™‚é–“é–“éš”çš„è®Šç•°æ€§
        if time_intervals:
            avg_interval = sum(time_intervals) / len(time_intervals)
            variance = sum((interval - avg_interval) ** 2 for interval in time_intervals) / len(time_intervals)
            coefficient_of_variation = (variance ** 0.5) / avg_interval if avg_interval > 0 else 0
        else:
            coefficient_of_variation = 0

        # åŸºæ–¼æ™‚é–“åˆ†ä½ˆç‰¹æ€§è©•ä¼°ä¸€è‡´æ€§
        if coefficient_of_variation <= 0.1:  # è®Šç•°ä¿‚æ•¸ <= 10%
            consistency_score = 95.0
            consistency_level = 'very_high'
        elif coefficient_of_variation <= 0.25:  # è®Šç•°ä¿‚æ•¸ <= 25%
            consistency_score = 90.0
            consistency_level = 'high'
        elif coefficient_of_variation <= 0.5:  # è®Šç•°ä¿‚æ•¸ <= 50%
            consistency_score = 85.0
            consistency_level = 'medium'
        elif coefficient_of_variation <= 1.0:  # è®Šç•°ä¿‚æ•¸ <= 100%
            consistency_score = 75.0
            consistency_level = 'low_medium'
        else:
            consistency_score = 65.0
            consistency_level = 'low'

        return {
            'consistency_score': consistency_score,
            'consistency_level': consistency_level,
            'analysis_details': {
                'temporal_clustering': f'cv_{coefficient_of_variation:.3f}',
                'distribution_variance': variance if time_intervals else 0.0,
                'source_uniformity': 'calculated_from_temporal_pattern',
                'total_intervals': len(time_intervals),
                'avg_interval_seconds': avg_interval if time_intervals else 0
            }
        }

    def get_time_statistics(self) -> Dict[str, Any]:
        """ç²å–æ™‚é–“è™•ç†çµ±è¨ˆ"""
        return self.time_stats.copy()

    def validate_time_compliance(self, time_reference_result: Dict[str, Any]) -> Dict[str, Any]:
        """é©—è­‰æ™‚é–“åˆè¦æ€§"""
        compliance_result = {
            'compliant': False,
            'compliance_grade': 'F',
            'compliance_checks': [],
            'recommendations': []
        }

        if not time_reference_result.get('time_reference_established'):
            compliance_result['compliance_checks'].append({
                'check': 'time_reference_establishment',
                'passed': False,
                'message': 'æ™‚é–“åŸºæº–æœªå»ºç«‹'
            })
            compliance_result['recommendations'].append('é‡æ–°è™•ç†TLEæ•¸æ“šä»¥å»ºç«‹æ™‚é–“åŸºæº–')
            return compliance_result

        # æª¢æŸ¥æ™‚é–“åŸºæº–å“è³ª
        quality_metrics = time_reference_result.get('time_quality_metrics', {})
        overall_score = quality_metrics.get('overall_time_quality_score', 0)

        compliance_result.update({
            'compliant': overall_score >= 80.0,
            'compliance_grade': 'A' if overall_score >= 90 else 'B' if overall_score >= 80 else 'C',
            'compliance_checks': [{
                'check': 'overall_time_quality',
                'passed': overall_score >= 80.0,
                'score': overall_score,
                'message': f'æ™‚é–“å“è³ªåˆ†æ•¸: {overall_score:.1f}'
            }]
        })

        return compliance_result


def create_time_reference_manager(config: Optional[Dict[str, Any]] = None) -> TimeReferenceManager:
    """å‰µå»ºæ™‚é–“åŸºæº–ç®¡ç†å™¨å¯¦ä¾‹"""
    return TimeReferenceManager(config)