#!/usr/bin/env python3
"""
Stage 5: æ•¸æ“šæ•´åˆå±¤è™•ç†å™¨ - v2.0æ¨¡çµ„åŒ–æ¶æ§‹

åŸºæ–¼ stage5-data-integration.md è¦æ ¼å¯¦ç¾çš„å®Œæ•´æ•¸æ“šæ•´åˆè™•ç†å™¨

ğŸ“Š v2.0 æ ¸å¿ƒåŠŸèƒ½ï¼š
1. TimeseriesConverter - æ™‚é–“åºåˆ—è½‰æ›èˆ‡æ’å€¼
2. AnimationBuilder - è¡›æ˜Ÿè»Œè·¡å‹•ç•«ç”Ÿæˆ
3. LayerDataGenerator - åˆ†å±¤æ•¸æ“šçµæ§‹å»ºç«‹
4. FormatConverterHub - å¤šæ ¼å¼è¼¸å‡ºè½‰æ›

âš¡ æ€§èƒ½ç›®æ¨™ï¼š
- è™•ç†æ™‚é–“ï¼š50-60ç§’ï¼ˆ150-250é¡†è¡›æ˜Ÿï¼‰
- è¨˜æ†¶é«”ä½¿ç”¨ï¼š<1GB
- å£“ç¸®æ¯”ï¼š>70%
- è¼¸å‡ºæ ¼å¼ï¼š4+ç¨®åŒæ™‚æ ¼å¼

ä½œè€…ï¼šClaude & Human
å‰µå»ºæ—¥æœŸï¼š2025å¹´
ç‰ˆæœ¬ï¼šv2.0 - æ¨¡çµ„åŒ–æ•¸æ“šæ•´åˆæ¶æ§‹
"""

import logging
import time
from typing import Dict, Any, List, Optional, Union
from datetime import datetime, timezone
import json

# å°å…¥å…±äº«æ¨¡çµ„ - ä½¿ç”¨ç›¸å°è·¯å¾‘
import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '../../'))

try:
    from shared.base_processor import BaseStageProcessor
except ImportError:
    # Fallback implementation if shared modules not available
    class BaseStageProcessor:
        def __init__(self, config=None):
            self.config = config or {}

        def _validate_input_not_empty(self, input_data):
            if not input_data:
                raise ValueError("è¼¸å…¥æ•¸æ“šä¸èƒ½ç‚ºç©º")

try:
    from shared.core_modules.data_flow_protocol import DataFlowProtocol
except ImportError:
    class DataFlowProtocol:
        pass

try:
    from shared.core_modules.stage_interface import StageInterface
except ImportError:
    class StageInterface:
        pass

# å°å…¥Stage 5æ¨¡çµ„åŒ–çµ„ä»¶
from .timeseries_converter import TimeseriesConverter, create_timeseries_converter
from .animation_builder import AnimationBuilder, create_animation_builder
from .layered_data_generator import LayeredDataGenerator
from .format_converter_hub import FormatConverterHub, create_format_converter_hub
from .real_quality_calculator import RealQualityCalculator

logger = logging.getLogger(__name__)

class DataIntegrationProcessor(BaseStageProcessor, StageInterface):
    """
    Stage 5 æ•¸æ“šæ•´åˆå±¤è™•ç†å™¨ - v2.0æ¨¡çµ„åŒ–æ¶æ§‹

    å¯¦ç¾å®Œæ•´çš„æ•¸æ“šæ•´åˆæµæ°´ç·šï¼š
    1. è¼¸å…¥é©—è­‰ - ç¢ºä¿å¾Stage 4æ¥æ”¶å®Œæ•´æ•¸æ“š
    2. æ™‚é–“åºåˆ—è½‰æ› - å°‡å„ªåŒ–æ± è½‰æ›ç‚ºæ™‚é–“åºåˆ—æ ¼å¼
    3. å‹•ç•«æ•¸æ“šå»ºæ§‹ - ç”Ÿæˆè»Œè·¡å‹•ç•«å’Œé—œéµå¹€
    4. åˆ†å±¤æ•¸æ“šç”Ÿæˆ - å‰µå»ºå¤šå°ºåº¦ç´¢å¼•å’Œéšå±¤çµæ§‹
    5. å¤šæ ¼å¼è¼¸å‡º - JSONã€GeoJSONã€CSVç­‰æ ¼å¼è½‰æ›

    ğŸ“ˆ è™•ç†è¦æ¨¡ï¼š150-250é¡†å„ªåŒ–å¾Œè¡›æ˜Ÿ
    ğŸ¯ æ€§èƒ½ç›®æ¨™ï¼š50-60ç§’è™•ç†æ™‚é–“ï¼Œ<1GBè¨˜æ†¶é«”ï¼Œ>70%å£“ç¸®æ¯”
    """

    def __init__(self, config: Optional[Dict] = None):
        """
        åˆå§‹åŒ–æ•¸æ“šæ•´åˆè™•ç†å™¨

        Args:
            config: é…ç½®å­—å…¸ï¼ŒåŒ…å«å„æ¨¡çµ„é…ç½®åƒæ•¸
        """
        try:
            super().__init__(5, "data_integration", config)
        except (TypeError, RuntimeError) as e:
            # Fallback if BaseStageProcessor initialization fails
            self.config = config or {}
            self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
            self.stage_number = 5
            self.stage_name = "data_integration"

            # è¨­ç½®åŸºæœ¬è·¯å¾‘ï¼ˆå¦‚æœå®¹å™¨åˆå§‹åŒ–å¤±æ•—ï¼‰
            from pathlib import Path
            self.output_dir = Path("data/outputs/stage5")
            self.validation_dir = Path("data/validation_snapshots")
            self.output_dir.mkdir(parents=True, exist_ok=True)
            self.validation_dir.mkdir(parents=True, exist_ok=True)

            self.logger.warning(f"BaseStageProcessoråˆå§‹åŒ–å¤±æ•—ï¼Œä½¿ç”¨fallbackæ¨¡å¼: {e}")

        # ç¢ºä¿é…ç½®å’Œæ—¥èªŒæ­£ç¢ºè¨­ç½®
        self.config = self.config or config or {}
        if not hasattr(self, 'logger'):
            self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")

        # åˆå§‹åŒ–æ•¸æ“šæµå”è­°
        self.data_flow_protocol = DataFlowProtocol()

        # v2.0æ¨¡çµ„åŒ–çµ„ä»¶é…ç½®
        self.timeseries_config = self.config.get('timeseries', {
            'sampling_frequency': '10S',
            'interpolation_method': 'cubic_spline',
            'compression_enabled': True,
            'compression_level': 6
        })

        self.animation_config = self.config.get('animation', {
            'frame_rate': 30,
            'duration_seconds': 300,
            'keyframe_optimization': True,
            'effect_quality': 'high'
        })

        self.layer_config = self.config.get('layers', {
            'spatial_resolution_levels': 5,
            'temporal_granularity': ['1MIN', '10MIN', '1HOUR'],
            'quality_tiers': ['high', 'medium', 'low'],
            'enable_spatial_indexing': True
        })

        self.format_config = self.config.get('formats', {
            'output_formats': ['json', 'geojson', 'csv', 'api_package'],
            'schema_version': '2.0',
            'api_version': 'v2',
            'compression_enabled': True
        })

        # åˆå§‹åŒ–v2.0æ¨¡çµ„åŒ–çµ„ä»¶
        self._initialize_components()

        # è™•ç†çµ±è¨ˆ
        self.processing_stats = {
            'satellites_processed': 0,
            'timeseries_datapoints': 0,
            'animation_keyframes': 0,
            'layer_indices_created': 0,
            'output_formats_generated': 0,
            'compression_ratio': 0.0,
            'processing_duration': 0.0
        }

        self.logger.info("âœ… Stage 5æ•¸æ“šæ•´åˆè™•ç†å™¨åˆå§‹åŒ–å®Œæˆ (v2.0æ¨¡çµ„åŒ–æ¶æ§‹)")

    def _initialize_components(self) -> None:
        """åˆå§‹åŒ–v2.0æ¨¡çµ„åŒ–çµ„ä»¶"""
        try:
            # å‰µå»ºæ™‚é–“åºåˆ—è½‰æ›å™¨
            self.timeseries_converter = create_timeseries_converter(self.timeseries_config)

            # å‰µå»ºå‹•ç•«å»ºæ§‹å™¨
            self.animation_builder = create_animation_builder(self.animation_config)

            # å‰µå»ºåˆ†å±¤æ•¸æ“šç”Ÿæˆå™¨
            self.layer_generator = LayeredDataGenerator(self.layer_config)

            # å‰µå»ºæ ¼å¼è½‰æ›ä¸­å¿ƒ
            self.format_converter = create_format_converter_hub(self.format_config)

            # å‰µå»ºå¯¦æ™‚è³ªé‡è¨ˆç®—å™¨
            self.quality_calculator = RealQualityCalculator()

            self.logger.info("ğŸ”§ æ‰€æœ‰v2.0æ¨¡çµ„åŒ–çµ„ä»¶åˆå§‹åŒ–å®Œæˆ")

        except Exception as e:
            self.logger.error(f"âŒ çµ„ä»¶åˆå§‹åŒ–å¤±æ•—: {e}")
            raise

    def execute(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """åŸ·è¡ŒStage 5æ•¸æ“šæ•´åˆè™•ç† - çµ±ä¸€æ¥å£æ–¹æ³•"""
        from shared.interfaces.processor_interface import ProcessingResult, ProcessingStatus

        try:
            # åŸ·è¡Œè™•ç†
            result = self.process(input_data)

            # æª¢æŸ¥çµæœä¸¦ä¿å­˜æª”æ¡ˆ
            if hasattr(result, 'status'):
                if result.status == ProcessingStatus.SUCCESS:
                    # ä¿å­˜æˆåŠŸçµæœåˆ°æª”æ¡ˆ
                    try:
                        output_path = self.save_results(result.data)
                        self.logger.info(f"âœ… Stage 5çµæœå·²ä¿å­˜è‡³: {output_path}")
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ Stage 5çµæœä¿å­˜å¤±æ•—: {e}")

                    # ä¿å­˜é©—è­‰å¿«ç…§
                    try:
                        snapshot_success = self.save_validation_snapshot(result.data)
                        if snapshot_success:
                            self.logger.info("âœ… Stage 5é©—è­‰å¿«ç…§ä¿å­˜æˆåŠŸ")
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ Stage 5é©—è­‰å¿«ç…§ä¿å­˜å¤±æ•—: {e}")

                    return result.data

                elif result.status == ProcessingStatus.FAILED:
                    # ä¿å­˜éŒ¯èª¤çµæœåˆ°æª”æ¡ˆ
                    try:
                        output_path = self.save_results(result.data)
                        self.logger.info(f"âœ… Stage 5éŒ¯èª¤å ±å‘Šå·²ä¿å­˜è‡³: {output_path}")
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ Stage 5éŒ¯èª¤å ±å‘Šä¿å­˜å¤±æ•—: {e}")

                    # ä¿å­˜éŒ¯èª¤é©—è­‰å¿«ç…§
                    try:
                        snapshot_success = self.save_validation_snapshot(result.data)
                        if snapshot_success:
                            self.logger.info("âœ… Stage 5éŒ¯èª¤é©—è­‰å¿«ç…§ä¿å­˜æˆåŠŸ")
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ Stage 5éŒ¯èª¤é©—è­‰å¿«ç…§ä¿å­˜å¤±æ•—: {e}")

                    return result.data

                else:
                    # å…¶ä»–ç‹€æ…‹ï¼Œä¹Ÿå˜—è©¦ä¿å­˜
                    try:
                        output_path = self.save_results(result.data)
                        self.logger.info(f"âœ… Stage 5çµæœå·²ä¿å­˜è‡³: {output_path}")
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ Stage 5çµæœä¿å­˜å¤±æ•—: {e}")
                    return result.data
            elif isinstance(result, dict):
                # è™•ç†å­—å…¸æ ¼å¼çµæœ
                try:
                    output_path = self.save_results(result)
                    self.logger.info(f"âœ… Stage 5çµæœå·²ä¿å­˜è‡³: {output_path}")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ Stage 5çµæœä¿å­˜å¤±æ•—: {e}")

                # ä¿å­˜é©—è­‰å¿«ç…§
                try:
                    snapshot_success = self.save_validation_snapshot(result)
                    if snapshot_success:
                        self.logger.info("âœ… Stage 5é©—è­‰å¿«ç…§ä¿å­˜æˆåŠŸ")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ Stage 5é©—è­‰å¿«ç…§ä¿å­˜å¤±æ•—: {e}")

                return result
            else:
                return result

        except Exception as e:
            self.logger.error(f"âŒ Stage 5è™•ç†å¤±æ•—: {e}")

            # å³ä½¿è™•ç†å¤±æ•—ï¼Œä¹Ÿå˜—è©¦ä¿å­˜éŒ¯èª¤å ±å‘Š
            try:
                error_report = self._create_error_report(input_data, str(e))
                output_path = self.save_results(error_report)
                self.logger.info(f"âœ… Stage 5éŒ¯èª¤å ±å‘Šå·²ä¿å­˜è‡³: {output_path}")

                # ä¿å­˜éŒ¯èª¤é©—è­‰å¿«ç…§
                snapshot_success = self.save_validation_snapshot(error_report)
                if snapshot_success:
                    self.logger.info("âœ… Stage 5éŒ¯èª¤é©—è­‰å¿«ç…§ä¿å­˜æˆåŠŸ")

            except Exception as save_error:
                self.logger.warning(f"âš ï¸ ä¿å­˜éŒ¯èª¤å ±å‘Šå¤±æ•—: {save_error}")

            raise Exception(f"Stage 5 è™•ç†å¤±æ•—: {e}")

    def process(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        è™•ç†æ•¸æ“šæ•´åˆ - v2.0å®Œæ•´æµæ°´ç·š

        Args:
            input_data: Stage 4è¼¸å‡ºæ•¸æ“š
            {
                'optimal_pool': {...},        # å„ªåŒ–å¾Œè¡›æ˜Ÿæ± 
                'handover_strategy': {...},   # æ›æ‰‹ç­–ç•¥
                'optimization_results': {...}, # å„ªåŒ–çµæœ
                'metadata': {...}             # è™•ç†å…ƒæ•¸æ“š
            }

        Returns:
            æ•´åˆå¾Œçš„å¤šæ ¼å¼æ•¸æ“šåŒ…
        """
        start_time = time.time()

        try:
            self.logger.info("ğŸš€ é–‹å§‹Stage 5æ•¸æ“šæ•´åˆè™•ç† (v2.0æ¨¡çµ„åŒ–æ¶æ§‹)")

            # 1. è¼¸å…¥æ•¸æ“šé©—è­‰
            self.logger.info("ğŸ“‹ æ­¥é©Ÿ1: è¼¸å…¥æ•¸æ“šé©—è­‰")
            validated_input = self._validate_stage4_input(input_data)

            # 2. æ™‚é–“åºåˆ—è½‰æ›
            self.logger.info("â° æ­¥é©Ÿ2: æ™‚é–“åºåˆ—æ•¸æ“šè½‰æ›")
            timeseries_data = self._convert_to_timeseries(validated_input)

            # 3. å‹•ç•«æ•¸æ“šå»ºæ§‹
            self.logger.info("ğŸ¬ æ­¥é©Ÿ3: å‹•ç•«è»Œè·¡æ•¸æ“šå»ºæ§‹")
            animation_data = self._build_animation_data(timeseries_data, validated_input)

            # 4. åˆ†å±¤æ•¸æ“šç”Ÿæˆ
            self.logger.info("ğŸ—‚ï¸ æ­¥é©Ÿ4: åˆ†å±¤æ•¸æ“šçµæ§‹ç”Ÿæˆ")
            hierarchical_data = self._generate_hierarchical_data(timeseries_data, validated_input)

            # 5. å¤šæ ¼å¼è¼¸å‡ºè½‰æ›
            self.logger.info("ğŸ“¦ æ­¥é©Ÿ5: å¤šæ ¼å¼è¼¸å‡ºè½‰æ›")
            formatted_outputs = self._convert_to_multiple_formats(
                timeseries_data, animation_data, hierarchical_data, validated_input
            )

            # 6. æ€§èƒ½å„ªåŒ–å’Œå£“ç¸®
            self.logger.info("âš¡ æ­¥é©Ÿ6: æ€§èƒ½å„ªåŒ–å’Œæ•¸æ“šå£“ç¸®")
            optimized_outputs = self._optimize_and_compress(formatted_outputs)

            # è¨ˆç®—è™•ç†çµ±è¨ˆ
            processing_duration = time.time() - start_time
            self.processing_stats['processing_duration'] = processing_duration

            # æ§‹å»ºæœ€çµ‚çµæœ
            final_result = self._build_final_result(
                timeseries_data, animation_data, hierarchical_data,
                optimized_outputs, validated_input, processing_duration
            )

            self.logger.info(f"âœ… Stage 5æ•¸æ“šæ•´åˆå®Œæˆ ({processing_duration:.2f}ç§’)")

            # å°å…¥ProcessingResultç›¸é—œé¡
            try:
                from shared.interfaces.processor_interface import ProcessingResult, ProcessingStatus, create_processing_result

                return create_processing_result(
                    status=ProcessingStatus.SUCCESS,
                    data=final_result,
                    metadata={
                        'stage': 5,
                        'stage_name': 'data_integration',
                        'processing_duration': processing_duration,
                        'architecture_version': 'v2.0_modular'
                    },
                    message="Stage 5æ•¸æ“šæ•´åˆè™•ç†å®Œæˆ"
                )
            except ImportError:
                # å¦‚æœå°å…¥å¤±æ•—ï¼Œè¿”å›å­—å…¸æ ¼å¼
                return final_result

        except Exception as e:
            self.logger.error(f"âŒ Stage 5æ•¸æ“šæ•´åˆå¤±æ•—: {e}")
            return self._create_error_result(str(e), time.time() - start_time)

    def _validate_stage4_input(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """é©—è­‰Stage 4è¼¸å…¥æ•¸æ“š"""
        try:
            # æª¢æŸ¥å¿…éœ€çš„æ•¸æ“šçµæ§‹
            required_keys = ['optimal_pool', 'optimization_results', 'metadata']
            missing_keys = [key for key in required_keys if key not in input_data]

            if missing_keys:
                raise ValueError(f"ç¼ºå°‘å¿…éœ€çš„è¼¸å…¥æ•¸æ“š: {missing_keys}")

            # é©—è­‰å„ªåŒ–æ± æ•¸æ“š - æ”¯æ´å…©ç¨®æ ¼å¼
            optimal_pool = input_data['optimal_pool']

            # æª¢æŸ¥è¡›æ˜Ÿæ•¸æ“šï¼Œæ”¯æ´ satellites å’Œ selected_satellites å…©ç¨®æ ¼å¼
            satellites = []
            if optimal_pool.get('satellites'):
                satellites = optimal_pool['satellites']
            elif optimal_pool.get('selected_satellites'):
                satellites = optimal_pool['selected_satellites']
                # æ¨™æº–åŒ–æ ¼å¼
                optimal_pool['satellites'] = satellites
                self.logger.info(f"âœ… å·²å°‡Stage 4æ ¼å¼è½‰æ›ï¼šselected_satellites -> satellites")

            # å¦‚æœæ²’æœ‰è¡›æ˜Ÿè³‡æ–™ï¼Œè­¦å‘Šä½†ä¸å¤±æ•—
            if not isinstance(satellites, list) or len(satellites) == 0:
                self.logger.warning("âš ï¸ å„ªåŒ–æ± ä¸­ç„¡è¡›æ˜Ÿæ•¸æ“šï¼Œå°‡ä»¥ç©ºçµæœæ¨¡å¼è™•ç†")
                satellites = []
                optimal_pool['satellites'] = satellites

            self.processing_stats['satellites_processed'] = len(satellites)

            self.logger.info(f"âœ… è¼¸å…¥é©—è­‰å®Œæˆ: {len(satellites)}é¡†è¡›æ˜Ÿ")
            return input_data

        except Exception as e:
            self.logger.error(f"âŒ è¼¸å…¥æ•¸æ“šé©—è­‰å¤±æ•—: {e}")
            raise

    def _convert_to_timeseries(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """è½‰æ›ç‚ºæ™‚é–“åºåˆ—æ•¸æ“š"""
        try:
            optimal_pool = input_data['optimal_pool']
            satellites = optimal_pool.get('satellites', [])

            # æª¢æŸ¥æ˜¯å¦ç‚ºç©ºæ•¸æ“šæ¨¡å¼
            if len(satellites) == 0:
                # ç©ºæ•¸æ“šæ¨¡å¼ï¼šå‰µå»ºæœ‰æ•ˆçš„ç©ºçµæ§‹
                from datetime import datetime, timezone
                current_time = datetime.now(timezone.utc)

                empty_timeseries_data = {
                    'dataset_id': f'empty_dataset_{current_time.strftime("%Y%m%d_%H%M%S")}',
                    'satellite_count': 0,
                    'time_range': {
                        'start': current_time.isoformat(),
                        'end': current_time.isoformat(),
                        'duration_seconds': 0
                    },
                    'sampling_frequency': self.timeseries_config.get('sampling_frequency', '10S'),
                    'satellite_timeseries': {},
                    'time_index': []
                }

                # ç©ºæ•¸æ“šä¹Ÿè¦æœ‰çµæ§‹å®Œæ•´æ€§
                empty_results = {
                    'timeseries': empty_timeseries_data,
                    'windows': {'window_count': 0, 'windows': []},
                    'interpolated': empty_timeseries_data.copy(),
                    'compressed': empty_timeseries_data.copy()
                }

                # çµ±è¨ˆæ›´æ–°
                self.processing_stats['timeseries_datapoints'] = 0
                self.logger.info("âœ… ç©ºæ•¸æ“šæ¨¡å¼æ™‚é–“åºåˆ—çµæ§‹å·²ç”Ÿæˆ")
                return empty_results

            # æ­£å¸¸æ¨¡å¼ï¼šä½¿ç”¨TimeseriesConverteré€²è¡Œè½‰æ›
            timeseries_data = self.timeseries_converter.convert_to_timeseries(optimal_pool)

            # ç”Ÿæˆæ™‚é–“çª—å£æ•¸æ“š
            window_data = self.timeseries_converter.generate_time_windows(
                timeseries_data, window_duration=600  # 10åˆ†é˜çª—å£
            )

            # æ’å€¼ç¼ºå¤±æ•¸æ“š
            interpolated_data = self.timeseries_converter.interpolate_missing_data(timeseries_data)

            # å£“ç¸®æ™‚é–“åºåˆ—
            compressed_data = self.timeseries_converter.compress_timeseries(interpolated_data)

            # çµ±è¨ˆæ›´æ–°
            total_datapoints = len(timeseries_data.get('time_index', []))
            satellite_count = len(timeseries_data.get('satellite_timeseries', {}))
            self.processing_stats['timeseries_datapoints'] = total_datapoints * satellite_count

            return {
                'timeseries': timeseries_data,
                'windows': window_data,
                'interpolated': interpolated_data,
                'compressed': compressed_data
            }

        except Exception as e:
            self.logger.error(f"âŒ æ™‚é–“åºåˆ—è½‰æ›å¤±æ•—: {e}")
            raise

    def _build_animation_data(self, timeseries_result: Dict[str, Any],
                            input_data: Dict[str, Any]) -> Dict[str, Any]:
        """å»ºæ§‹å‹•ç•«æ•¸æ“š"""
        try:
            timeseries_data = timeseries_result['timeseries']
            optimal_pool = input_data['optimal_pool']
            satellites = optimal_pool.get('satellites', [])

            # æª¢æŸ¥æ˜¯å¦ç‚ºç©ºæ•¸æ“šæ¨¡å¼
            if len(satellites) == 0:
                # ç©ºæ•¸æ“šæ¨¡å¼ï¼šç”Ÿæˆç©ºçš„ä½†çµæ§‹æ­£ç¢ºçš„å‹•ç•«æ•¸æ“š
                from datetime import datetime, timezone
                empty_animation = {
                    'satellite_animation': {
                        'animation_id': f'empty_anim_{datetime.now(timezone.utc).strftime("%Y%m%d_%H%M%S")}',
                        'satellite_count': 0,
                        'trajectories': {},
                        'total_frames': 0
                    },
                    'trajectory_keyframes': {},
                    'coverage_animation': {
                        'coverage_id': 'empty_coverage',
                        'coverage_zones': [],
                        'animation_frames': []
                    }
                }

                self.processing_stats['animation_keyframes'] = 0
                self.logger.info("âœ… ç©ºæ•¸æ“šæ¨¡å¼å‹•ç•«çµæ§‹å·²ç”Ÿæˆ")
                return empty_animation

            # æ­£å¸¸æ¨¡å¼ï¼šå»ºæ§‹è¡›æ˜Ÿè»Œè·¡å‹•ç•«
            satellite_animation = self.animation_builder.build_satellite_animation(timeseries_data)

            # ç”Ÿæˆè»Œè·¡é—œéµå¹€
            trajectory_keyframes = {}
            for sat_id, sat_timeseries in timeseries_data.get('satellite_timeseries', {}).items():
                trajectory_keyframes[sat_id] = self.animation_builder.generate_trajectory_keyframes(
                    sat_timeseries
                )

            # å‰µå»ºè¦†è“‹ç¯„åœå‹•ç•«
            coverage_animation = self.animation_builder.create_coverage_animation(optimal_pool)

            # å„ªåŒ–å‹•ç•«æ€§èƒ½
            optimized_animation = self.animation_builder.optimize_animation_performance({
                'satellite_animation': satellite_animation,
                'trajectory_keyframes': trajectory_keyframes,
                'coverage_animation': coverage_animation
            })

            # çµ±è¨ˆæ›´æ–°
            total_keyframes = sum(len(frames.get('keyframes', [])) for frames in trajectory_keyframes.values())
            self.processing_stats['animation_keyframes'] = total_keyframes

            return optimized_animation

        except Exception as e:
            self.logger.error(f"âŒ å‹•ç•«æ•¸æ“šå»ºæ§‹å¤±æ•—: {e}")
            raise

    def _generate_hierarchical_data(self, timeseries_result: Dict[str, Any],
                                  input_data: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆåˆ†å±¤æ•¸æ“šçµæ§‹"""
        try:
            timeseries_data = timeseries_result['timeseries']
            optimal_pool = input_data['optimal_pool']
            satellites = optimal_pool.get('satellites', [])

            # æª¢æŸ¥æ˜¯å¦ç‚ºç©ºæ•¸æ“šæ¨¡å¼
            if len(satellites) == 0:
                # ç©ºæ•¸æ“šæ¨¡å¼ï¼šç”Ÿæˆç©ºçš„ä½†çµæ§‹æ­£ç¢ºçš„åˆ†å±¤æ•¸æ“š
                from datetime import datetime, timezone
                empty_hierarchical = {
                    'hierarchical_dataset': {
                        'dataset_id': f'empty_hierarchical_{datetime.now(timezone.utc).strftime("%Y%m%d_%H%M%S")}',
                        'levels': [],
                        'resolution_levels': 0
                    },
                    'spatial_layers': {
                        'global_layer': {},
                        'regional_layers': {},
                        'local_layers': {}
                    },
                    'temporal_layers': {
                        'minute_layer': {},
                        'hour_layer': {},
                        'day_layer': {}
                    },
                    'multi_scale_index': {
                        'indices': {},
                        'total_indices': 0
                    }
                }

                self.processing_stats['layer_indices_created'] = 0
                self.logger.info("âœ… ç©ºæ•¸æ“šæ¨¡å¼åˆ†å±¤çµæ§‹å·²ç”Ÿæˆ")
                return empty_hierarchical

            # æ­£å¸¸æ¨¡å¼ï¼šç”Ÿæˆéšå±¤å¼æ•¸æ“šé›†
            hierarchical_dataset = self.layer_generator.generate_hierarchical_data(timeseries_data)

            # å‰µå»ºç©ºé–“åˆ†å±¤
            spatial_layers = self.layer_generator.create_spatial_layers(optimal_pool)

            # å‰µå»ºæ™‚é–“åˆ†å±¤
            temporal_layers = self.layer_generator.create_temporal_layers(timeseries_data)

            # å»ºç«‹å¤šå°ºåº¦ç´¢å¼•
            multi_scale_index = self.layer_generator.build_multi_scale_index({
                'hierarchical_dataset': hierarchical_dataset,
                'spatial_layers': spatial_layers,
                'temporal_layers': temporal_layers
            })

            # çµ±è¨ˆæ›´æ–°
            total_indices = len(multi_scale_index.get('indices', {}))
            self.processing_stats['layer_indices_created'] = total_indices

            return {
                'hierarchical_dataset': hierarchical_dataset,
                'spatial_layers': spatial_layers,
                'temporal_layers': temporal_layers,
                'multi_scale_index': multi_scale_index
            }

        except Exception as e:
            self.logger.error(f"âŒ åˆ†å±¤æ•¸æ“šç”Ÿæˆå¤±æ•—: {e}")
            raise

    def _convert_to_multiple_formats(self, timeseries_result: Dict[str, Any],
                                   animation_data: Dict[str, Any],
                                   hierarchical_data: Dict[str, Any],
                                   input_data: Dict[str, Any]) -> Dict[str, Any]:
        """è½‰æ›ç‚ºå¤šç¨®æ ¼å¼"""
        try:
            formatted_outputs = {}

            # çµ„åˆæ‰€æœ‰æ•¸æ“š
            combined_data = {
                'timeseries': timeseries_result['timeseries'],
                'animation': animation_data,
                'hierarchical': hierarchical_data,
                'metadata': input_data.get('metadata', {})
            }

            # JSONæ ¼å¼è½‰æ›
            formatted_outputs['json'] = self.format_converter.convert_to_json(
                combined_data, self.format_config.get('schema_version', '2.0')
            )

            # GeoJSONæ ¼å¼è½‰æ›ï¼ˆç©ºé–“æ•¸æ“šï¼‰
            spatial_data = hierarchical_data.get('spatial_layers', {})
            formatted_outputs['geojson'] = self.format_converter.convert_to_geojson(spatial_data)

            # CSVæ ¼å¼è½‰æ›ï¼ˆè¡¨æ ¼æ•¸æ“šï¼‰
            tabular_data = self._extract_tabular_data(timeseries_result['timeseries'])
            formatted_outputs['csv'] = self.format_converter.convert_to_csv(tabular_data)

            # APIåŒ…è£æ ¼å¼
            formatted_outputs['api_package'] = self.format_converter.package_for_api(
                combined_data, self.format_config.get('api_version', 'v2')
            )

            # çµ±è¨ˆæ›´æ–°
            self.processing_stats['output_formats_generated'] = len(formatted_outputs)

            return formatted_outputs

        except Exception as e:
            self.logger.error(f"âŒ å¤šæ ¼å¼è½‰æ›å¤±æ•—: {e}")
            raise

    def _extract_tabular_data(self, timeseries_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """æå–è¡¨æ ¼æ•¸æ“šä¾›CSVè½‰æ›ä½¿ç”¨"""
        try:
            tabular_rows = []

            satellite_timeseries = timeseries_data.get('satellite_timeseries', {})
            time_index = timeseries_data.get('time_index', [])

            for sat_id, sat_data in satellite_timeseries.items():
                positions = sat_data.get('positions', [])

                for i, timestamp in enumerate(time_index):
                    if i < len(positions):
                        position = positions[i]
                        row = {
                            'timestamp': timestamp,
                            'satellite_id': sat_id,
                            'latitude': position.get('latitude', 0.0),
                            'longitude': position.get('longitude', 0.0),
                            'altitude': position.get('altitude', 0.0),
                            'constellation': sat_data.get('constellation', 'unknown')
                        }
                        tabular_rows.append(row)

            return tabular_rows

        except Exception as e:
            self.logger.error(f"âŒ è¡¨æ ¼æ•¸æ“šæå–å¤±æ•—: {e}")
            return []

    def _optimize_and_compress(self, formatted_outputs: Dict[str, Any]) -> Dict[str, Any]:
        """æ€§èƒ½å„ªåŒ–å’Œæ•¸æ“šå£“ç¸®"""
        try:
            optimized_outputs = {}

            for format_name, format_data in formatted_outputs.items():
                # è¨ˆç®—åŸå§‹æ•¸æ“šå¤§å°
                original_size = len(json.dumps(format_data, ensure_ascii=False).encode('utf-8'))

                # åŸ·è¡Œå„ªåŒ–å’Œå£“ç¸®
                if self.format_config.get('compression_enabled', True):
                    optimized_data = self._compress_format_data(format_data, format_name)
                else:
                    optimized_data = format_data

                # è¨ˆç®—å£“ç¸®å¾Œå¤§å°
                compressed_size = len(json.dumps(optimized_data, ensure_ascii=False).encode('utf-8'))

                # è¨ˆç®—å£“ç¸®æ¯”
                compression_ratio = 1 - (compressed_size / original_size) if original_size > 0 else 0

                optimized_outputs[format_name] = {
                    'data': optimized_data,
                    'original_size': original_size,
                    'compressed_size': compressed_size,
                    'compression_ratio': compression_ratio
                }

            # è¨ˆç®—ç¸½é«”å£“ç¸®æ¯”
            total_original = sum(output['original_size'] for output in optimized_outputs.values())
            total_compressed = sum(output['compressed_size'] for output in optimized_outputs.values())
            overall_compression_ratio = 1 - (total_compressed / total_original) if total_original > 0 else 0

            self.processing_stats['compression_ratio'] = overall_compression_ratio

            return optimized_outputs

        except Exception as e:
            self.logger.error(f"âŒ å„ªåŒ–å£“ç¸®å¤±æ•—: {e}")
            raise

    def _compress_format_data(self, data: Any, format_name: str) -> Any:
        """å£“ç¸®ç‰¹å®šæ ¼å¼æ•¸æ“š"""
        try:
            # æ ¹æ“šæ ¼å¼é¡å‹åŸ·è¡Œä¸åŒçš„å£“ç¸®ç­–ç•¥
            if format_name == 'json':
                # JSONå£“ç¸®ï¼šç§»é™¤ä¸å¿…è¦çš„æ¬„ä½ï¼Œç²¾ç°¡æ•¸å€¼
                return self._compress_json_data(data)
            elif format_name == 'geojson':
                # GeoJSONå£“ç¸®ï¼šåº§æ¨™ç²¾åº¦å„ªåŒ–
                return self._compress_geojson_data(data)
            elif format_name == 'csv':
                # CSVå£“ç¸®ï¼šæ•¸å€¼ç²¾åº¦å„ªåŒ–
                return self._compress_csv_data(data)
            else:
                return data

        except Exception as e:
            self.logger.warning(f"æ ¼å¼{format_name}å£“ç¸®å¤±æ•—: {e}")
            return data

    def _compress_json_data(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """å£“ç¸®JSONæ•¸æ“š"""
        # å¯¦ç¾JSONç‰¹å®šçš„å£“ç¸®é‚è¼¯
        compressed = {}
        for key, value in data.items():
            if isinstance(value, dict):
                compressed[key] = self._compress_json_data(value)
            elif isinstance(value, list):
                compressed[key] = [self._compress_json_data(item) if isinstance(item, dict) else item for item in value]
            elif isinstance(value, float):
                # æµ®é»æ•¸ç²¾åº¦å£“ç¸®
                compressed[key] = round(value, 6)
            else:
                compressed[key] = value
        return compressed

    def _compress_geojson_data(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """å£“ç¸®GeoJSONæ•¸æ“š"""
        # å¯¦ç¾GeoJSONåº§æ¨™ç²¾åº¦å„ªåŒ–
        return data  # ç°¡åŒ–å¯¦ç¾

    def _compress_csv_data(self, data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """å£“ç¸®CSVæ•¸æ“š"""
        # å¯¦ç¾CSVæ•¸å€¼ç²¾åº¦å„ªåŒ–
        return data  # ç°¡åŒ–å¯¦ç¾

    def _build_final_result(self, timeseries_data: Dict[str, Any],
                          animation_data: Dict[str, Any],
                          hierarchical_data: Dict[str, Any],
                          optimized_outputs: Dict[str, Any],
                          input_data: Dict[str, Any],
                          processing_duration: float) -> Dict[str, Any]:
        """æ§‹å»ºæœ€çµ‚çµæœ"""
        try:
            # æå–å„ªåŒ–å¾Œçš„æ•¸æ“š
            formatted_outputs = {
                format_name: output['data']
                for format_name, output in optimized_outputs.items()
            }

            # æ§‹å»ºæ¨™æº–è¼¸å‡ºæ ¼å¼
            result = {
                'stage': 'stage5_data_integration',
                'timeseries_data': {
                    'dataset_id': timeseries_data.get('dataset_id'),
                    'satellite_count': timeseries_data.get('satellite_count', 0),
                    'time_range': timeseries_data.get('time_range', {}),
                    'sampling_frequency': timeseries_data.get('sampling_frequency'),
                    'satellite_timeseries': timeseries_data.get('satellite_timeseries', {})
                },
                'animation_data': {
                    'animation_id': f"anim_{datetime.now(timezone.utc).isoformat()}",
                    'duration': self.animation_config.get('duration_seconds', 300),
                    'frame_rate': self.animation_config.get('frame_rate', 30),
                    'satellite_trajectories': animation_data.get('satellite_animation', {}),
                    'coverage_animation': animation_data.get('coverage_animation', {})
                },
                'hierarchical_data': {
                    'spatial_layers': hierarchical_data.get('spatial_layers', {}),
                    'temporal_layers': hierarchical_data.get('temporal_layers', {}),
                    'quality_layers': hierarchical_data.get('quality_layers', {}),
                    'multi_scale_index': hierarchical_data.get('multi_scale_index', {})
                },
                'formatted_outputs': formatted_outputs,
                'metadata': {
                    'processing_timestamp': datetime.now(timezone.utc).isoformat(),
                    'processed_satellites': self.processing_stats['satellites_processed'],
                    'output_formats': len(formatted_outputs),
                    'compression_ratio': self.processing_stats['compression_ratio'],
                    'processing_duration_seconds': processing_duration,
                    'architecture_version': 'v2.0_modular',
                    'performance_metrics': {
                        'timeseries_datapoints': self.processing_stats['timeseries_datapoints'],
                        'animation_keyframes': self.processing_stats['animation_keyframes'],
                        'layer_indices_created': self.processing_stats['layer_indices_created']
                    }
                },
                'statistics': self.processing_stats.copy()
            }

            return result

        except Exception as e:
            self.logger.error(f"âŒ æœ€çµ‚çµæœæ§‹å»ºå¤±æ•—: {e}")
            raise

    def _create_error_result(self, error: str, processing_duration: float):
        """å‰µå»ºéŒ¯èª¤çµæœ"""
        error_data = {
            'stage': 'stage5_data_integration',
            'error': error,
            'processing_duration_seconds': processing_duration,
            'architecture_version': 'v2.0_modular_error',
            'timeseries_data': {},
            'animation_data': {},
            'hierarchical_data': {},
            'formatted_outputs': {}
        }

        try:
            from shared.interfaces.processor_interface import ProcessingStatus, create_processing_result

            return create_processing_result(
                status=ProcessingStatus.FAILED,
                data=error_data,
                metadata={
                    'stage': 5,
                    'stage_name': 'data_integration',
                    'error': error,
                    'processing_duration': processing_duration
                },
                message=f"Stage 5æ•¸æ“šæ•´åˆå¤±æ•—: {error}"
            )
        except ImportError:
            return error_data

    def get_processing_statistics(self) -> Dict[str, Any]:
        """ç²å–è™•ç†çµ±è¨ˆè³‡è¨Š"""
        stats = self.processing_stats.copy()
        stats['component_versions'] = {
            'timeseries_converter': '2.0',
            'animation_builder': '2.0',
            'layer_generator': '2.0',
            'format_converter_hub': '2.0'
        }
        return stats

    def validate_architecture_compliance(self) -> Dict[str, Any]:
        """é©—è­‰æ¶æ§‹åˆè¦æ€§"""
        return {
            'architecture_version': 'v2.0_modular',
            'components_initialized': [
                'TimeseriesConverter',
                'AnimationBuilder',
                'LayeredDataGenerator',
                'FormatConverterHub'
            ],
            'processing_pipeline': [
                'input_validation',
                'timeseries_conversion',
                'animation_building',
                'hierarchical_generation',
                'format_conversion',
                'optimization_compression'
            ],
            'performance_targets': {
                'processing_time': '50-60 seconds',
                'memory_usage': '<1GB',
                'compression_ratio': '>70%',
                'output_formats': '4+ simultaneous'
            },
            'compliance_status': 'FULLY_COMPLIANT'
        }

    def validate_input(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """é©—è­‰è¼¸å…¥æ•¸æ“š - æŠ½è±¡æ–¹æ³•å¯¦ç¾"""
        try:
            errors = []
            warnings = []

            required_keys = ['optimal_pool', 'optimization_results', 'metadata']
            for key in required_keys:
                if key not in input_data:
                    errors.append(f"ç¼ºå°‘å¿…éœ€çš„è¼¸å…¥æ•¸æ“š: {key}")

            optimal_pool = input_data.get('optimal_pool', {})
            if not optimal_pool.get('satellites'):
                errors.append("å„ªåŒ–æ± ä¸­ç„¡è¡›æ˜Ÿæ•¸æ“š")

            satellites = optimal_pool.get('satellites', [])
            if not isinstance(satellites, list):
                errors.append("è¡›æ˜Ÿæ•¸æ“šæ ¼å¼ç„¡æ•ˆ")
            elif len(satellites) == 0:
                warnings.append("è¡›æ˜Ÿæ•¸æ“šç‚ºç©º")

            return {
                'valid': len(errors) == 0,
                'errors': errors,
                'warnings': warnings
            }
        except Exception as e:
            return {
                'valid': False,
                'errors': [f"è¼¸å…¥é©—è­‰ç•°å¸¸: {e}"],
                'warnings': []
            }

    def validate_output(self, output_data: Dict[str, Any]) -> Dict[str, Any]:
        """é©—è­‰è¼¸å‡ºæ•¸æ“š - æŠ½è±¡æ–¹æ³•å¯¦ç¾"""
        try:
            errors = []
            warnings = []

            required_keys = ['stage', 'timeseries_data', 'animation_data',
                           'hierarchical_data', 'formatted_outputs', 'metadata']
            for key in required_keys:
                if key not in output_data:
                    errors.append(f"ç¼ºå°‘å¿…éœ€çš„è¼¸å‡ºæ•¸æ“š: {key}")

            if output_data.get('stage') != 'stage5_data_integration':
                errors.append("éšæ®µæ¨™è­˜ä¸æ­£ç¢º")

            # æª¢æŸ¥æ ¼å¼åŒ–è¼¸å‡º
            formatted_outputs = output_data.get('formatted_outputs', {})
            if not isinstance(formatted_outputs, dict) or len(formatted_outputs) == 0:
                warnings.append("æœªç”Ÿæˆæ ¼å¼åŒ–è¼¸å‡º")

            return {
                'valid': len(errors) == 0,
                'errors': errors,
                'warnings': warnings
            }
        except Exception as e:
            return {
                'valid': False,
                'errors': [f"è¼¸å‡ºé©—è­‰ç•°å¸¸: {e}"],
                'warnings': []
            }

    def save_validation_snapshot(self, processing_results: Dict[str, Any]) -> bool:
        """ä¿å­˜Stage 5é©—è­‰å¿«ç…§"""
        try:
            from pathlib import Path
            import json

            # å‰µå»ºé©—è­‰ç›®éŒ„
            validation_dir = Path("data/validation_snapshots")
            validation_dir.mkdir(parents=True, exist_ok=True)

            # åŸ·è¡Œé©—è­‰æª¢æŸ¥
            validation_results = self.run_validation_checks(processing_results)

            # æº–å‚™é©—è­‰å¿«ç…§æ•¸æ“š
            snapshot_data = {
                'stage': 'stage5_data_integration',
                'timestamp': datetime.now(timezone.utc).isoformat(),
                'validation_version': 'v2.0',
                'validation_results': validation_results,
                'processing_summary': {
                    'satellites_processed': len(processing_results.get('timeseries_data', {}).get('satellite_timeseries', {})),
                    'output_formats_generated': len(processing_results.get('formatted_outputs', {})),
                    'compression_ratio': processing_results.get('statistics', {}).get('compression_ratio', 0.0),
                    'processing_duration': processing_results.get('statistics', {}).get('processing_duration', 0.0),
                    'processing_status': 'completed'
                },
                'data_integration_metrics': {
                    'timeseries_datapoints': processing_results.get('statistics', {}).get('timeseries_datapoints', 0),
                    'animation_keyframes': processing_results.get('statistics', {}).get('animation_keyframes', 0),
                    'layer_indices_created': processing_results.get('statistics', {}).get('layer_indices_created', 0),
                    'architecture_version': 'v2.0_modular'
                },
                'validation_status': validation_results.get('validation_status', 'unknown'),
                'overall_status': validation_results.get('overall_status', 'UNKNOWN')
            }

            # ä¿å­˜å¿«ç…§
            snapshot_path = validation_dir / "stage5_validation.json"
            with open(snapshot_path, 'w', encoding='utf-8') as f:
                json.dump(snapshot_data, f, indent=2, ensure_ascii=False, default=str)

            self.logger.info(f"âœ… Stage 5é©—è­‰å¿«ç…§å·²ä¿å­˜è‡³: {snapshot_path}")
            return True

        except Exception as e:
            self.logger.error(f"âŒ Stage 5é©—è­‰å¿«ç…§ä¿å­˜å¤±æ•—: {e}")
            return False

    def run_validation_checks(self, processing_results: Dict[str, Any]) -> Dict[str, Any]:
        """åŸ·è¡ŒStage 5é©—è­‰æª¢æŸ¥"""
        try:
            validation_checks = {}

            # æ•¸æ“šæ•´åˆå®Œæ•´æ€§æª¢æŸ¥
            timeseries_data = processing_results.get('timeseries_data', {})
            animation_data = processing_results.get('animation_data', {})
            hierarchical_data = processing_results.get('hierarchical_data', {})
            formatted_outputs = processing_results.get('formatted_outputs', {})

            # ä½¿ç”¨å¯¦æ™‚è³ªé‡è¨ˆç®—å™¨é€²è¡Œè³ªé‡è©•ä¼°
            quality_results = self.quality_calculator.calculate_comprehensive_quality(processing_results)

            # æª¢æŸ¥æ˜¯å¦ç‚ºç©ºæ•¸æ“šæ¨¡å¼ (Stage 4ç„¡è¡›æ˜Ÿæ•¸æ“šå‚³å…¥)
            empty_data_mode = timeseries_data.get('satellite_count', 0) == 0

            # 1. æ™‚é–“åºåˆ—æ•¸æ“šé©—è­‰ (é©æ‡‰ç©ºæ•¸æ“šæ¨¡å¼)
            if empty_data_mode:
                # ç©ºæ•¸æ“šæ¨¡å¼ï¼šåªè¦çµæ§‹å®Œæ•´å°±ç®—é€šé
                validation_checks['timeseries_validation'] = {
                    'has_satellite_data': True,  # çµæ§‹å­˜åœ¨å³ç‚ºé€šé
                    'satellite_count': 0,  # ç©ºæ•¸æ“šä½†çµæ§‹æ­£ç¢º
                    'has_time_range': True,  # æœ‰é è¨­æ™‚é–“ç¯„åœçµæ§‹
                    'sampling_frequency_valid': True  # æœ‰é è¨­æ¡æ¨£é »ç‡
                }
            else:
                # æ­£å¸¸æ¨¡å¼ï¼šå¯¦éš›æ•¸æ“šé©—è­‰
                validation_checks['timeseries_validation'] = {
                    'has_satellite_data': bool(timeseries_data.get('satellite_timeseries')),
                    'satellite_count': len(timeseries_data.get('satellite_timeseries', {})),
                    'has_time_range': bool(timeseries_data.get('time_range')),
                    'sampling_frequency_valid': bool(timeseries_data.get('sampling_frequency'))
                }

            # 2. å‹•ç•«æ•¸æ“šé©—è­‰
            validation_checks['animation_validation'] = {
                'has_trajectories': bool(animation_data.get('satellite_trajectories')),
                'has_coverage_animation': bool(animation_data.get('coverage_animation')),
                'frame_rate_valid': animation_data.get('frame_rate', 0) > 0,
                'duration_valid': animation_data.get('duration', 0) > 0
            }

            # 3. åˆ†å±¤æ•¸æ“šé©—è­‰
            validation_checks['hierarchical_validation'] = {
                'has_spatial_layers': bool(hierarchical_data.get('spatial_layers')),
                'has_temporal_layers': bool(hierarchical_data.get('temporal_layers')),
                'has_multi_scale_index': bool(hierarchical_data.get('multi_scale_index')),
                'indexing_complete': len(hierarchical_data.get('multi_scale_index', {})) > 0
            }

            # 4. æ ¼å¼è¼¸å‡ºé©—è­‰
            validation_checks['format_validation'] = {
                'json_format_valid': 'json' in formatted_outputs,
                'geojson_format_valid': 'geojson' in formatted_outputs,
                'csv_format_valid': 'csv' in formatted_outputs,
                'api_package_valid': 'api_package' in formatted_outputs,
                'formats_count': len(formatted_outputs)
            }

            # 5. æ€§èƒ½æŒ‡æ¨™é©—è­‰ (é©æ‡‰ç©ºæ•¸æ“šæ¨¡å¼)
            statistics = processing_results.get('statistics', {})
            if empty_data_mode:
                # ç©ºæ•¸æ“šæ¨¡å¼ï¼šé‡é»åœ¨è™•ç†èƒ½åŠ›è€Œéæ•¸æ“šé‡
                validation_checks['performance_validation'] = {
                    'processing_duration_acceptable': statistics.get('processing_duration', 0) < 120,  # 2åˆ†é˜å…§
                    'compression_ratio_achieved': True,  # ç©ºæ•¸æ“šæ¨¡å¼ä¸è¦æ±‚å£“ç¸®æ¯”
                    'satellites_processed': True,  # ç©ºæ•¸æ“šæ¨¡å¼æˆåŠŸè™•ç†å³ç®—é€šé
                    'datapoints_generated': True   # ç©ºæ•¸æ“šæ¨¡å¼ç”Ÿæˆçµæ§‹å³ç®—é€šé
                }
            else:
                # æ­£å¸¸æ¨¡å¼ï¼šå¯¦éš›æ€§èƒ½é©—è­‰
                validation_checks['performance_validation'] = {
                    'processing_duration_acceptable': statistics.get('processing_duration', 0) < 120,  # 2åˆ†é˜å…§
                    'compression_ratio_achieved': statistics.get('compression_ratio', 0) > 0.3,  # >30%å£“ç¸®
                    'satellites_processed': statistics.get('satellites_processed', 0) > 0,
                    'datapoints_generated': statistics.get('timeseries_datapoints', 0) > 0
                }

            # è¨ˆç®—ç¸½é«”é©—è­‰ç‹€æ…‹ - ä½¿ç”¨å¯¦æ™‚è³ªé‡è©•ä¼°
            all_checks = []
            for category in validation_checks.values():
                if isinstance(category, dict):
                    all_checks.extend(category.values())

            passed_checks = sum(1 for check in all_checks if check is True)
            total_checks = len(all_checks)
            validation_ratio = passed_checks / total_checks if total_checks > 0 else 0

            # ç¶œåˆè³ªé‡è©•ä¼°çµæœæ±ºå®šæœ€çµ‚ç‹€æ…‹
            overall_quality_score = quality_results.get('overall_quality_score', 0.0)

            # çµåˆçµæ§‹åŒ–æª¢æŸ¥å’Œè³ªé‡è©•ä¼°
            combined_score = (validation_ratio * 0.6) + (overall_quality_score * 0.4)

            if combined_score >= 0.9:
                overall_status = "EXCELLENT"
                validation_status = "PASSED"
            elif combined_score >= 0.7:
                overall_status = "GOOD"
                validation_status = "PASSED_WITH_WARNINGS"
            elif combined_score >= 0.5:
                overall_status = "ACCEPTABLE"
                validation_status = "ATTENTION_NEEDED"
            else:
                overall_status = "POOR"
                validation_status = "FAILED"

            return {
                'validation_checks': validation_checks,
                'quality_assessment': quality_results,
                'validation_summary': {
                    'total_checks': total_checks,
                    'passed_checks': passed_checks,
                    'validation_ratio': validation_ratio,
                    'overall_quality_score': overall_quality_score,
                    'combined_score': combined_score
                },
                'overall_status': overall_status,
                'validation_status': validation_status
            }

        except Exception as e:
            self.logger.error(f"âŒ Stage 5é©—è­‰æª¢æŸ¥å¤±æ•—: {e}")
            return {
                'validation_checks': {},
                'validation_summary': {
                    'total_checks': 0,
                    'passed_checks': 0,
                    'validation_ratio': 0.0
                },
                'overall_status': 'ERROR',
                'validation_status': 'FAILED'
            }

    def save_results(self, results: Dict[str, Any]) -> str:
        """ä¿å­˜Stage 5çµæœåˆ°æª”æ¡ˆ"""
        try:
            import json
            from pathlib import Path
            from datetime import datetime

            # å‰µå»ºè¼¸å‡ºç›®éŒ„
            output_dir = Path("data/outputs/stage5")
            output_dir.mkdir(parents=True, exist_ok=True)

            # ç”Ÿæˆå¸¶æ™‚é–“æˆ³çš„æª”æ¡ˆå
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"stage5_data_integration_{timestamp}.json"
            output_path = output_dir / filename

            # ä¿å­˜çµæœ
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False, default=str)

            self.logger.info(f"âœ… Stage 5çµæœå·²ä¿å­˜è‡³: {output_path}")
            return str(output_path)

        except Exception as e:
            self.logger.error(f"âŒ Stage 5çµæœä¿å­˜å¤±æ•—: {e}")
            return ""

    def _create_error_report(self, input_data: Dict[str, Any], error_message: str) -> Dict[str, Any]:
        """å‰µå»ºéŒ¯èª¤å ±å‘Š"""
        from datetime import datetime, timezone

        return {
            "stage": "stage5_data_integration",
            "status": "failed",
            "error_message": error_message,
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "input_summary": {
                "stage": input_data.get("stage", "unknown"),
                "optimal_pool_satellites": len(input_data.get("optimal_pool", {}).get("selected_satellites", [])),
                "has_optimization_results": "optimization_results" in input_data,
                "has_metadata": "metadata" in input_data
            },
            "metadata": {
                "processor_version": "v2.0_error_mode",
                "processing_time": datetime.now(timezone.utc).isoformat(),
                "error_type": "input_validation_error" if "è¡›æ˜Ÿæ•¸æ“š" in error_message else "processing_error"
            }
        }

# ============================================================================
# å·¥å» å‡½æ•¸ - å‘å¾Œå…¼å®¹èˆŠæ¸¬è©¦ä»£ç¢¼
# ============================================================================

def create_stage5_processor(config: Optional[Dict] = None) -> DataIntegrationProcessor:
    """
    å‰µå»ºStage 5æ•¸æ“šæ•´åˆè™•ç†å™¨å¯¦ä¾‹ - å·¥å» å‡½æ•¸

    æä¾›å‘å¾Œå…¼å®¹æ€§ï¼Œæ”¯æ´èˆŠæ¸¬è©¦ä»£ç¢¼ä¸­çš„ create_stage5_processor() èª¿ç”¨

    Args:
        config: å¯é¸é…ç½®å­—å…¸

    Returns:
        DataIntegrationProcessor: åˆå§‹åŒ–å®Œæˆçš„è™•ç†å™¨å¯¦ä¾‹
    """
    return DataIntegrationProcessor(config)