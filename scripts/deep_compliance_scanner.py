#!/usr/bin/env python3
"""
æ·±åº¦å­¸è¡“åˆè¦æƒæå™¨ v1.0

å°ˆç‚ºç™¼ç¾éš±è—é•è¦è€Œè¨­è¨ˆçš„é«˜ç´šæƒæå·¥å…·
åŒ…å«èªç¾©åˆ†æã€ä¾è³´é—œä¿‚åˆ†æã€é…ç½®æ–‡ä»¶æª¢æŸ¥ç­‰å¤šå±¤æ¬¡æƒæ
"""

import re
import ast
import json
import yaml
import logging
import importlib
from pathlib import Path
from typing import Dict, List, Any, Set
from dataclasses import dataclass
from datetime import datetime

@dataclass
class ViolationDetails:
    type: str
    severity: str  # CRITICAL, HIGH, MEDIUM, LOW
    file_path: str
    line_number: int
    context: str
    description: str
    suggestion: str

class DeepComplianceScanner:
    """æ·±åº¦åˆè¦æƒæå™¨"""

    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.violations: List[ViolationDetails] = []

        # æ“´å±•çš„é•è¦æ¨¡å¼åº«
        self.critical_patterns = {
            'hardcoded_physics_constants': [
                r'299792458',           # å…‰é€Ÿ
                r'6\.67430e-11',        # é‡åŠ›å¸¸æ•¸
                r'9\.80665',            # æ¨™æº–é‡åŠ›åŠ é€Ÿåº¦
                r'1\.380649e-23',       # ç»çˆ¾èŒ²æ›¼å¸¸æ•¸
            ],
            'hidden_mock_usage': [
                r'[Mm]ock[A-Z]\w+',     # MockPrediction, MockData
                r'\w+[Mm]ock\w*',       # DataMock, TestMock
                r'[Ff]ake[A-Z]\w+',     # FakeData, FakeService
                r'[Ss]imulat\w+',       # Simulation, Simulator
                r'[Dd]ummy\w*',         # Dummy, DummyData
            ],
            'test_module_leakage': [
                r'from.*testing.*import',
                r'import.*test\w+',
                r'from.*test.*import',
                r'import.*mock',
            ],
            'random_data_generation': [
                r'random\.\w+',
                r'numpy\.random\.\w+',
                r'np\.random\.\w+',
                r'randint\(',
                r'uniform\(',
                r'normal\(',
            ]
        }

        self.high_patterns = {
            'undocumented_hardcoded_values': [
                r'>=?\s*\d+\.?\d*(?![e\-\+])',  # >= 70, > 5.0
                r'==?\s*\d+\.?\d*(?![e\-\+])',  # == 0, = 5
                r'\*\s*\d+\.?\d*(?![e\-\+])',   # * 2, * 1.5
                r'/\s*\d+\.?\d*(?![e\-\+])',    # / 1000
            ],
            'magic_numbers': [
                r'range\(\d+,\s*\d+\)',         # range(0, 100)
                r'sleep\(\d+\.?\d*\)',          # sleep(5)
                r'timeout=\d+\.?\d*',           # timeout=30
            ],
            'configuration_hardcoding': [
                r'elevation.*[0-9]+\.?[0-9]*',  # elevationé–€æª»ç¡¬ç·¨ç¢¼
                r'rsrp.*-?[0-9]+\.?[0-9]*',     # RSRPç¡¬ç·¨ç¢¼
                r'frequency.*[0-9]+\.?[0-9]*',  # é »ç‡ç¡¬ç·¨ç¢¼
            ]
        }

    def scan_project(self, project_path: Path) -> Dict[str, Any]:
        """åŸ·è¡Œå®Œæ•´é …ç›®æƒæ"""
        self.logger.info("ğŸ” é–‹å§‹æ·±åº¦åˆè¦æƒæ...")

        scan_results = {
            'scan_metadata': {
                'timestamp': datetime.now().isoformat(),
                'tool_version': '1.0',
                'scan_type': 'deep_compliance',
                'project_path': str(project_path)
            },
            'violations': [],
            'scan_statistics': {
                'files_scanned': 0,
                'critical_violations': 0,
                'high_violations': 0,
                'medium_violations': 0,
                'low_violations': 0
            },
            'scan_coverage': {
                'python_files': 0,
                'config_files': 0,
                'test_files': 0,
                'other_files': 0
            }
        }

        # 1. æƒæPythonæºä»£ç¢¼
        self._scan_python_files(project_path, scan_results)

        # 2. æƒæé…ç½®æ–‡ä»¶
        self._scan_config_files(project_path, scan_results)

        # 3. åˆ†æä¾è³´é—œä¿‚
        self._analyze_dependencies(project_path, scan_results)

        # 4. èªç¾©åˆ†æ
        self._semantic_analysis(project_path, scan_results)

        # 5. äº¤å‰å¼•ç”¨æª¢æŸ¥
        self._cross_reference_check(project_path, scan_results)

        # æ•´ç†çµæœ
        scan_results['violations'] = [
            {
                'type': v.type,
                'severity': v.severity,
                'file_path': v.file_path,
                'line_number': v.line_number,
                'context': v.context,
                'description': v.description,
                'suggestion': v.suggestion
            }
            for v in self.violations
        ]

        # çµ±è¨ˆ
        for violation in self.violations:
            scan_results['scan_statistics'][f"{violation.severity.lower()}_violations"] += 1

        return scan_results

    def _scan_python_files(self, project_path: Path, results: Dict[str, Any]):
        """æƒæPythonæ–‡ä»¶"""
        python_files = list(project_path.rglob("*.py"))
        results['scan_coverage']['python_files'] = len(python_files)

        for py_file in python_files:
            if self._should_skip_file(py_file):
                continue

            try:
                content = py_file.read_text(encoding='utf-8')
                self._check_critical_patterns(content, py_file)
                self._check_high_patterns(content, py_file)
                self._ast_analysis(content, py_file)
                results['scan_statistics']['files_scanned'] += 1

            except Exception as e:
                self.logger.warning(f"âš ï¸ ç„¡æ³•æƒæ {py_file}: {e}")

    def _scan_config_files(self, project_path: Path, results: Dict[str, Any]):
        """æƒæé…ç½®æ–‡ä»¶"""
        config_patterns = ["*.yaml", "*.yml", "*.json", "*.toml", "*.ini"]
        config_files = []

        for pattern in config_patterns:
            config_files.extend(project_path.rglob(pattern))

        results['scan_coverage']['config_files'] = len(config_files)

        for config_file in config_files:
            try:
                content = config_file.read_text(encoding='utf-8')
                self._check_config_violations(content, config_file)

            except Exception as e:
                self.logger.warning(f"âš ï¸ ç„¡æ³•æƒæé…ç½®æ–‡ä»¶ {config_file}: {e}")

    def _check_critical_patterns(self, content: str, file_path: Path):
        """æª¢æŸ¥Criticalç´šåˆ¥æ¨¡å¼"""
        lines = content.split('\n')

        for pattern_type, patterns in self.critical_patterns.items():
            for pattern in patterns:
                for line_num, line in enumerate(lines, 1):
                    matches = re.finditer(pattern, line, re.IGNORECASE)
                    for match in matches:
                        # ä¸Šä¸‹æ–‡æ„ŸçŸ¥æª¢æŸ¥
                        if self._is_acceptable_context(content, match.start(), pattern_type):
                            continue

                        self.violations.append(ViolationDetails(
                            type=pattern_type,
                            severity="CRITICAL",
                            file_path=str(file_path),
                            line_number=line_num,
                            context=line.strip(),
                            description=f"ç™¼ç¾Criticalç´šåˆ¥é•è¦: {match.group()}",
                            suggestion=self._get_suggestion(pattern_type, match.group())
                        ))

    def _check_high_patterns(self, content: str, file_path: Path):
        """æª¢æŸ¥Highç´šåˆ¥æ¨¡å¼"""
        lines = content.split('\n')

        for pattern_type, patterns in self.high_patterns.items():
            for pattern in patterns:
                for line_num, line in enumerate(lines, 1):
                    matches = re.finditer(pattern, line, re.IGNORECASE)
                    for match in matches:
                        if self._is_acceptable_context(content, match.start(), pattern_type):
                            continue

                        self.violations.append(ViolationDetails(
                            type=pattern_type,
                            severity="HIGH",
                            file_path=str(file_path),
                            line_number=line_num,
                            context=line.strip(),
                            description=f"ç™¼ç¾Highç´šåˆ¥é•è¦: {match.group()}",
                            suggestion=self._get_suggestion(pattern_type, match.group())
                        ))

    def _check_config_violations(self, content: str, file_path: Path):
        """æª¢æŸ¥é…ç½®æ–‡ä»¶é•è¦"""
        try:
            # å˜—è©¦è§£æç‚ºYAML/JSON
            if file_path.suffix in ['.yaml', '.yml']:
                config_data = yaml.safe_load(content)
            elif file_path.suffix == '.json':
                config_data = json.loads(content)
            else:
                return

            # æª¢æŸ¥é…ç½®ä¸­çš„ç¡¬ç·¨ç¢¼å€¼
            self._check_config_hardcoded_values(config_data, file_path, "")

        except Exception as e:
            self.logger.debug(f"é…ç½®æ–‡ä»¶è§£æå¤±æ•— {file_path}: {e}")

    def _check_config_hardcoded_values(self, data: Any, file_path: Path, key_path: str):
        """éæ­¸æª¢æŸ¥é…ç½®æ–‡ä»¶ä¸­çš„ç¡¬ç·¨ç¢¼å€¼"""
        if isinstance(data, dict):
            for key, value in data.items():
                new_path = f"{key_path}.{key}" if key_path else key
                self._check_config_hardcoded_values(value, file_path, new_path)

        elif isinstance(data, list):
            for i, item in enumerate(data):
                new_path = f"{key_path}[{i}]"
                self._check_config_hardcoded_values(item, file_path, new_path)

        elif isinstance(data, (int, float)):
            # æª¢æŸ¥æ˜¯å¦ç‚ºå¯ç–‘çš„ç¡¬ç·¨ç¢¼å€¼
            if self._is_suspicious_config_value(data, key_path):
                self.violations.append(ViolationDetails(
                    type="config_hardcoded_value",
                    severity="HIGH",
                    file_path=str(file_path),
                    line_number=0,  # é…ç½®æ–‡ä»¶é›£ä»¥ç¢ºå®šè¡Œè™Ÿ
                    context=f"{key_path}: {data}",
                    description=f"é…ç½®æ–‡ä»¶ä¸­çš„å¯ç–‘ç¡¬ç·¨ç¢¼å€¼: {data}",
                    suggestion="æ·»åŠ å­¸è¡“ä¾æ“šè¨»é‡‹æˆ–ç§»è‡³æ¨™æº–å¸¸æ•¸é¡"
                ))

    def _ast_analysis(self, content: str, file_path: Path):
        """ASTèªæ³•æ¨¹åˆ†æ"""
        try:
            tree = ast.parse(content)
            visitor = ViolationASTVisitor(file_path, self)
            visitor.visit(tree)

        except SyntaxError:
            # èªæ³•éŒ¯èª¤çš„æ–‡ä»¶è·³é
            pass

    def _analyze_dependencies(self, project_path: Path, results: Dict[str, Any]):
        """åˆ†æä¾è³´é—œä¿‚"""
        # æª¢æŸ¥importèªå¥ä¸­çš„å¯ç–‘ä¾è³´
        for py_file in project_path.rglob("*.py"):
            if self._should_skip_file(py_file):
                continue

            try:
                content = py_file.read_text(encoding='utf-8')
                self._check_suspicious_imports(content, py_file)

            except Exception as e:
                continue

    def _semantic_analysis(self, project_path: Path, results: Dict[str, Any]):
        """èªç¾©åˆ†æ"""
        # æª¢æŸ¥è®Šæ•¸åå’Œå‡½æ•¸åä¸­çš„å¯ç–‘æ¨¡å¼
        for py_file in project_path.rglob("*.py"):
            if self._should_skip_file(py_file):
                continue

            try:
                content = py_file.read_text(encoding='utf-8')
                self._check_semantic_violations(content, py_file)

            except Exception as e:
                continue

    def _cross_reference_check(self, project_path: Path, results: Dict[str, Any]):
        """äº¤å‰å¼•ç”¨æª¢æŸ¥"""
        # æª¢æŸ¥æ˜¯å¦æœ‰å¾ªç’°ä¾è³´æˆ–ä¸ç•¶å¼•ç”¨
        pass

    def _is_acceptable_context(self, content: str, position: int, pattern_type: str) -> bool:
        """ä¸Šä¸‹æ–‡æ„ŸçŸ¥æª¢æŸ¥"""
        # æå–å‘¨åœä¸Šä¸‹æ–‡
        start = max(0, position - 100)
        end = min(len(content), position + 100)
        context = content[start:end]

        # æª¢æŸ¥æ˜¯å¦åœ¨è¨»é‡‹ä¸­
        if any(marker in context for marker in ['#', '"""', "'''"]):
            return True

        # æª¢æŸ¥æ˜¯å¦ç‚ºå¸¸æ•¸å®šç¾©
        if 'SPEED_OF_LIGHT' in context or 'physics_constants' in context:
            return True

        # æª¢æŸ¥æ˜¯å¦ç‚ºé©—è­‰/æª¢æ¸¬é‚è¼¯
        detection_indicators = [
            'check', 'validate', 'detect', 'scan', 'verify',
            'test', 'assert', 'ensure', 'confirm'
        ]
        if any(indicator in context.lower() for indicator in detection_indicators):
            return True

        return False

    def _is_suspicious_config_value(self, value: float, key_path: str) -> bool:
        """åˆ¤æ–·é…ç½®å€¼æ˜¯å¦å¯ç–‘"""
        # å¸¸è¦‹çš„å¯ç–‘å€¼
        suspicious_values = [5.0, 10.0, 15.0, -85, -70, -100, 45.0]

        if value in suspicious_values:
            # æª¢æŸ¥æ˜¯å¦æœ‰å­¸è¡“ä¾æ“šæ¨™è¨»
            suspicious_keys = ['elevation', 'rsrp', 'threshold', 'timeout', 'percentage']
            if any(key in key_path.lower() for key in suspicious_keys):
                return True

        return False

    def _check_suspicious_imports(self, content: str, file_path: Path):
        """æª¢æŸ¥å¯ç–‘çš„importèªå¥"""
        lines = content.split('\n')

        for line_num, line in enumerate(lines, 1):
            # æª¢æŸ¥æ¸¬è©¦æ¨¡çµ„æ´©æ¼
            if re.search(r'from.*testing.*import|import.*test\w+', line):
                if not self._is_test_file(file_path):
                    self.violations.append(ViolationDetails(
                        type="test_module_leakage",
                        severity="MEDIUM",
                        file_path=str(file_path),
                        line_number=line_num,
                        context=line.strip(),
                        description="ç”Ÿç”¢ä»£ç¢¼ä¸­å¼•å…¥æ¸¬è©¦æ¨¡çµ„",
                        suggestion="ç§»é™¤æ¸¬è©¦æ¨¡çµ„å¼•ç”¨æˆ–ç§»è‡³æ¸¬è©¦ä»£ç¢¼"
                    ))

    def _check_semantic_violations(self, content: str, file_path: Path):
        """æª¢æŸ¥èªç¾©é•è¦"""
        # æª¢æŸ¥è®Šæ•¸å‘½åä¸­çš„å¯ç–‘æ¨¡å¼
        suspicious_names = [
            r'\w*mock\w*', r'\w*fake\w*', r'\w*dummy\w*',
            r'\w*test\w*', r'\w*temp\w*', r'\w*tmp\w*'
        ]

        lines = content.split('\n')
        for line_num, line in enumerate(lines, 1):
            for pattern in suspicious_names:
                matches = re.finditer(pattern, line, re.IGNORECASE)
                for match in matches:
                    # æ’é™¤åˆç†ç”¨æ³•
                    if self._is_legitimate_usage(match.group(), line):
                        continue

                    self.violations.append(ViolationDetails(
                        type="suspicious_naming",
                        severity="MEDIUM",
                        file_path=str(file_path),
                        line_number=line_num,
                        context=line.strip(),
                        description=f"å¯ç–‘çš„è®Šæ•¸å‘½å: {match.group()}",
                        suggestion="ä½¿ç”¨æ›´å…·æè¿°æ€§çš„å‘½å"
                    ))

    def _should_skip_file(self, file_path: Path) -> bool:
        """åˆ¤æ–·æ˜¯å¦æ‡‰è©²è·³éæ–‡ä»¶"""
        skip_patterns = [
            '__pycache__', '.pyc', '.git', '.venv', 'venv',
            'node_modules', '.pytest_cache', '.coverage'
        ]

        return any(pattern in str(file_path) for pattern in skip_patterns)

    def _is_test_file(self, file_path: Path) -> bool:
        """åˆ¤æ–·æ˜¯å¦ç‚ºæ¸¬è©¦æ–‡ä»¶"""
        test_indicators = ['test_', '_test.py', '/tests/', '/test/']
        return any(indicator in str(file_path) for indicator in test_indicators)

    def _is_legitimate_usage(self, name: str, context: str) -> bool:
        """åˆ¤æ–·æ˜¯å¦ç‚ºåˆç†ç”¨æ³•"""
        # æª¢æŸ¥æ˜¯å¦åœ¨è¨»é‡‹æˆ–æ–‡æª”å­—ç¬¦ä¸²ä¸­
        if '#' in context or '"""' in context or "'''" in context:
            return True

        # æª¢æŸ¥æ˜¯å¦ç‚ºç¦æ­¢æ€§èªå¥
        prohibitive_patterns = ['ç¦æ­¢', 'not', 'avoid', 'prevent', 'ä¸å¾—']
        if any(pattern in context for pattern in prohibitive_patterns):
            return True

        return False

    def _get_suggestion(self, pattern_type: str, matched_text: str) -> str:
        """ç²å–ä¿®å¾©å»ºè­°"""
        suggestions = {
            'hardcoded_physics_constants': "ä½¿ç”¨ shared.constants.physics_constants ä¸­çš„æ¨™æº–å¸¸æ•¸",
            'hidden_mock_usage': "ç§»é™¤Mockä½¿ç”¨ï¼Œæ”¹ç”¨çœŸå¯¦æ•¸æ“šå’Œç®—æ³•",
            'test_module_leakage': "ç§»é™¤æ¸¬è©¦æ¨¡çµ„å¼•ç”¨ï¼Œæˆ–å°‡ä»£ç¢¼ç§»è‡³æ¸¬è©¦ç›®éŒ„",
            'random_data_generation': "ä½¿ç”¨ç¢ºå®šæ€§ç®—æ³•æ›¿ä»£éš¨æ©Ÿæ•¸ç”Ÿæˆ",
            'undocumented_hardcoded_values': "ç§»è‡³å¸¸æ•¸é¡ä¸¦æ·»åŠ å­¸è¡“ä¾æ“šè¨»é‡‹",
            'magic_numbers': "å®šç¾©ç‚ºå‘½åå¸¸æ•¸ä¸¦æä¾›å­¸è¡“ä¾æ“š",
            'configuration_hardcoding': "åœ¨é…ç½®æ–‡ä»¶ä¸­æ·»åŠ å­¸è¡“æ¨™æº–å¼•ç”¨è¨»é‡‹"
        }

        return suggestions.get(pattern_type, "è«‹æŸ¥é–±å­¸è¡“æ¨™æº–åˆè¦æŒ‡å—")


class ViolationASTVisitor(ast.NodeVisitor):
    """ASTè¨ªå•å™¨ç”¨æ–¼æ·±åº¦èªæ³•åˆ†æ"""

    def __init__(self, file_path: Path, scanner: DeepComplianceScanner):
        self.file_path = file_path
        self.scanner = scanner

    def visit_Constant(self, node):
        """æª¢æŸ¥å¸¸æ•¸ç¯€é»"""
        if isinstance(node.value, (int, float)):
            # æª¢æŸ¥æ˜¯å¦ç‚ºå¯ç–‘çš„ç¡¬ç·¨ç¢¼å€¼
            if self._is_suspicious_constant(node.value):
                self.scanner.violations.append(ViolationDetails(
                    type="ast_hardcoded_constant",
                    severity="MEDIUM",
                    file_path=str(self.file_path),
                    line_number=node.lineno,
                    context=f"å¸¸æ•¸å€¼: {node.value}",
                    description=f"ASTæª¢æ¸¬åˆ°å¯ç–‘å¸¸æ•¸: {node.value}",
                    suggestion="è€ƒæ…®å°‡å¸¸æ•¸ç§»è‡³å°ˆç”¨å¸¸æ•¸é¡"
                ))

        self.generic_visit(node)

    def _is_suspicious_constant(self, value) -> bool:
        """åˆ¤æ–·å¸¸æ•¸æ˜¯å¦å¯ç–‘"""
        # å¸¸è¦‹çš„å¯ç–‘å¸¸æ•¸å€¼
        suspicious_constants = [
            299792458,    # å…‰é€Ÿ
            5, 10, 15,    # å¸¸è¦‹ä»°è§’é–€æª»
            -85, -70, -100, # å¸¸è¦‹RSRPé–€æª»
            96, 109,      # è»Œé“é€±æœŸ
            1000, 1e3, 1e6, 1e9  # å–®ä½è½‰æ›
        ]

        return value in suspicious_constants


def main():
    """ä¸»å‡½æ•¸"""
    import argparse

    parser = argparse.ArgumentParser(description="æ·±åº¦å­¸è¡“åˆè¦æƒæå™¨")
    parser.add_argument("--project-path", default=".", help="é …ç›®è·¯å¾‘")
    parser.add_argument("--output", default="deep_scan_report.json", help="è¼¸å‡ºæ–‡ä»¶")
    parser.add_argument("--verbose", action="store_true", help="è©³ç´°è¼¸å‡º")

    args = parser.parse_args()

    # è¨­ç½®æ—¥èªŒ
    logging.basicConfig(
        level=logging.INFO if args.verbose else logging.WARNING,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )

    # åŸ·è¡Œæƒæ
    scanner = DeepComplianceScanner()
    project_path = Path(args.project_path)

    print("ğŸ” é–‹å§‹æ·±åº¦åˆè¦æƒæ...")
    results = scanner.scan_project(project_path)

    # ä¿å­˜çµæœ
    with open(args.output, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    # å ±å‘Šçµæœ
    stats = results['scan_statistics']
    total_violations = sum([
        stats['critical_violations'],
        stats['high_violations'],
        stats['medium_violations'],
        stats['low_violations']
    ])

    print(f"ğŸ“Š æƒæå®Œæˆï¼")
    print(f"   æƒææ–‡ä»¶: {stats['files_scanned']}")
    print(f"   ç¸½é•è¦æ•¸: {total_violations}")
    print(f"   Critical: {stats['critical_violations']}")
    print(f"   High: {stats['high_violations']}")
    print(f"   Medium: {stats['medium_violations']}")
    print(f"   Low: {stats['low_violations']}")
    print(f"   å ±å‘Šä¿å­˜è‡³: {args.output}")

    if total_violations == 0:
        print("âœ… æ­å–œï¼æœªç™¼ç¾å­¸è¡“æ¨™æº–é•è¦")
    else:
        print("âš ï¸ ç™¼ç¾å­¸è¡“æ¨™æº–é•è¦ï¼Œè«‹æŸ¥çœ‹è©³ç´°å ±å‘Š")
        return 1

    return 0


if __name__ == "__main__":
    exit(main())