#!/usr/bin/env python3
"""
éšæ®µäºŒæ€§èƒ½åŸºæº–æ¸¬è©¦è…³æœ¬
æ¯”è¼ƒåŸå§‹ç‰ˆæœ¬vså„ªåŒ–ç‰ˆæœ¬çš„æ€§èƒ½å·®ç•°

ä½¿ç”¨æ–¹æ³•:
python scripts/benchmark_stage2_optimization.py [--sample-size 100] [--gpu] [--cpu-only]
"""

import os
import sys
import time
import argparse
import json
import logging
from datetime import datetime, timezone
from typing import Dict, Any, List

# æ·»åŠ é …ç›®æ ¹ç›®éŒ„åˆ°è·¯å¾‘
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
sys.path.insert(0, os.path.join(project_root, 'src'))

# å°å…¥è™•ç†å™¨
from stages.stage2_orbital_computing.stage2_orbital_computing_processor import Stage2OrbitalComputingProcessor
from stages.stage2_orbital_computing.optimized_stage2_processor import OptimizedStage2Processor

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class Stage2Benchmark:
    """éšæ®µäºŒæ€§èƒ½åŸºæº–æ¸¬è©¦å™¨"""

    def __init__(self, sample_size: int = None):
        self.sample_size = sample_size
        self.results = {}

    def load_stage1_output(self) -> Dict[str, Any]:
        """è¼‰å…¥éšæ®µä¸€è¼¸å‡ºä½œç‚ºæ¸¬è©¦æ•¸æ“š"""
        # å°‹æ‰¾æœ€æ–°çš„éšæ®µä¸€è¼¸å‡ºæ–‡ä»¶
        stage1_dir = os.path.join(project_root, 'data', 'outputs', 'stage1')

        if not os.path.exists(stage1_dir):
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°éšæ®µä¸€è¼¸å‡ºç›®éŒ„: {stage1_dir}")

        # æ‰¾æœ€æ–°çš„JSONæ–‡ä»¶
        json_files = [f for f in os.listdir(stage1_dir) if f.endswith('.json')]
        if not json_files:
            raise FileNotFoundError(f"éšæ®µä¸€è¼¸å‡ºç›®éŒ„ä¸­æ²’æœ‰JSONæ–‡ä»¶: {stage1_dir}")

        # ä½¿ç”¨æœ€æ–°çš„æ–‡ä»¶
        latest_file = sorted(json_files)[-1]
        stage1_output_path = os.path.join(stage1_dir, latest_file)

        if not os.path.exists(stage1_output_path):
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°éšæ®µä¸€è¼¸å‡ºæ–‡ä»¶: {stage1_output_path}")

        with open(stage1_output_path, 'r', encoding='utf-8') as f:
            stage1_data = json.load(f)

        # å¦‚æœæŒ‡å®šäº†æ¨£æœ¬å¤§å°ï¼Œé€²è¡Œæ¡æ¨£
        if self.sample_size and 'data' in stage1_data:
            tle_data = stage1_data['data'].get('tle_data', [])
            if len(tle_data) > self.sample_size:
                tle_data = tle_data[:self.sample_size]
                stage1_data['data']['tle_data'] = tle_data
                logger.info(f"ğŸ“Š æ¡æ¨£æ¨¡å¼: ä½¿ç”¨ {self.sample_size} é¡†è¡›æ˜Ÿé€²è¡Œæ¸¬è©¦")

        logger.info(f"ğŸ“ è¼‰å…¥éšæ®µä¸€è¼¸å‡º: {stage1_output_path}")
        logger.info(f"ğŸ“Š è¡›æ˜Ÿæ•¸é‡: {len(stage1_data.get('data', {}).get('tle_data', []))}")

        return stage1_data

    def benchmark_original_processor(self, stage1_output: Dict[str, Any]) -> Dict[str, Any]:
        """åŸºæº–æ¸¬è©¦åŸå§‹è™•ç†å™¨"""
        logger.info("ğŸ“Š é–‹å§‹æ¸¬è©¦åŸå§‹éšæ®µäºŒè™•ç†å™¨...")
        start_time = time.time()

        try:
            # å‰µå»ºåŸå§‹è™•ç†å™¨
            original_processor = Stage2OrbitalComputingProcessor()

            # åŸ·è¡Œè™•ç†
            result = original_processor.execute(stage1_output)

            execution_time = time.time() - start_time

            benchmark_result = {
                'processor_type': 'original',
                'execution_time': execution_time,
                'success': True,
                'error': None,
                'satellites_processed': result.get('metadata', {}).get('input_count', 0),
                'visible_satellites': result.get('metadata', {}).get('output_count', 0),
                'memory_usage': self._get_memory_usage()
            }

            logger.info(f"âœ… åŸå§‹è™•ç†å™¨å®Œæˆ: {execution_time:.2f}ç§’")

        except Exception as e:
            execution_time = time.time() - start_time
            benchmark_result = {
                'processor_type': 'original',
                'execution_time': execution_time,
                'success': False,
                'error': str(e),
                'satellites_processed': 0,
                'visible_satellites': 0,
                'memory_usage': self._get_memory_usage()
            }

            logger.error(f"âŒ åŸå§‹è™•ç†å™¨å¤±æ•—: {e}")

        return benchmark_result

    def benchmark_optimized_processor(self, stage1_output: Dict[str, Any],
                                    enable_gpu: bool = True) -> Dict[str, Any]:
        """åŸºæº–æ¸¬è©¦å„ªåŒ–è™•ç†å™¨"""
        logger.info(f"ğŸš€ é–‹å§‹æ¸¬è©¦å„ªåŒ–éšæ®µäºŒè™•ç†å™¨ (GPU: {enable_gpu})...")
        start_time = time.time()

        try:
            # å‰µå»ºå„ªåŒ–è™•ç†å™¨
            optimized_processor = OptimizedStage2Processor(enable_optimization=True)

            # å¦‚æœä¸ä½¿ç”¨GPUï¼Œç¦ç”¨GPUçµ„ä»¶
            if not enable_gpu and hasattr(optimized_processor, 'parallel_sgp4'):
                optimized_processor.parallel_sgp4.config.enable_gpu = False
                optimized_processor.gpu_converter = None

            # åŸ·è¡Œè™•ç†
            result = optimized_processor.execute(stage1_output)

            execution_time = time.time() - start_time

            # æå–å„ªåŒ–æŒ‡æ¨™
            optimization_metrics = result.get('optimization_metrics', {})

            benchmark_result = {
                'processor_type': 'optimized',
                'execution_time': execution_time,
                'success': True,
                'error': None,
                'satellites_processed': result.get('metadata', {}).get('input_count', 0),
                'visible_satellites': result.get('metadata', {}).get('output_count', 0),
                'memory_usage': self._get_memory_usage(),
                'optimization_metrics': optimization_metrics,
                'gpu_enabled': enable_gpu,
                'gpu_used': optimization_metrics.get('performance_breakdown', {}).get('gpu_acceleration_used', False),
                'cpu_parallel_used': optimization_metrics.get('performance_breakdown', {}).get('cpu_parallel_used', False)
            }

            logger.info(f"âœ… å„ªåŒ–è™•ç†å™¨å®Œæˆ: {execution_time:.2f}ç§’")

        except Exception as e:
            execution_time = time.time() - start_time
            benchmark_result = {
                'processor_type': 'optimized',
                'execution_time': execution_time,
                'success': False,
                'error': str(e),
                'satellites_processed': 0,
                'visible_satellites': 0,
                'memory_usage': self._get_memory_usage(),
                'gpu_enabled': enable_gpu,
                'gpu_used': False,
                'cpu_parallel_used': False
            }

            logger.error(f"âŒ å„ªåŒ–è™•ç†å™¨å¤±æ•—: {e}")

        return benchmark_result

    def _get_memory_usage(self) -> Dict[str, float]:
        """ç²å–è¨˜æ†¶é«”ä½¿ç”¨æƒ…æ³"""
        try:
            import psutil
            process = psutil.Process()
            memory_info = process.memory_info()

            return {
                'rss_mb': memory_info.rss / (1024 * 1024),
                'vms_mb': memory_info.vms / (1024 * 1024)
            }
        except ImportError:
            return {'rss_mb': 0, 'vms_mb': 0}

    def run_comparison_benchmark(self, test_gpu: bool = True,
                               test_cpu_only: bool = True) -> Dict[str, Any]:
        """é‹è¡Œæ¯”è¼ƒåŸºæº–æ¸¬è©¦"""
        logger.info("ğŸ¯ é–‹å§‹éšæ®µäºŒæ€§èƒ½æ¯”è¼ƒæ¸¬è©¦...")

        # è¼‰å…¥æ¸¬è©¦æ•¸æ“š
        stage1_output = self.load_stage1_output()

        # æ¸¬è©¦é…ç½®
        test_configs = [
            {'name': 'original', 'func': self.benchmark_original_processor, 'args': [stage1_output]}
        ]

        if test_gpu:
            test_configs.append({
                'name': 'optimized_gpu',
                'func': self.benchmark_optimized_processor,
                'args': [stage1_output, True]
            })

        if test_cpu_only:
            test_configs.append({
                'name': 'optimized_cpu',
                'func': self.benchmark_optimized_processor,
                'args': [stage1_output, False]
            })

        # åŸ·è¡Œæ¸¬è©¦
        benchmark_results = {}
        for config in test_configs:
            logger.info(f"\nğŸ”¬ æ¸¬è©¦é…ç½®: {config['name']}")
            result = config['func'](*config['args'])
            benchmark_results[config['name']] = result

        # ç”Ÿæˆæ¯”è¼ƒå ±å‘Š
        comparison_report = self._generate_comparison_report(benchmark_results)

        # å®Œæ•´æ¸¬è©¦çµæœ
        full_results = {
            'benchmark_timestamp': datetime.now(timezone.utc).isoformat(),
            'test_configuration': {
                'sample_size': self.sample_size,
                'test_gpu': test_gpu,
                'test_cpu_only': test_cpu_only
            },
            'individual_results': benchmark_results,
            'comparison_report': comparison_report
        }

        return full_results

    def _generate_comparison_report(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆæ¯”è¼ƒå ±å‘Š"""
        report = {
            'performance_comparison': {},
            'recommendations': []
        }

        # ç²å–åŸºæº–æ™‚é–“ï¼ˆåŸå§‹è™•ç†å™¨ï¼‰
        baseline_time = None
        if 'original' in results and results['original']['success']:
            baseline_time = results['original']['execution_time']

        # è¨ˆç®—æ€§èƒ½æ”¹å–„
        for test_name, result in results.items():
            if test_name == 'original' or not result['success']:
                continue

            if baseline_time and baseline_time > 0:
                speedup = baseline_time / result['execution_time']
                improvement = ((baseline_time - result['execution_time']) / baseline_time) * 100

                report['performance_comparison'][test_name] = {
                    'execution_time': result['execution_time'],
                    'speedup_ratio': speedup,
                    'improvement_percentage': improvement,
                    'time_saved_seconds': baseline_time - result['execution_time']
                }

                # ç”Ÿæˆå»ºè­°
                if speedup > 3:
                    report['recommendations'].append(f"ğŸš€ {test_name}: é¡¯è‘—æ€§èƒ½æ”¹å–„ ({speedup:.1f}x)")
                elif speedup > 1.5:
                    report['recommendations'].append(f"âœ… {test_name}: è‰¯å¥½æ€§èƒ½æ”¹å–„ ({speedup:.1f}x)")
                else:
                    report['recommendations'].append(f"âš ï¸ {test_name}: æ€§èƒ½æ”¹å–„æœ‰é™ ({speedup:.1f}x)")

        return report

    def save_results(self, results: Dict[str, Any], output_path: str = None):
        """ä¿å­˜æ¸¬è©¦çµæœ"""
        if output_path is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = os.path.join(project_root, 'data', f'stage2_benchmark_{timestamp}.json')

        os.makedirs(os.path.dirname(output_path), exist_ok=True)

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)

        logger.info(f"ğŸ“Š æ¸¬è©¦çµæœå·²ä¿å­˜: {output_path}")

    def print_summary(self, results: Dict[str, Any]):
        """æ‰“å°æ¸¬è©¦æ‘˜è¦"""
        print("\n" + "="*60)
        print("ğŸ¯ éšæ®µäºŒæ€§èƒ½åŸºæº–æ¸¬è©¦æ‘˜è¦")
        print("="*60)

        comparison = results.get('comparison_report', {}).get('performance_comparison', {})

        for test_name, metrics in comparison.items():
            print(f"\nğŸ“Š {test_name.upper()}:")
            print(f"  â±ï¸  åŸ·è¡Œæ™‚é–“: {metrics['execution_time']:.2f}ç§’")
            print(f"  ğŸš€ æ€§èƒ½æå‡: {metrics['speedup_ratio']:.2f}x")
            print(f"  ğŸ“ˆ æ”¹å–„ç™¾åˆ†æ¯”: {metrics['improvement_percentage']:.1f}%")
            print(f"  ğŸ’¾ ç¯€çœæ™‚é–“: {metrics['time_saved_seconds']:.1f}ç§’")

        print(f"\nğŸ–ï¸ å»ºè­°:")
        for recommendation in results.get('comparison_report', {}).get('recommendations', []):
            print(f"  {recommendation}")

        print("\n" + "="*60)


def main():
    parser = argparse.ArgumentParser(description='éšæ®µäºŒæ€§èƒ½åŸºæº–æ¸¬è©¦')
    parser.add_argument('--sample-size', type=int, default=None,
                       help='æ¸¬è©¦æ¨£æœ¬å¤§å°ï¼ˆè¡›æ˜Ÿæ•¸é‡ï¼‰')
    parser.add_argument('--gpu', action='store_true', default=True,
                       help='æ¸¬è©¦GPUåŠ é€Ÿç‰ˆæœ¬')
    parser.add_argument('--cpu-only', action='store_true', default=True,
                       help='æ¸¬è©¦CPUä¸¦è¡Œç‰ˆæœ¬')
    parser.add_argument('--original-only', action='store_true',
                       help='åªæ¸¬è©¦åŸå§‹ç‰ˆæœ¬')
    parser.add_argument('--output', type=str, default=None,
                       help='çµæœè¼¸å‡ºè·¯å¾‘')

    args = parser.parse_args()

    # å‰µå»ºåŸºæº–æ¸¬è©¦å™¨
    benchmark = Stage2Benchmark(sample_size=args.sample_size)

    if args.original_only:
        # åªæ¸¬è©¦åŸå§‹ç‰ˆæœ¬
        stage1_output = benchmark.load_stage1_output()
        result = benchmark.benchmark_original_processor(stage1_output)
        print(f"åŸå§‹è™•ç†å™¨åŸ·è¡Œæ™‚é–“: {result['execution_time']:.2f}ç§’")
    else:
        # é‹è¡Œå®Œæ•´æ¯”è¼ƒæ¸¬è©¦
        results = benchmark.run_comparison_benchmark(
            test_gpu=args.gpu,
            test_cpu_only=args.cpu_only
        )

        # ä¿å­˜çµæœ
        benchmark.save_results(results, args.output)

        # æ‰“å°æ‘˜è¦
        benchmark.print_summary(results)


if __name__ == '__main__':
    main()