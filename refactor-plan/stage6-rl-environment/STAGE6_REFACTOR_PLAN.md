# ğŸ¤– Stage 6: RLç’°å¢ƒæ¨™æº–åŒ–è¨ˆåŠƒ

## ğŸ“‹ éšæ®µæ¦‚è¦½

**ç›®æ¨™**ï¼šå»ºç«‹æ¨™æº–åŒ–çš„å¼·åŒ–å­¸ç¿’ç’°å¢ƒï¼Œæ”¯æ´è¡›æ˜Ÿæ›æ‰‹å„ªåŒ–ç ”ç©¶

**æ™‚ç¨‹**ï¼šç¬¬4é€±å¾ŒåŠ + ç¬¬5é€± (5å€‹å·¥ä½œæ—¥)

**å„ªå…ˆç´š**ï¼šğŸš¨ é«˜é¢¨éšª - ç¼ºä¹æ¨™æº–RLæ¥å£

**ç¾ç‹€å•é¡Œ**ï¼šç¼ºä¹RLæ¨™æº–æ¥å£ï¼Œç„¡gymnasiumæ•´åˆï¼Œç„¡æ³•é€²è¡Œæ¨™æº–åŒ–RLç ”ç©¶

## ğŸ¯ é‡æ§‹ç›®æ¨™

### æ ¸å¿ƒç›®æ¨™
- âœ… **Gymnasiumæ¨™æº–ç’°å¢ƒ**: ç¬¦åˆRLç¤¾ç¾¤æ¨™æº–çš„ç’°å¢ƒæ¥å£
- âœ… **å¤šç®—æ³•æ”¯æ´**: DQN/PPO/SAC/A2Cç­‰ä¸»æµRLç®—æ³•
- âœ… **çœŸå¯¦æ•¸æ“šæ•´åˆ**: åŸºæ–¼å‰äº”éšæ®µçš„çœŸå¯¦è»Œé“èˆ‡ä¿¡è™Ÿæ•¸æ“š
- âœ… **å­¸è¡“ç ”ç©¶å°å‘**: æ”¯æ´è«–æ–‡ç™¼è¡¨çš„æ¨™æº–åŒ–RLå¯¦é©—

### å­¸è¡“ç ”ç©¶è¦æ±‚ (åŸºæ–¼ docs/final.md)
- **å¤šç®—æ³•æ”¯æ´**: DQN/A3C/PPO/SACç­‰ä¸»æµç®—æ³•æ¯”è¼ƒ
- **å¯¦æ™‚æ±ºç­–æ”¯æ´**: æ¯«ç§’ç´šéŸ¿æ‡‰çš„æ›æ‰‹æ±ºç­–æ¨ç†
- **è¨“ç·´æ•¸æ“šç”Ÿæˆ**: åŸºæ–¼Stage 5çš„3GPPäº‹ä»¶è§¸ç™¼æ¢ä»¶
- **å°æ¯”ç ”ç©¶**: èˆ‡å‚³çµ±MLæ–¹æ³•çš„æ€§èƒ½å°æ¯”èƒ½åŠ›

## ğŸ”§ æŠ€è¡“å¯¦ç¾

### å¥—ä»¶é¸æ“‡ç†ç”±

#### Gymnasium (RLç’°å¢ƒæ¨™æº–)
```python
# RLç¤¾ç¾¤æ¨™æº–å„ªå‹¢
âœ… OpenAI Gymçš„å®˜æ–¹ç¹¼æ‰¿è€…
âœ… RLç ”ç©¶ç¤¾ç¾¤æ¨™æº–æ¥å£
âœ… å®Œæ•´çš„ç’°å¢ƒé©—è­‰å·¥å…·
âœ… è±å¯Œçš„æ–‡æª”èˆ‡ç¯„ä¾‹
âœ… èˆ‡ä¸»æµRLåº«å®Œç¾æ•´åˆ
```

#### Stable-Baselines3 (SOTA RLç®—æ³•åº«)
```python
# å­¸è¡“ç´šRLç®—æ³•
âœ… DQN/PPO/SAC/A2Cæ¨™æº–å¯¦ç¾
âœ… å­¸è¡“ç ”ç©¶å“è³ªä¿è­‰
âœ… å®Œæ•´çš„è¨“ç·´ç®¡é“
âœ… ç®—æ³•æ€§èƒ½å°æ¯”èƒ½åŠ›
âœ… è«–æ–‡ç™¼è¡¨ç´šå¯¦ç¾
```

#### Scikit-learn (å‚³çµ±MLå°æ¯”)
```python
# å°æ¯”ç ”ç©¶æ”¯æ´
âœ… Random Forestç­‰å‚³çµ±ç®—æ³•
âœ… æ¨™æº–åŒ–MLç®¡é“
âœ… ç‰¹å¾µå·¥ç¨‹å·¥å…·
âœ… èˆ‡RLçµæœå°æ¯”åˆ†æ
```

### æ–°æ¶æ§‹è¨­è¨ˆ

```python
# RLç’°å¢ƒæ¶æ§‹
rl_environment/
â”œâ”€â”€ gymnasium_env.py            # Gymnasiumæ¨™æº–ç’°å¢ƒ
â”œâ”€â”€ state_processor.py          # ç‹€æ…‹ç©ºé–“è™•ç†
â”œâ”€â”€ reward_calculator.py        # çå‹µå‡½æ•¸è¨ˆç®—
â”œâ”€â”€ algorithm_trainer.py        # å¤šç®—æ³•è¨“ç·´å™¨
â”œâ”€â”€ traditional_ml_baseline.py  # å‚³çµ±MLåŸºæº–
â””â”€â”€ experiment_manager.py       # å¯¦é©—ç®¡ç†å™¨
```

## ğŸ“… å¯¦æ–½è¨ˆåŠƒ (5å¤©)

### Day 1-2: Gymnasiumç’°å¢ƒæ ¸å¿ƒ
```bash
# å®‰è£RLæ ¸å¿ƒå¥—ä»¶
pip install gymnasium>=0.29.0
pip install stable-baselines3[extra]>=2.0.0
pip install scikit-learn>=1.3.0

# æ›¿æ›çµ„ä»¶
âœ… æ–°å¢ï¼šgymnasium.Envæ¨™æº–ç’°å¢ƒ
âœ… æ–°å¢ï¼šstable_baselines3ç®—æ³•åº«
âœ… æ–°å¢ï¼šå‚³çµ±MLå°æ¯”åŸºæº–
```

```python
# gymnasium_env.py Gymnasiumæ¨™æº–ç’°å¢ƒ
import gymnasium as gym
from gymnasium import spaces
import numpy as np
from typing import Dict, List, Tuple, Any

class SatelliteHandoverEnv(gym.Env):
    """è¡›æ˜Ÿæ›æ‰‹RLç’°å¢ƒ - Gymnasiumæ¨™æº–æ¥å£"""

    metadata = {"render_modes": ["human"], "render_fps": 4}

    def __init__(self, constellation_data: Dict, render_mode: str = None):
        super().__init__()

        self.constellation_data = constellation_data
        self.render_mode = render_mode

        # ç‹€æ…‹ç©ºé–“å®šç¾© (åŸºæ–¼å‰äº”éšæ®µæ•¸æ“š)
        # [æœå‹™è¡›æ˜Ÿç‹€æ…‹(9) + å€™é¸è¡›æ˜Ÿç‹€æ…‹(5Ã—3=15) + æ± ç‹€æ…‹(1) = 25ç¶­]
        self.observation_space = spaces.Box(
            low=-np.inf,
            high=np.inf,
            shape=(25,),
            dtype=np.float32
        )

        # å‹•ä½œç©ºé–“å®šç¾© (é›¢æ•£å‹•ä½œ)
        # [ä¿æŒç•¶å‰è¡›æ˜Ÿ, åˆ‡æ›è‡³å€™é¸è¡›æ˜Ÿ1-5] = 6å€‹å‹•ä½œ
        self.action_space = spaces.Discrete(6)

        # ç’°å¢ƒç‹€æ…‹
        self.current_serving_satellite = None
        self.candidate_satellites = []
        self.current_time_step = 0
        self.max_episode_steps = 100  # é™åˆ¶episodeé•·åº¦
        self.pool_state_data = None

        # çå‹µå‡½æ•¸é…ç½®
        self.reward_config = {
            'signal_quality_weight': 0.4,
            'handover_penalty': -1.0,
            'service_interruption_penalty': -10.0,
            'continuity_bonus': 2.0,
            'qos_threshold': -95.0  # RSRPé–€æª»
        }

    def _get_observation(self) -> np.ndarray:
        """ç²å–ç•¶å‰ç’°å¢ƒç‹€æ…‹ - æ•´åˆå‰äº”éšæ®µæ•¸æ“š"""

        # 1. æœå‹™è¡›æ˜Ÿç‰¹å¾µ (9ç¶­) - æ•´åˆStage 2è»Œé“ + Stage 3åº§æ¨™æ•¸æ“š
        if self.current_serving_satellite:
            serving_features = np.array([
                self.current_serving_satellite.position_teme[0],  # Xä½ç½®
                self.current_serving_satellite.position_teme[1],  # Yä½ç½®
                self.current_serving_satellite.position_teme[2],  # Zä½ç½®
                self.current_serving_satellite.elevation,         # ä»°è§’
                self.current_serving_satellite.azimuth,           # æ–¹ä½è§’
                self.current_serving_satellite.distance_km,       # è·é›¢
                self._calculate_rsrp(self.current_serving_satellite), # RSRP
                self._calculate_rsrq(self.current_serving_satellite), # RSRQ
                1.0 if self.current_serving_satellite.elevation > 5.0 else 0.0  # å¯è¦‹æ€§
            ])
        else:
            serving_features = np.zeros(9)

        # 2. å€™é¸è¡›æ˜Ÿç‰¹å¾µ (5Ã—3=15ç¶­)
        candidate_features = []
        for i in range(5):  # æœ€å¤š5å€‹å€™é¸è¡›æ˜Ÿ
            if i < len(self.candidate_satellites):
                candidate = self.candidate_satellites[i]
                features = [
                    candidate.elevation,
                    self._calculate_rsrp(candidate),
                    candidate.distance_km
                ]
            else:
                features = [0.0, -120.0, 2000.0]  # é»˜èªå€¼ï¼šç„¡è¡›æ˜Ÿ
            candidate_features.extend(features)

        # 3. è¡›æ˜Ÿæ± ç‹€æ…‹ (1ç¶­) - æ•´åˆStage 4æ± åˆ†ææ•¸æ“š
        if self.pool_state_data:
            pool_health = len(self.pool_state_data.visible_satellites) / 15.0  # æ¨™æº–åŒ–åˆ°[0,1]
        else:
            pool_health = 0.0

        # åˆä½µæ‰€æœ‰ç‰¹å¾µ
        observation = np.concatenate([
            serving_features,
            candidate_features,
            [pool_health]
        ]).astype(np.float32)

        return observation

    def step(self, action: int) -> Tuple[np.ndarray, float, bool, bool, Dict]:
        """åŸ·è¡Œå‹•ä½œä¸¦è¿”å›çµæœ"""

        info = {}
        reward = 0.0

        # è¨˜éŒ„å‹•ä½œå‰ç‹€æ…‹
        prev_rsrp = self._calculate_rsrp(self.current_serving_satellite) if self.current_serving_satellite else -120.0

        # åŸ·è¡Œæ›æ‰‹å‹•ä½œ
        if action == 0:  # ä¿æŒç•¶å‰è¡›æ˜Ÿ
            info['action_type'] = 'maintain'

            # æª¢æŸ¥ç•¶å‰è¡›æ˜Ÿæ˜¯å¦ä»ç„¶å¯ç”¨
            if (self.current_serving_satellite and
                self.current_serving_satellite.elevation <= 0):
                # è¡›æ˜Ÿå·²ä¸å¯è¦‹ï¼Œå¼·åˆ¶æ›æ‰‹
                reward += self.reward_config['service_interruption_penalty']
                info['forced_handover'] = True
                self._emergency_handover()

        else:  # åˆ‡æ›åˆ°å€™é¸è¡›æ˜Ÿ
            candidate_idx = action - 1
            if candidate_idx < len(self.candidate_satellites):
                new_satellite = self.candidate_satellites[candidate_idx]

                # æª¢æŸ¥å€™é¸è¡›æ˜Ÿå¯ç”¨æ€§
                if new_satellite.elevation > 0:
                    self.current_serving_satellite = new_satellite
                    reward += self.reward_config['handover_penalty']
                    info['action_type'] = 'handover'
                    info['new_satellite'] = new_satellite.satellite_name
                else:
                    reward += self.reward_config['service_interruption_penalty']
                    info['action_type'] = 'invalid_handover'

        # è¨ˆç®—çå‹µ
        reward += self._calculate_step_reward()

        # æ›´æ–°ç’°å¢ƒç‹€æ…‹
        self._update_environment_state()

        # æª¢æŸ¥çµ‚æ­¢æ¢ä»¶
        terminated = self._is_terminated()
        truncated = self.current_time_step >= self.max_episode_steps

        self.current_time_step += 1

        return self._get_observation(), reward, terminated, truncated, info

    def reset(self, seed: int = None, options: Dict = None) -> Tuple[np.ndarray, Dict]:
        """é‡ç½®ç’°å¢ƒåˆ°åˆå§‹ç‹€æ…‹"""

        super().reset(seed=seed)

        # é‡ç½®ç’°å¢ƒç‹€æ…‹
        self.current_time_step = 0

        # å¾constellation_dataä¸­ç²å–åˆå§‹ç‹€æ…‹
        if self.constellation_data:
            initial_state = self._get_initial_state()
            self.current_serving_satellite = initial_state['serving']
            self.candidate_satellites = initial_state['candidates'][:5]  # æœ€å¤š5å€‹å€™é¸
            self.pool_state_data = initial_state['pool_state']

        info = {
            'serving_satellite': self.current_serving_satellite.satellite_name if self.current_serving_satellite else None,
            'num_candidates': len(self.candidate_satellites)
        }

        return self._get_observation(), info

    def _calculate_rsrp(self, satellite: SatelliteCoordinates) -> float:
        """è¨ˆç®—RSRP - æ•´åˆStage 3ä¿¡è™Ÿè™•ç†"""
        if not satellite:
            return -120.0

        # ä½¿ç”¨Stage 3çš„ä¿¡è™Ÿè™•ç†ç®—æ³•
        distance_km = satellite.distance_km
        tx_power_dbm = 43.0
        frequency_ghz = 2.0
        antenna_gain = 15.0

        path_loss = (
            20 * np.log10(distance_km) +
            20 * np.log10(frequency_ghz) +
            92.45
        )

        rsrp = tx_power_dbm + antenna_gain - path_loss
        return rsrp

    def _calculate_step_reward(self) -> float:
        """è¨ˆç®—å–®æ­¥çå‹µ"""

        if not self.current_serving_satellite:
            return self.reward_config['service_interruption_penalty']

        # ä¿¡è™Ÿå“è³ªçå‹µ
        current_rsrp = self._calculate_rsrp(self.current_serving_satellite)
        if current_rsrp > self.reward_config['qos_threshold']:
            signal_reward = (current_rsrp - self.reward_config['qos_threshold']) / 10.0
        else:
            signal_reward = (current_rsrp - self.reward_config['qos_threshold']) / 20.0

        signal_reward *= self.reward_config['signal_quality_weight']

        # æœå‹™é€£çºŒæ€§çå‹µ
        continuity_reward = self.reward_config['continuity_bonus']

        return signal_reward + continuity_reward
```

### Day 3: å¤šç®—æ³•è¨“ç·´å™¨
```python
# algorithm_trainer.py å¤šç®—æ³•æ”¯æ´
from stable_baselines3 import DQN, PPO, SAC, A2C
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.callbacks import EvalCallback

class MultiAlgorithmTrainer:
    """å¤šç®—æ³•RLè¨“ç·´å™¨ - å­¸è¡“ç ”ç©¶å°å‘"""

    def __init__(self, env_config: Dict):
        self.env_config = env_config
        self.algorithms = {}
        self.training_results = {}

    def setup_algorithms(self) -> Dict:
        """è¨­ç½®å¤šç¨®RLç®—æ³•"""

        # å‰µå»ºè¨“ç·´ç’°å¢ƒ
        train_env = make_vec_env(
            SatelliteHandoverEnv,
            n_envs=1,  # å­¸è¡“ç ”ç©¶ç”¨ï¼Œä¸éœ€è¦å¤šç’°å¢ƒä¸¦è¡Œ
            env_kwargs=self.env_config
        )

        # DQN - é›¢æ•£å‹•ä½œç©ºé–“ç¶“å…¸ç®—æ³•
        self.algorithms['DQN'] = DQN(
            policy="MlpPolicy",
            env=train_env,
            learning_rate=1e-4,
            buffer_size=10000,  # é©ä¸­çš„buffer size
            learning_starts=1000,
            batch_size=32,
            gamma=0.99,
            target_update_interval=500,
            exploration_initial_eps=1.0,
            exploration_final_eps=0.1,
            exploration_fraction=0.3,
            verbose=1
        )

        # PPO - ç©©å®šçš„ç­–ç•¥æ¢¯åº¦ç®—æ³•
        self.algorithms['PPO'] = PPO(
            policy="MlpPolicy",
            env=train_env,
            learning_rate=3e-4,
            n_steps=1024,
            batch_size=64,
            n_epochs=4,
            gamma=0.99,
            gae_lambda=0.95,
            clip_range=0.2,
            verbose=1
        )

        # A2C - è¼•é‡ç´šActor-Critic
        self.algorithms['A2C'] = A2C(
            policy="MlpPolicy",
            env=train_env,
            learning_rate=7e-4,
            n_steps=5,
            gamma=0.99,
            gae_lambda=1.0,
            verbose=1
        )

        # SAC - é©ç”¨æ–¼é€£çºŒæ§åˆ¶ (é€™è£¡é©é…ç‚ºé›¢æ•£)
        # æ³¨æ„ï¼šSACé€šå¸¸ç”¨æ–¼é€£çºŒå‹•ä½œï¼Œé€™è£¡ç‚ºäº†æ¯”è¼ƒç ”ç©¶è€ŒåŒ…å«
        if self._can_use_sac():
            continuous_env = self._create_continuous_adaptation()
            self.algorithms['SAC'] = SAC(
                policy="MlpPolicy",
                env=continuous_env,
                learning_rate=3e-4,
                buffer_size=10000,
                batch_size=64,
                gamma=0.99,
                verbose=1
            )

        return self.algorithms

    def train_all_algorithms(self, total_timesteps: int = 50000) -> Dict:
        """è¨“ç·´æ‰€æœ‰ç®—æ³• - é©åˆå­¸è¡“ç ”ç©¶çš„è¦æ¨¡"""

        for algo_name, model in self.algorithms.items():
            print(f"\né–‹å§‹è¨“ç·´ {algo_name}...")

            # è¨­ç½®è©•ä¼°å›èª¿
            eval_env = make_vec_env(SatelliteHandoverEnv, n_envs=1, env_kwargs=self.env_config)
            eval_callback = EvalCallback(
                eval_env,
                best_model_save_path=f"./models/{algo_name}/",
                log_path=f"./logs/{algo_name}/",
                eval_freq=5000,
                deterministic=True,
                render=False
            )

            # è¨“ç·´æ¨¡å‹
            model.learn(
                total_timesteps=total_timesteps,
                callback=eval_callback
            )

            # ä¿å­˜æœ€çµ‚æ¨¡å‹
            model.save(f"./models/{algo_name}/final_model")

            # è©•ä¼°è¨“ç·´çµæœ
            self.training_results[algo_name] = self._evaluate_algorithm(model, eval_env)

            print(f"{algo_name} è¨“ç·´å®Œæˆ")

        return self.training_results

    def _evaluate_algorithm(self, model, eval_env) -> Dict:
        """è©•ä¼°ç®—æ³•æ€§èƒ½"""

        episode_rewards = []
        episode_lengths = []

        for episode in range(10):  # è©•ä¼°10å€‹episode
            obs = eval_env.reset()
            total_reward = 0
            steps = 0

            while True:
                action, _states = model.predict(obs, deterministic=True)
                obs, reward, done, info = eval_env.step(action)
                total_reward += reward[0]
                steps += 1

                if done[0]:
                    break

            episode_rewards.append(total_reward)
            episode_lengths.append(steps)

        return {
            'mean_reward': np.mean(episode_rewards),
            'std_reward': np.std(episode_rewards),
            'mean_episode_length': np.mean(episode_lengths),
            'total_timesteps': model.num_timesteps
        }
```

### Day 4: å‚³çµ±MLå°æ¯”åŸºæº–
```python
# traditional_ml_baseline.py å‚³çµ±MLå°æ¯”
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, accuracy_score

class TraditionalMLBaseline:
    """å‚³çµ±MLåŸºæº–æ¨¡å‹ - ç”¨æ–¼èˆ‡RLç®—æ³•å°æ¯”"""

    def __init__(self):
        self.models = {}
        self.scaler = StandardScaler()

    def prepare_training_data(self, handover_scenarios: List[HandoverScenario]) -> Tuple[np.ndarray, np.ndarray]:
        """æº–å‚™å‚³çµ±MLè¨“ç·´æ•¸æ“š - æ•´åˆStage 5äº‹ä»¶æ•¸æ“š"""

        features = []
        labels = []

        for scenario in handover_scenarios:
            # æå–ç‰¹å¾µå‘é‡
            feature_vector = self._extract_features(scenario)
            features.append(feature_vector)

            # ç¢ºå®šæœ€ä½³å‹•ä½œæ¨™ç±¤
            optimal_action = self._determine_optimal_action(scenario)
            labels.append(optimal_action)

        features_array = np.array(features)
        labels_array = np.array(labels)

        # ç‰¹å¾µæ¨™æº–åŒ–
        features_scaled = self.scaler.fit_transform(features_array)

        return features_scaled, labels_array

    def train_baseline_models(self, features: np.ndarray, labels: np.ndarray) -> Dict:
        """è¨“ç·´å‚³çµ±MLåŸºæº–æ¨¡å‹"""

        # Random Forest
        rf_model = RandomForestClassifier(
            n_estimators=100,
            max_depth=10,
            random_state=42
        )
        rf_model.fit(features, labels)
        self.models['RandomForest'] = rf_model

        # SVM
        svm_model = SVC(
            kernel='rbf',
            C=1.0,
            gamma='scale',
            random_state=42
        )
        svm_model.fit(features, labels)
        self.models['SVM'] = svm_model

        # è©•ä¼°æ¨¡å‹æ€§èƒ½
        results = {}
        for name, model in self.models.items():
            predictions = model.predict(features)
            accuracy = accuracy_score(labels, predictions)
            results[name] = {
                'accuracy': accuracy,
                'classification_report': classification_report(labels, predictions)
            }

        return results

    def _extract_features(self, scenario: HandoverScenario) -> np.ndarray:
        """å¾æ›æ‰‹å ´æ™¯æå–ç‰¹å¾µ"""

        serving = scenario.serving_satellite
        candidates = scenario.candidate_satellites[:3]  # æœ€å¤š3å€‹å€™é¸

        # æœå‹™è¡›æ˜Ÿç‰¹å¾µ
        serving_features = [
            serving.elevation,
            serving.distance_km,
            self._calculate_rsrp(serving),
            len(scenario.triggered_events)  # äº‹ä»¶æ•¸é‡
        ]

        # å€™é¸è¡›æ˜Ÿç‰¹å¾µ
        candidate_features = []
        for i in range(3):
            if i < len(candidates):
                candidate = candidates[i]
                candidate_features.extend([
                    candidate.elevation,
                    candidate.distance_km,
                    self._calculate_rsrp(candidate)
                ])
            else:
                candidate_features.extend([0.0, 2000.0, -120.0])

        return np.array(serving_features + candidate_features)

    def _determine_optimal_action(self, scenario: HandoverScenario) -> int:
        """ç¢ºå®šæœ€ä½³å‹•ä½œæ¨™ç±¤"""

        # ç°¡åŒ–çš„æœ€ä½³å‹•ä½œé‚è¼¯
        serving_rsrp = self._calculate_rsrp(scenario.serving_satellite)

        if serving_rsrp < -100.0:  # æœå‹™è¡›æ˜Ÿä¿¡è™Ÿå·®
            # å°‹æ‰¾æœ€ä½³å€™é¸è¡›æ˜Ÿ
            best_candidate_idx = 0
            best_rsrp = -120.0

            for i, candidate in enumerate(scenario.candidate_satellites[:5]):
                candidate_rsrp = self._calculate_rsrp(candidate)
                if candidate_rsrp > best_rsrp:
                    best_rsrp = candidate_rsrp
                    best_candidate_idx = i + 1  # å‹•ä½œ0æ˜¯ä¿æŒï¼Œå€™é¸å¾1é–‹å§‹

            return best_candidate_idx
        else:
            return 0  # ä¿æŒç•¶å‰è¡›æ˜Ÿ

class ExperimentComparator:
    """å¯¦é©—æ¯”è¼ƒå™¨ - RL vs å‚³çµ±ML"""

    def __init__(self):
        self.results = {}

    def compare_all_methods(self, rl_results: Dict, ml_results: Dict,
                          test_scenarios: List[HandoverScenario]) -> Dict:
        """æ¯”è¼ƒæ‰€æœ‰æ–¹æ³•çš„æ€§èƒ½"""

        comparison_results = {
            'rl_algorithms': rl_results,
            'traditional_ml': ml_results,
            'performance_comparison': {}
        }

        # åœ¨æ¸¬è©¦å ´æ™¯ä¸Šè©•ä¼°æ‰€æœ‰æ–¹æ³•
        for scenario in test_scenarios:
            scenario_results = {}

            # è©•ä¼°RLç®—æ³•
            for algo_name in rl_results.keys():
                # é€™è£¡éœ€è¦è¼‰å…¥è¨“ç·´å¥½çš„æ¨¡å‹ä¸¦é æ¸¬
                # ç°¡åŒ–å¯¦ç¾ï¼šä½¿ç”¨è¨“ç·´çµæœçš„mean_rewardä½œç‚ºæ€§èƒ½æŒ‡æ¨™
                scenario_results[f'RL_{algo_name}'] = rl_results[algo_name]['mean_reward']

            # è©•ä¼°å‚³çµ±ML
            for model_name in ml_results.keys():
                scenario_results[f'ML_{model_name}'] = ml_results[model_name]['accuracy']

            comparison_results['performance_comparison'][scenario.scenario_id] = scenario_results

        return comparison_results
```

### Day 5: å¯¦é©—ç®¡ç†èˆ‡æ•´åˆæ¸¬è©¦
```python
# experiment_manager.py å¯¦é©—ç®¡ç†
class ExperimentManager:
    """å¯¦é©—ç®¡ç†å™¨ - å­¸è¡“ç ”ç©¶å°å‘"""

    def __init__(self, constellation_data: Dict):
        self.constellation_data = constellation_data
        self.experiment_results = {}

    def run_complete_experiment(self, total_timesteps: int = 50000) -> Dict:
        """é‹è¡Œå®Œæ•´çš„RL vs å‚³çµ±MLå°æ¯”å¯¦é©—"""

        print("ğŸš€ é–‹å§‹å®Œæ•´çš„è¡›æ˜Ÿæ›æ‰‹å„ªåŒ–å¯¦é©—...")

        # 1. æº–å‚™è¨“ç·´ç’°å¢ƒ
        env_config = {'constellation_data': self.constellation_data}

        # 2. è¨“ç·´RLç®—æ³•
        print("\nğŸ“Š è¨“ç·´å¼·åŒ–å­¸ç¿’ç®—æ³•...")
        trainer = MultiAlgorithmTrainer(env_config)
        trainer.setup_algorithms()
        rl_results = trainer.train_all_algorithms(total_timesteps)

        # 3. è¨“ç·´å‚³çµ±MLåŸºæº–
        print("\nğŸ” è¨“ç·´å‚³çµ±MLåŸºæº–...")
        ml_baseline = TraditionalMLBaseline()

        # å¾Stage 5ç²å–è¨“ç·´å ´æ™¯
        training_scenarios = self._load_handover_scenarios()
        features, labels = ml_baseline.prepare_training_data(training_scenarios)
        ml_results = ml_baseline.train_baseline_models(features, labels)

        # 4. æ€§èƒ½æ¯”è¼ƒ
        print("\nğŸ“ˆ é€²è¡Œæ€§èƒ½æ¯”è¼ƒ...")
        comparator = ExperimentComparator()
        test_scenarios = training_scenarios[-100:]  # ä½¿ç”¨æœ€å¾Œ100å€‹å ´æ™¯ä½œç‚ºæ¸¬è©¦
        comparison_results = comparator.compare_all_methods(
            rl_results, ml_results, test_scenarios
        )

        # 5. ç”Ÿæˆå¯¦é©—å ±å‘Š
        report = self._generate_experiment_report(comparison_results)

        self.experiment_results = {
            'rl_results': rl_results,
            'ml_results': ml_results,
            'comparison': comparison_results,
            'report': report
        }

        print("\nâœ… å¯¦é©—å®Œæˆï¼")
        return self.experiment_results

    def _load_handover_scenarios(self) -> List[HandoverScenario]:
        """è¼‰å…¥æ›æ‰‹å ´æ™¯ - æ•´åˆStage 5æ•¸æ“š"""

        # é€™è£¡æ‡‰è©²å¾Stage 5çš„è¼¸å‡ºè¼‰å…¥å ´æ™¯æ•¸æ“š
        # ç°¡åŒ–å¯¦ç¾ï¼šç”Ÿæˆæ¨¡æ“¬å ´æ™¯
        scenarios = []

        # å¯¦éš›å¯¦ç¾æ™‚ï¼Œæ‡‰è©²è¼‰å…¥Stage 5ç”Ÿæˆçš„RLè¨“ç·´æ•¸æ“š
        # scenarios = load_scenarios_from_stage5("rl_training_data.json")

        return scenarios

    def _generate_experiment_report(self, results: Dict) -> str:
        """ç”Ÿæˆå¯¦é©—å ±å‘Š - é©åˆå­¸è¡“ç™¼è¡¨"""

        report = """
# è¡›æ˜Ÿæ›æ‰‹å„ªåŒ–å¯¦é©—å ±å‘Š

## å¯¦é©—æ¦‚è¿°
æœ¬å¯¦é©—æ¯”è¼ƒäº†å¤šç¨®å¼·åŒ–å­¸ç¿’ç®—æ³•èˆ‡å‚³çµ±æ©Ÿå™¨å­¸ç¿’æ–¹æ³•åœ¨LEOè¡›æ˜Ÿæ›æ‰‹å„ªåŒ–ä¸­çš„æ€§èƒ½è¡¨ç¾ã€‚

## ç®—æ³•æ¯”è¼ƒ
        """

        # æ·»åŠ è©³ç´°çµæœ
        for algo_name, result in results['rl_results'].items():
            report += f"""
### {algo_name} (å¼·åŒ–å­¸ç¿’)
- å¹³å‡çå‹µ: {result['mean_reward']:.2f}
- æ¨™æº–å·®: {result['std_reward']:.2f}
- å¹³å‡episodeé•·åº¦: {result['mean_episode_length']:.1f}
            """

        for model_name, result in results['ml_results'].items():
            report += f"""
### {model_name} (å‚³çµ±ML)
- æº–ç¢ºç‡: {result['accuracy']:.3f}
            """

        return report
```

## ğŸ§ª é©—è­‰æ¸¬è©¦

### Gymnasiumæ¨™æº–åˆè¦æ¸¬è©¦
```python
def test_gymnasium_compliance():
    """Gymnasiumæ¥å£åˆè¦æ€§æ¸¬è©¦"""

    env = SatelliteHandoverEnv({})

    # æª¢æŸ¥å¿…è¦æ–¹æ³•
    assert hasattr(env, 'step'), "ç¼ºå°‘stepæ–¹æ³•"
    assert hasattr(env, 'reset'), "ç¼ºå°‘resetæ–¹æ³•"
    assert hasattr(env, 'action_space'), "ç¼ºå°‘action_space"
    assert hasattr(env, 'observation_space'), "ç¼ºå°‘observation_space"

    # æ¸¬è©¦ç’°å¢ƒäº¤äº’
    obs, info = env.reset()
    assert obs.shape == (25,), "è§€å¯Ÿç©ºé–“ç¶­åº¦éŒ¯èª¤"

    action = env.action_space.sample()
    obs, reward, terminated, truncated, info = env.step(action)

    assert isinstance(reward, (int, float)), "çå‹µå¿…é ˆæ˜¯æ•¸å€¼"
    assert isinstance(terminated, bool), "terminatedå¿…é ˆæ˜¯å¸ƒçˆ¾å€¼"

def test_multi_algorithm_training():
    """å¤šç®—æ³•è¨“ç·´æ¸¬è©¦"""

    trainer = MultiAlgorithmTrainer({})
    algorithms = trainer.setup_algorithms()

    # é©—è­‰ç®—æ³•è¨­ç½®
    assert 'DQN' in algorithms, "DQNç®—æ³•æœªè¨­ç½®"
    assert 'PPO' in algorithms, "PPOç®—æ³•æœªè¨­ç½®"

    # ç°¡çŸ­è¨“ç·´æ¸¬è©¦
    results = trainer.train_all_algorithms(total_timesteps=1000)
    assert len(results) > 0, "è¨“ç·´çµæœç‚ºç©º"

def test_traditional_ml_comparison():
    """å‚³çµ±MLå°æ¯”æ¸¬è©¦"""

    baseline = TraditionalMLBaseline()

    # ç”Ÿæˆæ¸¬è©¦æ•¸æ“š
    test_scenarios = generate_test_scenarios(10)
    features, labels = baseline.prepare_training_data(test_scenarios)

    assert features.shape[0] == 10, "ç‰¹å¾µæ•¸é‡éŒ¯èª¤"
    assert len(labels) == 10, "æ¨™ç±¤æ•¸é‡éŒ¯èª¤"

    # è¨“ç·´æ¸¬è©¦
    results = baseline.train_baseline_models(features, labels)
    assert 'RandomForest' in results, "RandomForestçµæœç¼ºå¤±"
```

## ğŸ“Š æˆåŠŸæŒ‡æ¨™

### é‡åŒ–æŒ‡æ¨™
- **ç’°å¢ƒæ¨™æº–**: 100%ç¬¦åˆGymnasiumæ¥å£æ¨™æº–
- **ç®—æ³•æ”¯æ´**: æˆåŠŸè¨“ç·´4ç¨®RLç®—æ³• (DQN/PPO/SAC/A2C)
- **å°æ¯”ç ”ç©¶**: RL vs å‚³çµ±MLæ€§èƒ½å°æ¯”å®Œæˆ
- **è¨“ç·´æ•ˆç‡**: 50k timestepså…§é”åˆ°æ”¶æ–‚

### è³ªåŒ–æŒ‡æ¨™
- **å­¸è¡“æ¨™æº–**: ç¬¦åˆRLç ”ç©¶ç¤¾ç¾¤æ¨™æº–
- **å¯¦é©—å¯é‡ç¾**: å®Œæ•´çš„ç¨®å­æ§åˆ¶å’Œç‰ˆæœ¬ç®¡ç†
- **è«–æ–‡æº–å‚™**: æä¾›å­¸è¡“ç™¼è¡¨ç´šçš„å¯¦é©—çµæœ
- **æ–¹æ³•æ¯”è¼ƒ**: ç‚ºRLå„ªå‹¢æä¾›å®šé‡è­‰æ“š

## âš ï¸ é¢¨éšªæ§åˆ¶

### æŠ€è¡“é¢¨éšª
| é¢¨éšª | å½±éŸ¿ | æ‡‰å°ç­–ç•¥ |
|------|------|----------|
| ç®—æ³•æ”¶æ–‚å›°é›£ | é«˜ | èª¿æ•´è¶…åƒæ•¸ï¼Œç°¡åŒ–ç’°å¢ƒ |
| ç’°å¢ƒè¨­è¨ˆä¸ç•¶ | é«˜ | å……åˆ†æ¸¬è©¦ï¼Œå°ˆå®¶é©—è­‰ |
| æ•¸æ“šè³ªé‡å•é¡Œ | ä¸­ç­‰ | ä¾è³´å‰äº”éšæ®µçš„é«˜è³ªé‡æ•¸æ“š |
| è¨ˆç®—è³‡æºé™åˆ¶ | ä¸­ç­‰ | åˆç†è¨­ç½®è¨“ç·´è¦æ¨¡ |

### å­¸è¡“é¢¨éšª
- **æ¨™æº–åˆè¦**: å¿…é ˆ100%ç¬¦åˆGymnasiumæ¨™æº–
- **å¯¦é©—è¨­è¨ˆ**: éœ€è¦åˆç†çš„å°æ¯”å¯¦é©—è¨­è¨ˆ
- **çµæœå¯ä¿¡**: ç¢ºä¿å¯¦é©—çµæœå…·æœ‰å­¸è¡“åƒ¹å€¼
- **å¯é‡ç¾æ€§**: æä¾›å®Œæ•´çš„å¯¦é©—é‡ç¾èƒ½åŠ›

---

**æ–‡æª”ç‰ˆæœ¬**: v1.0 (ä¿®æ­£ç‰ˆ)
**å»ºç«‹æ—¥æœŸ**: 2024-01-15
**å‰ç½®æ¢ä»¶**: Stage 1-5å®Œæˆ (å®Œæ•´æ•¸æ“šè™•ç†éˆå¯ç”¨)
**é‡é»**: å­¸è¡“ç´šRLç ”ç©¶ç’°å¢ƒï¼Œæ”¯æ´è«–æ–‡ç™¼è¡¨
**å°ˆæ¡ˆå®Œæˆ**: å…­éšæ®µé‡æ§‹è¨ˆåŠƒå…¨éƒ¨å®Œæˆ