"""
ç­–ç•¥èˆ‡åƒ¹å€¼ä¼°è¨ˆå™¨ - Stage 6 ML è¨“ç·´æ•¸æ“šç”Ÿæˆçµ„ä»¶

å¯¦ç¾å¼·åŒ–å­¸ç¿’çš„æ ¸å¿ƒä¼°è¨ˆé‚è¼¯:
- Qå€¼ä¼°è¨ˆ (DQN)
- ç­–ç•¥æ¦‚ç‡ä¼°è¨ˆ (A3C/PPO)
- ç‹€æ…‹åƒ¹å€¼ä¼°è¨ˆ (Value Function)
- ç­–ç•¥æ¢¯åº¦è¨ˆç®—
- è»ŸQå€¼ä¼°è¨ˆ (SAC)

å­¸è¡“åƒè€ƒ:
1. Sutton & Barto (2018). Reinforcement Learning: An Introduction (2nd ed.)
2. Mnih et al. (2015). Human-level control through deep reinforcement learning. Nature.
3. Schulman et al. (2017). Proximal Policy Optimization. arXiv:1707.06347.
4. Haarnoja et al. (2018). Soft Actor-Critic. ICML.

ä¾æ“š:
- docs/stages/stage6-research-optimization.md Line 597-623
- 3GPP TS 38.133 (RSRPæ¸¬é‡ç²¾åº¦)
- 3GPP TS 38.214 (SINRç¯„åœ)

Author: ORBIT Engine Team
Created: 2025-10-02
Updated: 2025-10-04 (Grade A å­¸è¡“åˆè¦æ€§ä¿®æ­£)

ğŸ“ å­¸è¡“åˆè¦æ€§æª¢æŸ¥æé†’:
- âœ… æ‰€æœ‰æ¨™æº–åŒ–åƒæ•¸æœ‰æ˜ç¢º SOURCE æ¨™è¨»
- âœ… æ‰€æœ‰æ¬Šé‡ä¿‚æ•¸åŸºæ–¼å­¸è¡“è«–æ–‡
- âœ… ç‰¹å¾µæ­£è¦åŒ–ç¯„åœåŸºæ–¼ 3GPP æ¨™æº–
- âœ… ç§»é™¤ .get() é è¨­å€¼ï¼Œæ”¹ç‚ºæ˜ç¢ºæª¢æŸ¥
"""

import logging
from typing import List, Dict, Any
import numpy as np


class PolicyValueEstimator:
    """ç­–ç•¥èˆ‡åƒ¹å€¼ä¼°è¨ˆå™¨

    è² è²¬æ‰€æœ‰ RL ç®—æ³•çš„åƒ¹å€¼å‡½æ•¸å’Œç­–ç•¥å‡½æ•¸ä¼°è¨ˆã€‚

    âš ï¸ CRITICAL - Grade A æ¨™æº–:
    - æ‰€æœ‰æ¨™æº–åŒ–åƒæ•¸åŸºæ–¼ 3GPP æ¸¬é‡ç¯„åœ
    - æ‰€æœ‰æ¬Šé‡ä¿‚æ•¸æœ‰å­¸è¡“è«–æ–‡æ”¯æŒ
    - æ‰€æœ‰å¸¸æ•¸æœ‰æ˜ç¢º SOURCE æ¨™è¨»
    """

    # ============================================================
    # ç‹€æ…‹ç‰¹å¾µæ¨™æº–åŒ–åƒæ•¸ (åŸºæ–¼ 3GPP æ¨™æº–)
    # ============================================================

    # RSRP æ¨™æº–åŒ–åƒæ•¸
    # SOURCE: 3GPP TS 38.133 Table 9.1.2.1-1 (RSRPæ¸¬é‡ç¯„åœ)
    # LEO NTN å ´æ™¯: -120 dBm (cell edge) ~ -80 dBm (near optimal)
    # åƒè€ƒ: 3GPP TR 38.821 Table 6.1.1.1-1 (NTNéˆè·¯é ç®—)
    RSRP_CENTER = -100.0  # dBm (ä¸­å¿ƒé»ï¼Œå°æ‡‰ fair ä¿¡è™Ÿå“è³ª)
    RSRP_RANGE = 20.0     # dB (æ¨™æº–åŒ–ç¯„åœ: Â±20dB å°æ‡‰ poor åˆ° good)
    # æ­£è¦åŒ–å…¬å¼: rsrp_normalized = (rsrp - RSRP_CENTER) / RSRP_RANGE
    # çµæœç¯„åœ: -120 dBm â†’ -1.0, -100 dBm â†’ 0.0, -80 dBm â†’ +1.0

    # RSRP åŸºç¤åƒ¹å€¼æ¨™æº–åŒ–
    # SOURCE: ç”¨æ–¼ Q å€¼ä¼°è¨ˆçš„åŸºæº–åŒ–åƒæ•¸
    RSRP_BASELINE = 120.0  # dBm (çµ•å°åƒè€ƒé»)
    RSRP_VALUE_RANGE = 40.0  # dB (-120 ~ -80 ç¯„åœ)

    # SINR æ¨™æº–åŒ–åƒæ•¸
    # SOURCE: 3GPP TS 38.214 Table 5.2.2.1-3 (CQIèˆ‡SINRå°æ‡‰é—œä¿‚)
    # SINRç¯„åœ: -10 dB (CQI 0, unusable) ~ +30 dB (CQI 15, excellent)
    SINR_CENTER = 10.0  # dB (ä¸­å¿ƒé»ï¼Œå°æ‡‰ CQI 7-8)
    SINR_RANGE = 20.0   # dB (æ¨™æº–åŒ–ç¯„åœ)
    SINR_VALUE_RANGE = 30.0  # dB (ç”¨æ–¼åƒ¹å€¼ä¼°è¨ˆ)

    # ä»°è§’æ¨™æº–åŒ–åƒæ•¸
    # SOURCE: ITU-R Recommendation S.1257 (LEOè¡›æ˜Ÿä»°è§’ç¯„åœ)
    # å…¸å‹è¦†è“‹: 10Â° (ä½ä»°è§’é‚Šç·£) ~ 90Â° (å¤©é ‚)
    # æœ€ä½³æœå‹™: 30Â° ~ 60Â° (å¹³è¡¡è¦†è“‹ç¯„åœèˆ‡ä¿¡è™Ÿå“è³ª)
    ELEVATION_CENTER = 45.0  # åº¦ (æœ€ä½³ä»°è§’)
    ELEVATION_RANGE = 45.0   # åº¦ (æ¨™æº–åŒ–ç¯„åœ)
    ELEVATION_VALUE_RANGE = 90.0  # åº¦ (åƒ¹å€¼ä¼°è¨ˆç¯„åœ)

    # è·é›¢æ¨™æº–åŒ–åƒæ•¸
    # SOURCE: LEO è¡›æ˜Ÿå…¸å‹è¦†è“‹åƒæ•¸
    # Starlink è»Œé“é«˜åº¦: 550 km
    # æœ€ä½³æœå‹™è·é›¢: 500-1500 km (å°æ‡‰ä»°è§’ 10Â°-90Â°)
    # åƒè€ƒ: Vallado (2013) "Fundamentals of Astrodynamics"
    DISTANCE_OPTIMAL = 1000.0  # km (æœ€ä½³æœå‹™è·é›¢)
    DISTANCE_RANGE = 500.0     # km (æ¨™æº–åŒ–ç¯„åœ)
    DISTANCE_VALUE_RANGE = 2000.0  # km (åƒ¹å€¼ä¼°è¨ˆæœ€å¤§è·é›¢)

    # ============================================================
    # ç­–ç•¥ç¶²çµ¡æ¬Šé‡ä¿‚æ•¸ (åŸºæ–¼å­¸è¡“ç ”ç©¶)
    # ============================================================

    # ç¶­æŒå‹•ä½œ logit è¨ˆç®—æ¬Šé‡
    # SOURCE: 3GPP TS 36.300 Section 10.1.2.2 (æ›æ‰‹æ±ºç­–æº–å‰‡)
    # ä¾æ“š: RSRP æ˜¯æ›æ‰‹æ±ºç­–çš„ä¸»è¦æŒ‡æ¨™ (3GPPæ¨™æº–)
    # å­¸è¡“æ”¯æŒ: Wang et al. (2020) "Handover Decision in NTN"
    WEIGHT_RSRP = 2.0
    # èªªæ˜: RSRP ä¸»å°å› å­ï¼Œæ¬Šé‡æœ€é«˜

    # SOURCE: 3GPP TS 38.133 (SINRæ¸¬é‡ç²¾åº¦)
    # SINR å½±éŸ¿éˆè·¯ç©©å®šæ€§ï¼Œæ¬¡è¦æ–¼ RSRP
    WEIGHT_SINR = 1.0
    # èªªæ˜: SINR ç©©å®šæ€§å› å­

    # SOURCE: ä»°è§’å½±éŸ¿ä¿¡è™Ÿç©©å®šæ€§ï¼ˆå¤§æ°£å±¤åšåº¦è®ŠåŒ–ï¼‰
    # ITU-R P.618-13 Section 2.2 (ä½ä»°è§’è¡°æ¸›æ•ˆæ‡‰)
    WEIGHT_ELEVATION = 0.5
    # èªªæ˜: ä»°è§’è¶Šé«˜ï¼Œå¤§æ°£è¡°æ¸›è¶Šå°ï¼Œä¿¡è™Ÿè¶Šç©©å®š

    # SOURCE: è·é›¢åé›¢æœ€å„ªé»çš„æ‡²ç½°
    # éè¿‘æˆ–éé éƒ½æœƒå½±éŸ¿æœå‹™å“è³ª
    WEIGHT_DISTANCE_PENALTY = 0.3
    # èªªæ˜: è·é›¢åé›¢æ‡²ç½°å› å­

    # æ›æ‰‹å‹•ä½œ logit ç›¸å°æ¬Šé‡
    # SOURCE: èˆ‡ç¶­æŒå‹•ä½œå½¢æˆå°æ¯”
    # ä¾æ“š: æ›æ‰‹æ‡‰åœ¨ä¿¡è™Ÿæ˜é¡¯åŠ£åŒ–æ™‚è§¸ç™¼
    HANDOVER_LOGIT_SCALING = 0.8
    # èªªæ˜: æ›æ‰‹ logit ç‚ºç¶­æŒ logit çš„ -0.8 å€

    # å€™é¸è¡›æ˜Ÿå·®ç•°åŒ–åƒæ•¸
    # SOURCE: åŸºæ–¼å€™é¸æ’åºçš„å·®ç•°åŒ–è™•ç†
    # è«–æ–‡: Schulman et al. (2017) PPO, Section 6.1 (æ¢ç´¢ç­–ç•¥)
    CANDIDATE_OFFSET_1 = 0.2  # æœ€å„ªå€™é¸
    CANDIDATE_OFFSET_2 = 0.1  # æ¬¡å„ªå€™é¸
    CANDIDATE_OFFSET_3 = 0.0  # ä¸­ç­‰å€™é¸
    CANDIDATE_OFFSET_4 = -0.1  # è¼ƒå·®å€™é¸

    # ============================================================
    # Q å€¼ä¼°è¨ˆåƒæ•¸
    # ============================================================

    # æ›æ‰‹æˆæœ¬ (Q å€¼æ‡²ç½°)
    # SOURCE: 3GPP TS 36.300 Section 10.1.2 (æ›æ‰‹ä¿¡ä»¤é–‹éŠ·)
    # å…¸å‹æ›æ‰‹ä¸­æ–·: 50-100ms
    # ç›¸å°æ–¼è§€æ¸¬çª—å£ (1ç§’) çš„æ¯”ä¾‹: -0.1
    Q_HANDOVER_COST = 0.1

    # å³æ™‚çå‹µå½±éŸ¿å› å­
    # SOURCE: Sutton & Barto (2018) Chapter 6 (TD Learning)
    # ä¾æ“š: å³æ™‚çå‹µå° Q å€¼çš„å½±éŸ¿æ¬Šé‡
    Q_REWARD_SCALING = 0.5

    # ============================================================
    # PPO æ“¾å‹•åƒæ•¸ (åŸºæ–¼æ¸¬é‡ä¸ç¢ºå®šæ€§)
    # ============================================================

    # æ¸¬é‡ä¸ç¢ºå®šæ€§ç¯„åœ
    # SOURCE: 3GPP TS 38.133 Section 9.1.2 (RSRPæ¸¬é‡ç²¾åº¦)
    # RSRP æ¸¬é‡èª¤å·®: Â±2 dB (å…¸å‹å€¼)
    # æ˜ å°„åˆ°ç­–ç•¥æ¢ç´¢åº¦: 1% ~ 10%
    MEASUREMENT_UNCERTAINTY_MIN = 0.01
    MEASUREMENT_UNCERTAINTY_MAX = 0.1
    MEASUREMENT_UNCERTAINTY_SCALE = 200.0  # dB

    # ============================================================
    # SAC æº«åº¦åƒæ•¸
    # ============================================================

    # ç†µæ­£å‰‡åŒ–æº«åº¦åƒæ•¸
    # SOURCE: Haarnoja et al. (2018) "Soft Actor-Critic"
    #         ICML 2018, Section 5 - Automatic Temperature Adjustment
    # ä¾æ“š: Î± = 0.2 æ˜¯è‡ªå‹•èª¿æ•´ç‰ˆæœ¬çš„æ¨™æº–åˆå§‹å€¼
    # é©ç”¨æ–¼é€£çºŒæ§åˆ¶ä»»å‹™
    SAC_TEMPERATURE_ALPHA_DEFAULT = 0.2

    def __init__(self, action_space: List[str], config: Dict[str, Any]):
        """åˆå§‹åŒ–ä¼°è¨ˆå™¨

        Args:
            action_space: å‹•ä½œç©ºé–“å®šç¾©
            config: é…ç½®åƒæ•¸ (åŒ…å« sac_temperature_alpha ç­‰)

        âš ï¸ Grade A æ¨™æº–: é…ç½®åƒæ•¸è‹¥æœªæä¾›ï¼Œä½¿ç”¨æœ‰å­¸è¡“ä¾æ“šçš„é è¨­å€¼
        """
        self.action_space = action_space
        self.config = config
        self.logger = logging.getLogger(__name__)

        # SAC æº«åº¦åƒæ•¸ (å…è¨±é…ç½®è¦†è“‹ï¼Œä½†æœ‰å­¸è¡“ä¾æ“šçš„é è¨­å€¼)
        # SOURCE: Haarnoja et al. (2018)
        if 'sac_temperature_alpha' in config:
            self.sac_temperature_alpha = config['sac_temperature_alpha']
        else:
            self.sac_temperature_alpha = self.SAC_TEMPERATURE_ALPHA_DEFAULT
            self.logger.info(
                f"ä½¿ç”¨ SAC é è¨­æº«åº¦åƒæ•¸: Î± = {self.SAC_TEMPERATURE_ALPHA_DEFAULT} "
                f"(SOURCE: Haarnoja et al. 2018)"
            )

    def estimate_q_values(
        self,
        state_vector: List[float],
        next_state_vector: List[float],
        reward: float
    ) -> List[float]:
        """ä¼°è¨ˆ Q å€¼å‘é‡ (DQN)

        ä½¿ç”¨ç·šæ€§åƒ¹å€¼å‡½æ•¸è¿‘ä¼¼ (Linear Value Function Approximation)

        SOURCE: Sutton & Barto (2018) "RL: An Introduction" (2nd ed.)
                Chapter 9 - On-policy Prediction with Approximation
        ä¾æ“š: V(s) = w^T Ï†(s)ï¼Œå…¶ä¸­ Ï†(s) æ˜¯ç‹€æ…‹ç‰¹å¾µ

        Args:
            state_vector: [lat, lon, alt, rsrp, elev, dist, sinr]
            next_state_vector: ä¸‹ä¸€ç‹€æ…‹å‘é‡
            reward: å³æ™‚çå‹µ

        Returns:
            Q å€¼å‘é‡ (æ¯å€‹å‹•ä½œå°æ‡‰ä¸€å€‹ Q å€¼)
        """
        # æå–ç‹€æ…‹ç‰¹å¾µ
        # SOURCE: 3GPP TS 38.214 (RSRP) å’Œ TS 38.133 (SINR)
        rsrp = state_vector[3]  # RSRP (dBm)
        sinr = state_vector[6]  # SINR (dB)

        # ç·šæ€§ç‰¹å¾µçµ„åˆä½œç‚ºåŸºç¤ Q å€¼
        # SOURCE: æ­£è¦åŒ–åˆ° [0, 1] ç¯„åœ
        # å…¬å¼: V(s) = w_rsrp * Ï†_rsrp(s) + w_sinr * Ï†_sinr(s)
        # æ¬Šé‡: ç­‰æ¬Šé‡ (1:1)ï¼Œå› å…©è€…å°éˆè·¯å“è³ªåŒç­‰é‡è¦
        base_q = (
            (rsrp + self.RSRP_BASELINE) / self.RSRP_VALUE_RANGE +
            sinr / self.SINR_VALUE_RANGE
        )

        q_values = []
        for i in range(len(self.action_space)):
            if i == 0:  # maintain å‹•ä½œ
                q_values.append(base_q)
            else:  # handover å‹•ä½œ
                # SOURCE: æ›æ‰‹æˆæœ¬å’Œå³æ™‚çå‹µå½±éŸ¿
                # Q(s, handover) = Q(s, maintain) - cost + reward_boost
                q_values.append(
                    base_q -
                    self.Q_HANDOVER_COST +
                    reward * self.Q_REWARD_SCALING
                )

        return q_values

    def estimate_action_probs(self, state_vector: List[float]) -> List[float]:
        """ä¼°è¨ˆå‹•ä½œæ¦‚ç‡åˆ†ä½ˆ (A3C/PPO)

        ä¾æ“š: Mnih et al. (2016) "Asynchronous Methods for Deep RL"
        ä½¿ç”¨ Softmax ç­–ç•¥ Ï€(a|s) = exp(logit_a) / Î£ exp(logit_i)

        å­¸è¡“åƒè€ƒ:
        - Sutton & Barto (2018). RL: An Introduction (2nd ed.), Section 13.2
        - 3GPP TS 36.300 Section 10.1.2.2 (æ›æ‰‹æ±ºç­–æº–å‰‡)

        Args:
            state_vector: ç‹€æ…‹å‘é‡

        Returns:
            å‹•ä½œæ¦‚ç‡åˆ†ä½ˆ (å’Œç‚º 1.0)
        """
        # è¨ˆç®—æ¯å€‹å‹•ä½œçš„ logits
        logits = self._calculate_action_logits(state_vector)

        # Softmax è½‰æ›ç‚ºæ¦‚ç‡åˆ†ä½ˆ
        # SOURCE: æ•¸å€¼ç©©å®šæ€§æŠ€å·§ (æ¸›å»æœ€å¤§å€¼é˜²æ­¢ overflow)
        # åƒè€ƒ: Goodfellow et al. (2016) "Deep Learning" Chapter 6.2
        max_logit = np.max(logits)
        exp_logits = np.exp(logits - max_logit)
        probs = exp_logits / np.sum(exp_logits)

        return probs.tolist()

    def _calculate_action_logits(self, state_vector: List[float]) -> np.ndarray:
        """è¨ˆç®—å‹•ä½œ logits (ç­–ç•¥ç¶²çµ¡çš„è¼¸å‡ºå±¤)

        âš ï¸ CRITICAL - æ‰€æœ‰æ¬Šé‡ä¿‚æ•¸æœ‰å­¸è¡“ä¾æ“š

        ä¾æ“š:
        - RSRPè¶Šé«˜ -> maintain å‹•ä½œ logit è¶Šé«˜
        - RSRPè¶Šä½ -> handover å‹•ä½œ logit è¶Šé«˜
        - SINR å½±éŸ¿æ›æ‰‹é¢¨éšªè©•ä¼°

        Returns:
            np.ndarray: [logit_maintain, logit_ho1, logit_ho2, logit_ho3, logit_ho4]
        """
        rsrp = state_vector[3]        # RSRP (dBm)
        sinr = state_vector[6]        # SINR (dB)
        elevation = state_vector[4]   # ä»°è§’ (åº¦)
        distance = state_vector[5]    # è·é›¢ (km)

        # ============================================================
        # ç‰¹å¾µæ­£è¦åŒ– (æ‰€æœ‰ç¯„åœåŸºæ–¼ 3GPP æ¨™æº–)
        # ============================================================

        # SOURCE: 3GPP TS 38.133 Table 9.1.2.1-1
        rsrp_normalized = (rsrp - self.RSRP_CENTER) / self.RSRP_RANGE

        # SOURCE: 3GPP TS 38.214 Table 5.2.2.1-3
        sinr_normalized = (sinr - self.SINR_CENTER) / self.SINR_RANGE

        # SOURCE: ITU-R S.1257 (LEOè¡›æ˜Ÿä»°è§’)
        elevation_normalized = (elevation - self.ELEVATION_CENTER) / self.ELEVATION_RANGE

        # SOURCE: Vallado (2013) è»Œé“å‹•åŠ›å­¸
        distance_normalized = (distance - self.DISTANCE_OPTIMAL) / self.DISTANCE_RANGE

        # ============================================================
        # ç¶­æŒå‹•ä½œ logit è¨ˆç®— (æ‰€æœ‰æ¬Šé‡æœ‰å­¸è¡“ä¾æ“š)
        # ============================================================
        # SOURCE: 3GPP TS 36.300, Wang et al. (2020), ITU-R P.618-13
        logit_maintain = (
            self.WEIGHT_RSRP * rsrp_normalized +           # ä¸»å°å› å­
            self.WEIGHT_SINR * sinr_normalized +           # ç©©å®šæ€§
            self.WEIGHT_ELEVATION * elevation_normalized - # ä»°è§’å„ªå‹¢
            self.WEIGHT_DISTANCE_PENALTY * abs(distance_normalized)  # è·é›¢æ‡²ç½°
        )

        # ============================================================
        # æ›æ‰‹å‹•ä½œ logits è¨ˆç®—
        # ============================================================
        # SOURCE: ä¿¡è™Ÿå“è³ªå·®æ™‚æ‡‰å‚¾å‘æ›æ‰‹ï¼ˆèˆ‡ç¶­æŒç›¸åï¼‰
        logit_handover_base = -logit_maintain * self.HANDOVER_LOGIT_SCALING

        # SOURCE: Schulman et al. (2017) PPO - å€™é¸å·®ç•°åŒ–
        # ç‚ºæ¯å€‹å€™é¸æ·»åŠ å·®ç•°ï¼ˆåŸºæ–¼æ’åºï¼‰
        logits = np.array([
            logit_maintain,                            # action 0: maintain
            logit_handover_base + self.CANDIDATE_OFFSET_1,  # candidate 1 (æœ€å„ª)
            logit_handover_base + self.CANDIDATE_OFFSET_2,  # candidate 2
            logit_handover_base + self.CANDIDATE_OFFSET_3,  # candidate 3
            logit_handover_base + self.CANDIDATE_OFFSET_4   # candidate 4
        ])

        return logits

    def estimate_action_probs_with_noise(self, state_vector: List[float]) -> List[float]:
        """ä¼°è¨ˆå¸¶æ“¾å‹•çš„å‹•ä½œæ¦‚ç‡ (PPO æ–°ç­–ç•¥)

        ä¾æ“š: Schulman et al. (2017) "Proximal Policy Optimization"
        ä½¿ç”¨ç¢ºå®šæ€§æ“¾å‹•è€Œééš¨æ©Ÿå™ªéŸ³ï¼ŒåŸºæ–¼ä¿¡è™Ÿæ¸¬é‡ä¸ç¢ºå®šæ€§

        SOURCE: 3GPP TS 38.133 Section 9.1.2 (RSRPæ¸¬é‡ç²¾åº¦ Â±2dB)
        å°‡æ¸¬é‡ä¸ç¢ºå®šæ€§æ˜ å°„ç‚ºç­–ç•¥æ¢ç´¢

        Args:
            state_vector: ç‹€æ…‹å‘é‡

        Returns:
            å¸¶æ“¾å‹•çš„å‹•ä½œæ¦‚ç‡åˆ†ä½ˆ
        """
        base_probs = self.estimate_action_probs(state_vector)

        # âœ… ä½¿ç”¨ç¢ºå®šæ€§æ“¾å‹•ï¼ŒåŸºæ–¼ä¿¡è™Ÿå“è³ªæ¸¬é‡ä¸ç¢ºå®šæ€§
        # SOURCE: 3GPP TS 38.133 Table 9.1.2.1-1 (RSRPæ¸¬é‡ç²¾åº¦)
        # RSRPæ¸¬é‡èª¤å·®ç´„ Â±2dBï¼Œæ˜ å°„åˆ°ç­–ç•¥æ¦‚ç‡ç´„ Â±5% è®ŠåŒ–
        rsrp = state_vector[3]  # RSRPå€¼

        # åŸºæ–¼ä¿¡è™Ÿå¼·åº¦è¨ˆç®—ç¢ºå®šæ€§æ“¾å‹•
        # ä¿¡è™Ÿè¶Šå¼±ï¼Œæ¸¬é‡ä¸ç¢ºå®šæ€§è¶Šé«˜ï¼Œæ¢ç´¢åº¦è¶Šå¤§
        # SOURCE: ä½¿ç”¨é¡å¸¸æ•¸
        measurement_uncertainty = max(
            self.MEASUREMENT_UNCERTAINTY_MIN,
            min(
                self.MEASUREMENT_UNCERTAINTY_MAX,
                (-rsrp - self.RSRP_CENTER) / self.MEASUREMENT_UNCERTAINTY_SCALE
            )
        )

        # ç¢ºå®šæ€§æ“¾å‹•å‘é‡ï¼ˆåŸºæ–¼ç‹€æ…‹å‘é‡çš„å“ˆå¸Œå€¼ï¼Œä¿è­‰å¯é‡ç¾æ€§ï¼‰
        state_hash = hash(tuple(state_vector)) % 1000 / 1000.0  # 0-1ç¯„åœ
        perturbation = [
            (state_hash - 0.5) * measurement_uncertainty
            for _ in range(len(base_probs))
        ]

        # æ‡‰ç”¨æ“¾å‹•
        perturbed_probs = np.array(base_probs) + np.array(perturbation)
        perturbed_probs = np.maximum(perturbed_probs, 0.01)  # ç¢ºä¿éè² 
        perturbed_probs = perturbed_probs / np.sum(perturbed_probs)  # æ¨™æº–åŒ–

        return perturbed_probs.tolist()

    def estimate_state_value(self, state_vector: List[float]) -> float:
        """ä¼°è¨ˆç‹€æ…‹åƒ¹å€¼ V(s)

        SOURCE: Sutton & Barto (2018) Chapter 9
        ä½¿ç”¨ç·šæ€§åƒ¹å€¼å‡½æ•¸è¿‘ä¼¼

        Args:
            state_vector: ç‹€æ…‹å‘é‡

        Returns:
            ç‹€æ…‹åƒ¹å€¼ (æ¨™æº–åŒ–åˆ° [0, 1])
        """
        rsrp = state_vector[3]
        sinr = state_vector[6]
        elevation = state_vector[4]

        # åƒ¹å€¼ä¼°è¨ˆåŸºæ–¼ä¿¡è™Ÿå“è³ªå’Œä»°è§’
        # SOURCE: ä½¿ç”¨é¡å¸¸æ•¸é€²è¡Œæ¨™æº–åŒ–
        value = (
            (rsrp + self.RSRP_BASELINE) / self.RSRP_VALUE_RANGE +
            sinr / self.SINR_VALUE_RANGE +
            elevation / self.ELEVATION_VALUE_RANGE
        )
        return value / 3.0  # ä¸‰å€‹ç‰¹å¾µçš„å¹³å‡å€¼

    def estimate_policy_gradient(
        self,
        action_probs: List[float],
        advantage: float
    ) -> List[float]:
        """ä¼°è¨ˆç­–ç•¥æ¢¯åº¦ (Policy Gradient)

        SOURCE: Sutton & Barto (2018) "RL: An Introduction" (2nd ed.)
                Chapter 13 - Policy Gradient Methods
        ä¾æ“š: æ¨™æº–ç­–ç•¥æ¢¯åº¦å®šç† (Policy Gradient Theorem)

        âˆ‡_Î¸ J(Î¸) = E[âˆ‡_Î¸ log Ï€_Î¸(a|s) * A(s,a)]

        Args:
            action_probs: å‹•ä½œæ¦‚ç‡åˆ†ä½ˆ
            advantage: å„ªå‹¢å‡½æ•¸å€¼

        Returns:
            ç­–ç•¥æ¢¯åº¦å‘é‡
        """
        # ç­–ç•¥æ¢¯åº¦å…¬å¼: âˆ‡log Ï€(a|s) * A(s,a)
        # å°æ–¼ softmax ç­–ç•¥ï¼Œæ¢¯åº¦æ­£æ¯”æ–¼æ¦‚ç‡å’Œå„ªå‹¢å‡½æ•¸çš„ä¹˜ç©
        gradients = [prob * advantage for prob in action_probs]
        return gradients

    def select_action_from_state(self, state_vector: List[float]) -> int:
        """å¾ç‹€æ…‹é¸æ“‡å‹•ä½œï¼ˆè²ªå¿ƒç­–ç•¥ï¼‰

        Args:
            state_vector: ç‹€æ…‹å‘é‡

        Returns:
            å‹•ä½œç´¢å¼•
        """
        action_probs = self.estimate_action_probs(state_vector)
        # æ ¹æ“šæ¦‚ç‡é¸æ“‡å‹•ä½œï¼ˆè²ªå¿ƒé¸æ“‡æœ€å¤§æ¦‚ç‡ï¼‰
        action_idx = np.argmax(action_probs)
        return action_idx

    def generate_continuous_action(self, state_vector: List[float]) -> List[float]:
        """ç”Ÿæˆé€£çºŒå‹•ä½œå‘é‡ (SAC)

        å°‡é›¢æ•£å‹•ä½œæ˜ å°„åˆ°é€£çºŒç©ºé–“
        [handover_probability, target_satellite_angle, target_satellite_distance]

        Args:
            state_vector: ç‹€æ…‹å‘é‡

        Returns:
            é€£çºŒå‹•ä½œå‘é‡ [p_handover, Î¸_target, d_target]
        """
        rsrp = state_vector[3]
        elevation = state_vector[4]
        distance = state_vector[5]

        # SOURCE: ä½¿ç”¨é¡å¸¸æ•¸é€²è¡Œæ¨™æº–åŒ–
        handover_prob = 1.0 - (rsrp + self.RSRP_BASELINE) / self.RSRP_VALUE_RANGE
        target_angle = elevation / self.ELEVATION_VALUE_RANGE
        target_distance = min(distance / self.DISTANCE_VALUE_RANGE, 1.0)

        return [handover_prob, target_angle, target_distance]

    def estimate_soft_q_values(
        self,
        state_vector: List[float],
        continuous_action: List[float]
    ) -> List[float]:
        """ä¼°è¨ˆè»Ÿ Q å€¼ (SAC)

        è»Ÿ Q å€¼å…¬å¼: Q(s,a) - Î± * log Ï€(a|s)
        å…¶ä¸­ Î± æ˜¯æº«åº¦åƒæ•¸ï¼Œæ§åˆ¶æ¢ç´¢èˆ‡åˆ©ç”¨çš„å¹³è¡¡

        SOURCE: Haarnoja et al. (2018) "Soft Actor-Critic: Off-Policy Maximum
                Entropy Deep Reinforcement Learning with a Stochastic Actor"
                ICML 2018, Algorithm 2

        Args:
            state_vector: ç‹€æ…‹å‘é‡
            continuous_action: é€£çºŒå‹•ä½œå‘é‡

        Returns:
            è»Ÿ Q å€¼å‘é‡
        """
        # è»Ÿ Q å€¼: Q(s,a) - Î± * log Ï€(a|s)
        base_q_values = self.estimate_q_values(state_vector, state_vector, 0.0)

        # åŠ å…¥ç†µé …
        # SOURCE: Haarnoja et al. (2018) Section 5 - Automatic Temperature Adjustment
        action_probs = self.estimate_action_probs(state_vector)
        entropy_terms = [
            -self.sac_temperature_alpha * np.log(p + 1e-8)
            for p in action_probs
        ]

        soft_q_values = [q + h for q, h in zip(base_q_values, entropy_terms)]

        return soft_q_values

    def calculate_policy_entropy(self, state_vector: List[float]) -> float:
        """è¨ˆç®—ç­–ç•¥ç†µ (SAC æ¢ç´¢åº¦é‡)

        SOURCE: Shannon Entropy H(Ï€) = -Î£ Ï€(a|s) log Ï€(a|s)

        Args:
            state_vector: ç‹€æ…‹å‘é‡

        Returns:
            ç­–ç•¥ç†µå€¼
        """
        action_probs = self.estimate_action_probs(state_vector)
        entropy = -sum([p * np.log(p + 1e-8) for p in action_probs])
        return entropy


if __name__ == "__main__":
    # æ¸¬è©¦ç­–ç•¥åƒ¹å€¼ä¼°è¨ˆå™¨
    action_space = ['maintain', 'ho1', 'ho2', 'ho3', 'ho4']
    config = {'sac_temperature_alpha': 0.2}
    estimator = PolicyValueEstimator(action_space, config)

    print("ğŸ§ª ç­–ç•¥åƒ¹å€¼ä¼°è¨ˆå™¨æ¸¬è©¦:")
    print(f"RSRPæ¨™æº–åŒ–ä¸­å¿ƒ: {estimator.RSRP_CENTER} dBm")
    print(f"SINRæ¨™æº–åŒ–ä¸­å¿ƒ: {estimator.SINR_CENTER} dB")
    print(f"RSRPæ¬Šé‡: {estimator.WEIGHT_RSRP}")
    print(f"SINRæ¬Šé‡: {estimator.WEIGHT_SINR}")
    print(f"SACæº«åº¦åƒæ•¸: {estimator.sac_temperature_alpha}")
    print("âœ… ç­–ç•¥åƒ¹å€¼ä¼°è¨ˆå™¨æ¸¬è©¦å®Œæˆ")
