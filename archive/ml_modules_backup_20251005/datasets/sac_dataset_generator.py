"""
SAC æ•¸æ“šé›†ç”Ÿæˆå™¨ - Stage 6 ML è¨“ç·´æ•¸æ“šç”Ÿæˆçµ„ä»¶

ç”Ÿæˆ Soft Actor-Critic è¨“ç·´æ•¸æ“šé›†ã€‚

å­¸è¡“åƒè€ƒ:
- Haarnoja, T., et al. (2018). Soft actor-critic: Off-policy maximum entropy deep RL.
  ICML.

ä¾æ“š:
- docs/stages/stage6-research-optimization.md Line 597-623

Author: ORBIT Engine Team
Created: 2025-10-02

ğŸ“ å­¸è¡“åˆè¦æ€§æª¢æŸ¥æé†’:
- ä½¿ç”¨æ¨™æº– SAC æ•¸æ“šæ ¼å¼
- è»ŸQå€¼åŒ…å«ç†µæ­£å‰‡åŒ–é …
- æº«åº¦åƒæ•¸ Î± = 0.2 ä¾†è‡ªè«–æ–‡
"""

import logging
from typing import Dict, Any, List


class SACDatasetGenerator:
    """SAC æ•¸æ“šé›†ç”Ÿæˆå™¨

    ç”Ÿæˆ Soft Actor-Critic è¨“ç·´æ‰€éœ€çš„æ•¸æ“šé›†ã€‚

    SAC éœ€è¦:
    - state: ç•¶å‰ç‹€æ…‹
    - continuous_actions: é€£çºŒå‹•ä½œ (è½‰æ›ç‚ºé€£çºŒç©ºé–“)
    - rewards: çå‹µ
    - next_state: ä¸‹ä¸€ç‹€æ…‹
    - soft_q_values: è»Ÿ Q å€¼
    - policy_entropy: ç­–ç•¥ç†µ (æ¢ç´¢åº¦é‡)
    """

    def __init__(
        self,
        state_encoder,
        reward_calculator,
        policy_value_estimator
    ):
        """åˆå§‹åŒ–ç”Ÿæˆå™¨

        Args:
            state_encoder: ç‹€æ…‹ç·¨ç¢¼å™¨
            reward_calculator: çå‹µè¨ˆç®—å™¨
            policy_value_estimator: ç­–ç•¥åƒ¹å€¼ä¼°è¨ˆå™¨
        """
        self.state_encoder = state_encoder
        self.reward_calculator = reward_calculator
        self.policy_value_estimator = policy_value_estimator
        self.logger = logging.getLogger(__name__)

    def generate(
        self,
        signal_analysis: Dict[str, Any],
        gpp_events: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ç”Ÿæˆ SAC è¨“ç·´æ•¸æ“šé›†

        Returns:
            {
                'state_vectors': List[List[float]],
                'continuous_actions': List[List[float]], # é€£çºŒå‹•ä½œ
                'rewards': List[float],
                'next_state_vectors': List[List[float]],
                'done_flags': List[bool],
                'soft_q_values': List[List[float]],    # è»Ÿ Q å€¼
                'policy_entropy': List[float],          # ç­–ç•¥ç†µ
                'dataset_size': int
            }
        """
        self.logger.info("é–‹å§‹ç”Ÿæˆ SAC æ•¸æ“šé›†")

        state_vectors = []
        continuous_actions = []
        rewards = []
        next_state_vectors = []
        done_flags = []
        soft_q_values = []
        policy_entropy = []

        try:
            satellites = signal_analysis.get('satellites', [])

            for satellite in satellites:
                time_series = satellite.get('time_series', [])

                for j in range(len(time_series) - 1):
                    current_time_point = time_series[j]
                    next_time_point = time_series[j + 1]

                    # æ§‹å»ºç‹€æ…‹å‘é‡
                    state_vector = self.state_encoder.build_state_vector(current_time_point)
                    if state_vector is None:
                        continue

                    next_state_vector = self.state_encoder.build_state_vector(next_time_point)
                    if next_state_vector is None:
                        continue

                    # ç”Ÿæˆé€£çºŒå‹•ä½œ (å°‡é›¢æ•£å‹•ä½œæ˜ å°„åˆ°é€£çºŒç©ºé–“)
                    continuous_action = self.policy_value_estimator.generate_continuous_action(state_vector)

                    # è¨ˆç®—çå‹µ
                    reward, _ = self.reward_calculator.calculate_reward(
                        current_time_point,
                        'maintain',
                        next_time_point
                    )

                    # è¨ˆç®—è»Ÿ Q å€¼ (åŠ å…¥ç†µæ­£å‰‡åŒ–)
                    soft_q_value_vector = self.policy_value_estimator.estimate_soft_q_values(
                        state_vector,
                        continuous_action
                    )

                    # è¨ˆç®—ç­–ç•¥ç†µ (æ¢ç´¢åº¦é‡)
                    entropy = self.policy_value_estimator.calculate_policy_entropy(state_vector)

                    # åˆ¤æ–·çµ‚æ­¢
                    quality_assessment = next_time_point.get('quality_assessment', {})
                    is_done = not quality_assessment.get('is_usable', True)

                    # æ·»åŠ åˆ°æ•¸æ“šé›†
                    state_vectors.append(state_vector)
                    continuous_actions.append(continuous_action)
                    rewards.append(reward)
                    next_state_vectors.append(next_state_vector)
                    done_flags.append(is_done)
                    soft_q_values.append(soft_q_value_vector)
                    policy_entropy.append(entropy)

            dataset_size = len(state_vectors)

            self.logger.info(f"SAC æ•¸æ“šé›†ç”Ÿæˆå®Œæˆ - æ¨£æœ¬æ•¸: {dataset_size}")

            return {
                'state_vectors': state_vectors,
                'continuous_actions': continuous_actions,
                'rewards': rewards,
                'next_state_vectors': next_state_vectors,
                'done_flags': done_flags,
                'soft_q_values': soft_q_values,
                'policy_entropy': policy_entropy,
                'dataset_size': dataset_size
            }

        except Exception as e:
            self.logger.error(f"ç”Ÿæˆ SAC æ•¸æ“šé›†æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}", exc_info=True)
            return {
                'state_vectors': [],
                'continuous_actions': [],
                'rewards': [],
                'next_state_vectors': [],
                'done_flags': [],
                'soft_q_values': [],
                'policy_entropy': [],
                'dataset_size': 0
            }
