"""
DQN æ•¸æ“šé›†ç”Ÿæˆå™¨ - Stage 6 ML è¨“ç·´æ•¸æ“šç”Ÿæˆçµ„ä»¶

ç”Ÿæˆ Deep Q-Network è¨“ç·´æ•¸æ“šé›†ã€‚

å­¸è¡“åƒè€ƒ:
- Mnih, V., et al. (2015). Human-level control through deep reinforcement learning.
  Nature, 518(7540), 529-533.

ä¾æ“š:
- docs/stages/stage6-research-optimization.md Line 597-623

Author: ORBIT Engine Team
Created: 2025-10-02

ğŸ“ å­¸è¡“åˆè¦æ€§æª¢æŸ¥æé†’:
- ä½¿ç”¨æ¨™æº– DQN æ•¸æ“šæ ¼å¼ (state, action, reward, next_state, done)
- Qå€¼ä¼°è¨ˆåŸºæ–¼ç·šæ€§åƒ¹å€¼å‡½æ•¸è¿‘ä¼¼
"""

import logging
from typing import Dict, Any, List


class DQNDatasetGenerator:
    """DQN æ•¸æ“šé›†ç”Ÿæˆå™¨

    ç”Ÿæˆ Deep Q-Network è¨“ç·´æ‰€éœ€çš„æ•¸æ“šé›†ã€‚

    DQN éœ€è¦:
    - state: ç•¶å‰ç‹€æ…‹
    - action: åŸ·è¡Œçš„å‹•ä½œ
    - reward: ç²å¾—çš„çå‹µ
    - next_state: ä¸‹ä¸€ç‹€æ…‹
    - done: çµ‚æ­¢æ¨™è¨˜
    """

    def __init__(
        self,
        state_encoder,
        reward_calculator,
        policy_value_estimator,
        action_space: List[str]
    ):
        """åˆå§‹åŒ–ç”Ÿæˆå™¨

        Args:
            state_encoder: ç‹€æ…‹ç·¨ç¢¼å™¨
            reward_calculator: çå‹µè¨ˆç®—å™¨
            policy_value_estimator: ç­–ç•¥åƒ¹å€¼ä¼°è¨ˆå™¨
            action_space: å‹•ä½œç©ºé–“å®šç¾©
        """
        self.state_encoder = state_encoder
        self.reward_calculator = reward_calculator
        self.policy_value_estimator = policy_value_estimator
        self.action_space = action_space
        self.logger = logging.getLogger(__name__)

    def generate(
        self,
        signal_analysis: Dict[str, Any],
        gpp_events: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ç”Ÿæˆ DQN è¨“ç·´æ•¸æ“šé›†

        Returns:
            {
                'state_vectors': List[List[float]],  # (N, 7) ç‹€æ…‹å‘é‡
                'action_space': List[str],            # å‹•ä½œåç¨±
                'q_values': List[List[float]],        # (N, action_space_size) Qå€¼
                'reward_values': List[float],         # (N,) çå‹µå€¼
                'next_state_vectors': List[List[float]], # (N, 7) ä¸‹ä¸€ç‹€æ…‹
                'done_flags': List[bool],             # (N,) çµ‚æ­¢æ¨™è¨˜
                'dataset_size': int
            }
        """
        self.logger.info("é–‹å§‹ç”Ÿæˆ DQN æ•¸æ“šé›†")

        state_vectors = []
        q_values = []
        reward_values = []
        next_state_vectors = []
        done_flags = []

        try:
            # å¾ signal_analysis æå–è¡›æ˜Ÿæ•¸æ“š
            satellites = signal_analysis.get('satellites', [])

            for i, satellite in enumerate(satellites):
                # æª¢æŸ¥æ˜¯å¦æœ‰æ™‚é–“åºåˆ—æ•¸æ“š
                time_series = satellite.get('time_series', [])

                for j in range(len(time_series) - 1):
                    current_time_point = time_series[j]
                    next_time_point = time_series[j + 1]

                    # æ§‹å»ºç•¶å‰ç‹€æ…‹å‘é‡
                    state_vector = self.state_encoder.build_state_vector(current_time_point)
                    if state_vector is None:
                        continue

                    # æ§‹å»ºä¸‹ä¸€ç‹€æ…‹å‘é‡
                    next_state_vector = self.state_encoder.build_state_vector(next_time_point)
                    if next_state_vector is None:
                        continue

                    # è¨ˆç®—çå‹µ
                    action = 'maintain' if i == 0 else 'handover_to_candidate_1'
                    reward, _ = self.reward_calculator.calculate_reward(
                        current_time_point,
                        action,
                        next_time_point
                    )

                    # è¨ˆç®— Q å€¼ (åŸºæ–¼ä¿¡è™Ÿå“è³ªå’Œçå‹µä¼°è¨ˆ)
                    q_value_vector = self.policy_value_estimator.estimate_q_values(
                        state_vector,
                        next_state_vector,
                        reward
                    )

                    # åˆ¤æ–·æ˜¯å¦çµ‚æ­¢ (ä¿¡è™Ÿä¸­æ–·)
                    quality_assessment = next_time_point.get('quality_assessment', {})
                    is_done = not quality_assessment.get('is_usable', True)

                    # æ·»åŠ åˆ°æ•¸æ“šé›†
                    state_vectors.append(state_vector)
                    q_values.append(q_value_vector)
                    reward_values.append(reward)
                    next_state_vectors.append(next_state_vector)
                    done_flags.append(is_done)

            dataset_size = len(state_vectors)

            self.logger.info(f"DQN æ•¸æ“šé›†ç”Ÿæˆå®Œæˆ - æ¨£æœ¬æ•¸: {dataset_size}")

            return {
                'state_vectors': state_vectors,
                'action_space': self.action_space,
                'q_values': q_values,
                'reward_values': reward_values,
                'next_state_vectors': next_state_vectors,
                'done_flags': done_flags,
                'dataset_size': dataset_size
            }

        except Exception as e:
            self.logger.error(f"ç”Ÿæˆ DQN æ•¸æ“šé›†æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}", exc_info=True)
            return {
                'state_vectors': [],
                'action_space': self.action_space,
                'q_values': [],
                'reward_values': [],
                'next_state_vectors': [],
                'done_flags': [],
                'dataset_size': 0
            }
