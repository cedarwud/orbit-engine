"""
A3C æ•¸æ“šé›†ç”Ÿæˆå™¨ - Stage 6 ML è¨“ç·´æ•¸æ“šç”Ÿæˆçµ„ä»¶

ç”Ÿæˆ Asynchronous Advantage Actor-Critic è¨“ç·´æ•¸æ“šé›†ã€‚

å­¸è¡“åƒè€ƒ:
- Mnih, V., et al. (2016). Asynchronous methods for deep reinforcement learning.
  ICML.

ä¾æ“š:
- docs/stages/stage6-research-optimization.md Line 597-623

Author: ORBIT Engine Team
Created: 2025-10-02

ğŸ“ å­¸è¡“åˆè¦æ€§æª¢æŸ¥æé†’:
- ä½¿ç”¨æ¨™æº– Actor-Critic æ•¸æ“šæ ¼å¼
- å„ªå‹¢å‡½æ•¸ A(s,a) = Q(s,a) - V(s)
"""

import logging
from typing import Dict, Any, List


class A3CDatasetGenerator:
    """A3C æ•¸æ“šé›†ç”Ÿæˆå™¨

    ç”Ÿæˆ Asynchronous Advantage Actor-Critic è¨“ç·´æ‰€éœ€çš„æ•¸æ“šé›†ã€‚

    A3C éœ€è¦:
    - state: ç•¶å‰ç‹€æ…‹
    - action_probs: ç­–ç•¥æ¦‚ç‡åˆ†ä½ˆ
    - value_estimates: åƒ¹å€¼ä¼°è¨ˆ
    - advantage_functions: å„ªå‹¢å‡½æ•¸ A(s,a) = Q(s,a) - V(s)
    """

    def __init__(
        self,
        state_encoder,
        reward_calculator,
        policy_value_estimator,
        config: Dict[str, Any]
    ):
        """åˆå§‹åŒ–ç”Ÿæˆå™¨

        Args:
            state_encoder: ç‹€æ…‹ç·¨ç¢¼å™¨
            reward_calculator: çå‹µè¨ˆç®—å™¨
            policy_value_estimator: ç­–ç•¥åƒ¹å€¼ä¼°è¨ˆå™¨
            config: é…ç½®åƒæ•¸ (åŒ…å« discount_factor)
        """
        self.state_encoder = state_encoder
        self.reward_calculator = reward_calculator
        self.policy_value_estimator = policy_value_estimator
        self.config = config
        self.logger = logging.getLogger(__name__)

    def generate(
        self,
        signal_analysis: Dict[str, Any],
        gpp_events: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ç”Ÿæˆ A3C è¨“ç·´æ•¸æ“šé›†

        Returns:
            {
                'state_vectors': List[List[float]],
                'action_probs': List[List[float]],    # ç­–ç•¥æ¦‚ç‡
                'value_estimates': List[float],        # åƒ¹å€¼ä¼°è¨ˆ
                'advantage_functions': List[float],    # å„ªå‹¢å‡½æ•¸
                'policy_gradients': List[List[float]], # ç­–ç•¥æ¢¯åº¦
                'dataset_size': int
            }
        """
        self.logger.info("é–‹å§‹ç”Ÿæˆ A3C æ•¸æ“šé›†")

        state_vectors = []
        action_probs = []
        value_estimates = []
        advantage_functions = []
        policy_gradients = []

        try:
            satellites = signal_analysis.get('satellites', [])

            for satellite in satellites:
                time_series = satellite.get('time_series', [])

                for j in range(len(time_series) - 1):
                    current_time_point = time_series[j]
                    next_time_point = time_series[j + 1]

                    # æ§‹å»ºç‹€æ…‹å‘é‡
                    state_vector = self.state_encoder.build_state_vector(current_time_point)
                    if state_vector is None:
                        continue

                    # è¨ˆç®—ç­–ç•¥æ¦‚ç‡ (åŸºæ–¼ä¿¡è™Ÿå“è³ª)
                    action_prob_vector = self.policy_value_estimator.estimate_action_probs(state_vector)

                    # è¨ˆç®—åƒ¹å€¼ä¼°è¨ˆ V(s)
                    value_estimate = self.policy_value_estimator.estimate_state_value(state_vector)

                    # è¨ˆç®—å„ªå‹¢å‡½æ•¸
                    reward, _ = self.reward_calculator.calculate_reward(
                        current_time_point,
                        'maintain',
                        next_time_point
                    )
                    next_value = self.policy_value_estimator.estimate_state_value(
                        self.state_encoder.build_state_vector(next_time_point) or [0] * 7
                    )
                    advantage = reward + self.config['discount_factor'] * next_value - value_estimate

                    # è¨ˆç®—ç­–ç•¥æ¢¯åº¦
                    policy_gradient = self.policy_value_estimator.estimate_policy_gradient(
                        action_prob_vector,
                        advantage
                    )

                    # æ·»åŠ åˆ°æ•¸æ“šé›†
                    state_vectors.append(state_vector)
                    action_probs.append(action_prob_vector)
                    value_estimates.append(value_estimate)
                    advantage_functions.append(advantage)
                    policy_gradients.append(policy_gradient)

            dataset_size = len(state_vectors)

            self.logger.info(f"A3C æ•¸æ“šé›†ç”Ÿæˆå®Œæˆ - æ¨£æœ¬æ•¸: {dataset_size}")

            return {
                'state_vectors': state_vectors,
                'action_probs': action_probs,
                'value_estimates': value_estimates,
                'advantage_functions': advantage_functions,
                'policy_gradients': policy_gradients,
                'dataset_size': dataset_size
            }

        except Exception as e:
            self.logger.error(f"ç”Ÿæˆ A3C æ•¸æ“šé›†æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}", exc_info=True)
            return {
                'state_vectors': [],
                'action_probs': [],
                'value_estimates': [],
                'advantage_functions': [],
                'policy_gradients': [],
                'dataset_size': 0
            }
