"""
PPO æ•¸æ“šé›†ç”Ÿæˆå™¨ - Stage 6 ML è¨“ç·´æ•¸æ“šç”Ÿæˆçµ„ä»¶

ç”Ÿæˆ Proximal Policy Optimization è¨“ç·´æ•¸æ“šé›†ã€‚

å­¸è¡“åƒè€ƒ:
- Schulman, J., et al. (2017). Proximal policy optimization algorithms.
  arXiv:1707.06347.

ä¾æ“š:
- docs/stages/stage6-research-optimization.md Line 597-623

Author: ORBIT Engine Team
Created: 2025-10-02

ğŸ“ å­¸è¡“åˆè¦æ€§æª¢æŸ¥æé†’:
- ä½¿ç”¨æ¨™æº– PPO æ•¸æ“šæ ¼å¼
- clip_epsilon = 0.2 ä¾†è‡ªè«–æ–‡æ¨™æº–è¨­ç½®
"""

import logging
from typing import Dict, Any, List
import numpy as np


class PPODatasetGenerator:
    """PPO æ•¸æ“šé›†ç”Ÿæˆå™¨

    ç”Ÿæˆ Proximal Policy Optimization è¨“ç·´æ‰€éœ€çš„æ•¸æ“šé›†ã€‚

    PPO éœ€è¦:
    - state: ç•¶å‰ç‹€æ…‹
    - actions_taken: å¯¦éš›åŸ·è¡Œçš„å‹•ä½œ
    - old_policy_probs: èˆŠç­–ç•¥æ¦‚ç‡
    - new_policy_probs: æ–°ç­–ç•¥æ¦‚ç‡
    - advantages: å„ªå‹¢å‡½æ•¸
    - returns: å›å ±
    """

    def __init__(
        self,
        state_encoder,
        reward_calculator,
        policy_value_estimator,
        action_space: List[str],
        config: Dict[str, Any]
    ):
        """åˆå§‹åŒ–ç”Ÿæˆå™¨

        Args:
            state_encoder: ç‹€æ…‹ç·¨ç¢¼å™¨
            reward_calculator: çå‹µè¨ˆç®—å™¨
            policy_value_estimator: ç­–ç•¥åƒ¹å€¼ä¼°è¨ˆå™¨
            action_space: å‹•ä½œç©ºé–“å®šç¾©
            config: é…ç½®åƒæ•¸ (åŒ…å« discount_factor, ppo_clip_epsilon)
        """
        self.state_encoder = state_encoder
        self.reward_calculator = reward_calculator
        self.policy_value_estimator = policy_value_estimator
        self.action_space = action_space
        self.config = config
        self.logger = logging.getLogger(__name__)

    def generate(
        self,
        signal_analysis: Dict[str, Any],
        gpp_events: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ç”Ÿæˆ PPO è¨“ç·´æ•¸æ“šé›†

        Returns:
            {
                'state_vectors': List[List[float]],
                'actions_taken': List[int],            # å¯¦éš›å‹•ä½œ
                'old_policy_probs': List[float],       # èˆŠç­–ç•¥æ¦‚ç‡
                'new_policy_probs': List[float],       # æ–°ç­–ç•¥æ¦‚ç‡
                'advantages': List[float],             # å„ªå‹¢å‡½æ•¸
                'returns': List[float],                # å›å ±
                'clipped_ratios': List[float],         # è£å‰ªæ¯”ç‡
                'dataset_size': int
            }
        """
        self.logger.info("é–‹å§‹ç”Ÿæˆ PPO æ•¸æ“šé›†")

        state_vectors = []
        actions_taken = []
        old_policy_probs = []
        new_policy_probs = []
        advantages = []
        returns = []
        clipped_ratios = []

        try:
            satellites = signal_analysis.get('satellites', [])

            for satellite in satellites:
                time_series = satellite.get('time_series', [])

                for j in range(len(time_series) - 1):
                    current_time_point = time_series[j]
                    next_time_point = time_series[j + 1]

                    # æ§‹å»ºç‹€æ…‹å‘é‡
                    state_vector = self.state_encoder.build_state_vector(current_time_point)
                    if state_vector is None:
                        continue

                    # æ±ºå®šå‹•ä½œ (åŸºæ–¼ä¿¡è™Ÿå“è³ª)
                    action_idx = self.policy_value_estimator.select_action_from_state(state_vector)

                    # è¨ˆç®—èˆŠç­–ç•¥æ¦‚ç‡
                    old_action_probs = self.policy_value_estimator.estimate_action_probs(state_vector)
                    old_prob = old_action_probs[action_idx]

                    # è¨ˆç®—æ–°ç­–ç•¥æ¦‚ç‡ (åŠ å…¥æ¢ç´¢å™ªéŸ³)
                    new_action_probs = self.policy_value_estimator.estimate_action_probs_with_noise(state_vector)
                    new_prob = new_action_probs[action_idx]

                    # è¨ˆç®—å„ªå‹¢å‡½æ•¸å’Œå›å ±
                    reward, _ = self.reward_calculator.calculate_reward(
                        current_time_point,
                        self.action_space[action_idx],
                        next_time_point
                    )

                    value_estimate = self.policy_value_estimator.estimate_state_value(state_vector)
                    next_value = self.policy_value_estimator.estimate_state_value(
                        self.state_encoder.build_state_vector(next_time_point) or [0] * 7
                    )
                    advantage = reward + self.config['discount_factor'] * next_value - value_estimate
                    return_value = reward + self.config['discount_factor'] * next_value

                    # è¨ˆç®—è£å‰ªæ¯”ç‡ (PPO æ ¸å¿ƒæ©Ÿåˆ¶)
                    # SOURCE: Schulman et al. (2017) "Proximal Policy Optimization Algorithms"
                    #         arXiv:1707.06347v2, Section 6.1, Table 3
                    # clip_epsilon = 0.2 æ˜¯åŸå§‹è«–æ–‡çš„æ¨™æº–è¶…åƒæ•¸
                    # ä¾æ“š: åœ¨ Atariã€MuJoCo ç­‰ç’°å¢ƒä¸­é©—è­‰æœ‰æ•ˆ
                    ratio = new_prob / (old_prob + 1e-8)
                    clip_epsilon = self.config.get('ppo_clip_epsilon', 0.2)
                    clipped_ratio = np.clip(ratio, 1 - clip_epsilon, 1 + clip_epsilon)

                    # æ·»åŠ åˆ°æ•¸æ“šé›†
                    state_vectors.append(state_vector)
                    actions_taken.append(action_idx)
                    old_policy_probs.append(old_prob)
                    new_policy_probs.append(new_prob)
                    advantages.append(advantage)
                    returns.append(return_value)
                    clipped_ratios.append(clipped_ratio)

            dataset_size = len(state_vectors)

            self.logger.info(f"PPO æ•¸æ“šé›†ç”Ÿæˆå®Œæˆ - æ¨£æœ¬æ•¸: {dataset_size}")

            return {
                'state_vectors': state_vectors,
                'actions_taken': actions_taken,
                'old_policy_probs': old_policy_probs,
                'new_policy_probs': new_policy_probs,
                'advantages': advantages,
                'returns': returns,
                'clipped_ratios': clipped_ratios,
                'dataset_size': dataset_size
            }

        except Exception as e:
            self.logger.error(f"ç”Ÿæˆ PPO æ•¸æ“šé›†æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}", exc_info=True)
            return {
                'state_vectors': [],
                'actions_taken': [],
                'old_policy_probs': [],
                'new_policy_probs': [],
                'advantages': [],
                'returns': [],
                'clipped_ratios': [],
                'dataset_size': 0
            }
