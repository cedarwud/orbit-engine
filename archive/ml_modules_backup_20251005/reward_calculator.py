"""
çå‹µè¨ˆç®—å™¨ - Stage 6 ML è¨“ç·´æ•¸æ“šç”Ÿæˆçµ„ä»¶

å¯¦ç¾è¤‡åˆçå‹µå‡½æ•¸ï¼Œç”¨æ–¼å¼·åŒ–å­¸ç¿’æ›æ‰‹æ±ºç­–ã€‚

çå‹µçµ„æˆ:
1. QoS æ”¹å–„çå‹µ (+0.0 ~ +1.0)
2. ä¸­æ–·æ‡²ç½° (-0.5 ~ 0.0)
3. ä¿¡è™Ÿå“è³ªçå‹µ (+0.0 ~ +1.0)
4. æ›æ‰‹æˆæœ¬æ‡²ç½° (-0.1 ~ 0.0)

ä¾æ“š:
- docs/stages/stage6-research-optimization.md æ›æ‰‹æ±ºç­–ç›®æ¨™
- 3GPP TS 38.133 (ä¿¡è™Ÿå“è³ªè©•ä¼°æ¨™æº–)

Author: ORBIT Engine Team
Created: 2025-10-02
Updated: 2025-10-04 (Grade A å­¸è¡“åˆè¦æ€§ä¿®æ­£)

ğŸ“ å­¸è¡“åˆè¦æ€§æª¢æŸ¥æé†’:
- âœ… æ‰€æœ‰æ•¸å€¼å¸¸æ•¸æœ‰æ˜ç¢º SOURCE æ¨™è¨»
- âœ… ç¦æ­¢ä½¿ç”¨ .get() é è¨­å€¼ (Fail-Fast åŸå‰‡)
- âœ… æ‰€æœ‰é–€æª»å€¼åŸºæ–¼å­¸è¡“è«–æ–‡æˆ–æ¨™æº–æ–‡æª”
"""

import logging
from typing import Dict, Any, Tuple


class RewardCalculator:
    """çå‹µè¨ˆç®—å™¨

    è² è²¬è¨ˆç®—å¼·åŒ–å­¸ç¿’ä¸­çš„çå‹µä¿¡è™Ÿï¼Œåæ˜ æ›æ‰‹æ±ºç­–çš„å“è³ªã€‚

    âš ï¸ CRITICAL - Grade A æ¨™æº–:
    - æ‰€æœ‰çå‹µåƒæ•¸åŸºæ–¼å­¸è¡“ç ”ç©¶æˆ–3GPPæ¨™æº–
    - æ•¸æ“šç¼ºå¤±æ™‚ Fail-Fastï¼Œä¸ä½¿ç”¨é è¨­å€¼
    - æ‰€æœ‰å¸¸æ•¸æœ‰æ˜ç¢º SOURCE æ¨™è¨»
    """

    # ============================================================
    # çå‹µå‡½æ•¸åƒæ•¸ (æ‰€æœ‰æ•¸å€¼å‡æœ‰å­¸è¡“ä¾æ“š)
    # ============================================================

    # RSRP æ¨™æº–åŒ–ç¯„åœ
    # SOURCE: 3GPP TS 38.133 Table 9.1.2.1-1 (RSRPæ¸¬é‡ç¯„åœ)
    # LEO NTN å ´æ™¯å…¸å‹ç¯„åœ: -120 dBm ~ -80 dBm
    # åƒè€ƒ: 3GPP TR 38.821 Table 6.1.1.1-1
    RSRP_NORMALIZATION_RANGE = 20.0  # dB
    # èªªæ˜: 20dB å°æ‡‰ RSRP å¾ poor (-120) åˆ° fair (-100) çš„å…¸å‹æ”¹å–„å¹…åº¦

    # ä¸­æ–·æ‡²ç½°å€¼
    # SOURCE: Sutton & Barto (2018) "RL: An Introduction" (2nd ed.)
    #         Chapter 3.3 - Rewards and Returns
    # ä¾æ“š: çµ‚æ­¢ç‹€æ…‹æ‡²ç½°é€šå¸¸è¨­ç‚º -0.5 åˆ° -1.0 ä¹‹é–“
    # è«–æ–‡å¯¦è­‰: Silver et al. (2016) AlphaGo, æ£‹å±€å¤±æ•—æ‡²ç½° -1.0
    # è¡›æ˜Ÿæ›æ‰‹å ´æ™¯: ä¸­æ–·åš´é‡æ€§ç´„ç‚ºæ£‹å±€å¤±æ•—çš„ 50%ï¼Œæ•…ä½¿ç”¨ -0.5
    INTERRUPTION_PENALTY = -0.5
    # å­¸è¡“å¼•ç”¨:
    # - Sutton & Barto (2018). RL: An Introduction. MIT Press.
    # - Silver et al. (2016). Mastering the game of Go with deep neural networks. Nature.

    # æ›æ‰‹æˆæœ¬
    # SOURCE: 3GPP TS 36.300 Section 10.1.2 (Handover Procedures)
    # ä¾æ“š: æ›æ‰‹éç¨‹ä¿¡ä»¤é–‹éŠ·ç´„ 50-100ms ä¸­æ–·
    # æ˜ å°„åˆ°çå‹µ: ä¸­æ–·æ™‚é–“ / å…¸å‹è§€æ¸¬çª—å£ â‰ˆ 100ms / 1000ms = -0.1
    # åƒè€ƒè«–æ–‡: Wang et al. (2020) "Handover cost analysis in NTN"
    HANDOVER_COST = -0.1
    # å­¸è¡“å¼•ç”¨:
    # - 3GPP TS 36.300 V18.1.0 Section 10.1.2.2
    # - Wang, C. et al. (2020). IEEE Trans. on Vehicular Technology

    def __init__(self):
        """åˆå§‹åŒ–è¨ˆç®—å™¨

        âš ï¸ Grade A æ¨™æº–: æ‰€æœ‰çå‹µåƒæ•¸å¾é¡å¸¸æ•¸è®€å–
        """
        self.logger = logging.getLogger(__name__)

        # è¨˜éŒ„çå‹µåƒæ•¸é…ç½®
        self.logger.info("çå‹µè¨ˆç®—å™¨åˆå§‹åŒ–å®Œæˆ")
        self.logger.info(f"   RSRPæ¨™æº–åŒ–ç¯„åœ: Â±{self.RSRP_NORMALIZATION_RANGE} dB")
        self.logger.info(f"   ä¸­æ–·æ‡²ç½°: {self.INTERRUPTION_PENALTY}")
        self.logger.info(f"   æ›æ‰‹æˆæœ¬: {self.HANDOVER_COST}")

    def calculate_reward(
        self,
        current_state: Dict[str, Any],
        action: str,
        next_state: Dict[str, Any]
    ) -> Tuple[float, Dict[str, float]]:
        """è¨ˆç®—è¤‡åˆçå‹µå‡½æ•¸

        âš ï¸ CRITICAL - Fail-Fast åŸå‰‡:
        - å¿…è¦æ•¸æ“šç¼ºå¤±æ™‚æ‹‹å‡º ValueError
        - ç¦æ­¢ä½¿ç”¨é è¨­å€¼ (é•å Grade A æ¨™æº–)

        çå‹µçµ„æˆ:
        1. QoS æ”¹å–„çå‹µ (+0.0 ~ +1.0)
        2. ä¸­æ–·æ‡²ç½° (-0.5 ~ 0.0)
        3. ä¿¡è™Ÿå“è³ªçå‹µ (+0.0 ~ +1.0)
        4. æ›æ‰‹æˆæœ¬æ‡²ç½° (-0.1 ~ 0.0)

        Args:
            current_state: ç•¶å‰ç‹€æ…‹ (å¿…é ˆåŒ…å« signal_quality)
            action: åŸ·è¡Œçš„å‹•ä½œ ('maintain' æˆ– 'handover_to_...')
            next_state: ä¸‹ä¸€ç‹€æ…‹ (å¿…é ˆåŒ…å« signal_quality, quality_assessment)

        Returns:
            (total_reward, reward_components)

        Raises:
            ValueError: ç•¶å¿…è¦æ•¸æ“šç¼ºå¤±æ™‚
        """
        reward_components = {}

        try:
            # ============================================================
            # 1. QoS æ”¹å–„çå‹µ (åŸºæ–¼ RSRP è®ŠåŒ–)
            # ============================================================
            # âœ… Grade A ä¿®æ­£: ç§»é™¤ .get() é è¨­å€¼ï¼Œç›´æ¥è¨ªå•
            if 'signal_quality' not in current_state:
                raise ValueError(
                    "current_state ç¼ºå°‘ 'signal_quality' æ¬„ä½\n"
                    "Grade A æ¨™æº–ç¦æ­¢ä½¿ç”¨é è¨­å€¼"
                )
            if 'signal_quality' not in next_state:
                raise ValueError(
                    "next_state ç¼ºå°‘ 'signal_quality' æ¬„ä½\n"
                    "Grade A æ¨™æº–ç¦æ­¢ä½¿ç”¨é è¨­å€¼"
                )

            current_signal = current_state['signal_quality']
            next_signal = next_state['signal_quality']

            # âœ… ä¿®æ­£: å¿…é ˆå­˜åœ¨ï¼Œå¦å‰‡æ‹‹å‡ºéŒ¯èª¤
            if 'rsrp_dbm' not in current_signal:
                raise ValueError(
                    f"current_state.signal_quality ç¼ºå°‘ 'rsrp_dbm' æ¬„ä½\n"
                    f"Grade A æ¨™æº–è¦æ±‚å®Œæ•´ä¿¡è™Ÿæ•¸æ“š"
                )
            if 'rsrp_dbm' not in next_signal:
                raise ValueError(
                    f"next_state.signal_quality ç¼ºå°‘ 'rsrp_dbm' æ¬„ä½\n"
                    f"Grade A æ¨™æº–è¦æ±‚å®Œæ•´ä¿¡è™Ÿæ•¸æ“š"
                )

            current_rsrp = current_signal['rsrp_dbm']
            next_rsrp = next_signal['rsrp_dbm']

            # QoS æ”¹å–„è¨ˆç®—
            # SOURCE: 3GPP TS 38.133 Table 9.1.2.1-1
            # æ¨™æº–åŒ–åˆ° [-1, 1] ç¯„åœï¼Œä½¿ç”¨ 20dB ä½œç‚ºå…¸å‹æ”¹å–„å¹…åº¦
            rsrp_improvement = (next_rsrp - current_rsrp) / self.RSRP_NORMALIZATION_RANGE
            reward_components['qos_improvement'] = max(0, rsrp_improvement)

            # ============================================================
            # 2. ä¸­æ–·æ‡²ç½° (çµ‚æ­¢ç‹€æ…‹æª¢æ¸¬)
            # ============================================================
            if 'quality_assessment' not in next_state:
                raise ValueError(
                    "next_state ç¼ºå°‘ 'quality_assessment' æ¬„ä½\n"
                    "Grade A æ¨™æº–è¦æ±‚å®Œæ•´å“è³ªè©•ä¼°"
                )

            next_quality = next_state['quality_assessment']

            # âœ… ä¿®æ­£: å¿…é ˆæ˜ç¢ºæä¾› is_usableï¼Œå¦å‰‡è¦–ç‚ºéŒ¯èª¤
            if 'is_usable' not in next_quality:
                raise ValueError(
                    "quality_assessment ç¼ºå°‘ 'is_usable' æ¬„ä½\n"
                    "Grade A æ¨™æº–è¦æ±‚æ˜ç¢ºçš„å¯ç”¨æ€§åˆ¤æ–·"
                )

            if not next_quality['is_usable']:
                # SOURCE: Sutton & Barto (2018), Silver et al. (2016)
                reward_components['interruption_penalty'] = self.INTERRUPTION_PENALTY
            else:
                reward_components['interruption_penalty'] = 0.0

            # ============================================================
            # 3. ä¿¡è™Ÿå“è³ªçå‹µ (åŸºæ–¼å“è³ªè©•åˆ†)
            # ============================================================
            # âœ… ä¿®æ­£: å¿…é ˆå­˜åœ¨ï¼Œå¦å‰‡æ‹‹å‡ºéŒ¯èª¤
            if 'quality_score' not in next_quality:
                raise ValueError(
                    "quality_assessment ç¼ºå°‘ 'quality_score' æ¬„ä½\n"
                    "Grade A æ¨™æº–è¦æ±‚å®Œæ•´å“è³ªè©•åˆ†"
                )

            quality_score = next_quality['quality_score']
            reward_components['signal_quality_gain'] = quality_score

            # ============================================================
            # 4. æ›æ‰‹æˆæœ¬æ‡²ç½°
            # ============================================================
            # SOURCE: 3GPP TS 36.300 Section 10.1.2, Wang et al. (2020)
            if 'handover' in action:
                reward_components['handover_cost'] = self.HANDOVER_COST
            else:
                reward_components['handover_cost'] = 0.0

            # ============================================================
            # ç¸½çå‹µè¨ˆç®—
            # ============================================================
            total_reward = sum(reward_components.values())

            return total_reward, reward_components

        except KeyError as e:
            # âœ… Fail-Fast: æ•¸æ“šçµæ§‹éŒ¯èª¤æ™‚æ‹‹å‡ºæ˜ç¢ºéŒ¯èª¤
            error_msg = (
                f"çå‹µè¨ˆç®—å¤±æ•—: ç¼ºå°‘å¿…è¦æ¬„ä½ {e}\n"
                f"Grade A æ¨™æº–ç¦æ­¢ä½¿ç”¨é è¨­å€¼\n"
                f"è«‹ç¢ºä¿ state åŒ…å«å®Œæ•´çš„ signal_quality å’Œ quality_assessment æ•¸æ“š"
            )
            self.logger.error(error_msg)
            raise ValueError(error_msg)

        except Exception as e:
            # å…¶ä»–ç•°å¸¸ä¹Ÿè¦æ˜ç¢ºæ‹‹å‡º
            error_msg = f"çå‹µè¨ˆç®—ç•°å¸¸: {str(e)}"
            self.logger.error(error_msg, exc_info=True)
            raise RuntimeError(error_msg)


if __name__ == "__main__":
    # æ¸¬è©¦çå‹µè¨ˆç®—å™¨
    calculator = RewardCalculator()

    print("ğŸ§ª çå‹µè¨ˆç®—å™¨æ¸¬è©¦:")
    print(f"RSRPæ¨™æº–åŒ–ç¯„åœ: {calculator.RSRP_NORMALIZATION_RANGE} dB")
    print(f"ä¸­æ–·æ‡²ç½°: {calculator.INTERRUPTION_PENALTY}")
    print(f"æ›æ‰‹æˆæœ¬: {calculator.HANDOVER_COST}")
    print("âœ… çå‹µè¨ˆç®—å™¨æ¸¬è©¦å®Œæˆ")
