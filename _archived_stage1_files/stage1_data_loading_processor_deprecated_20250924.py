#!/usr/bin/env python3
"""
Stage 1: æ•¸æ“šè¼‰å…¥å±¤è™•ç†å™¨ (é‡æ§‹ç‰ˆæœ¬)

é‡æ§‹åŸå‰‡ï¼š
- å°ˆæ³¨æ•¸æ“šè¼‰å…¥å’Œé©—è­‰ï¼Œç§»é™¤SGP4è¨ˆç®—åŠŸèƒ½
- ä½¿ç”¨å…±äº«çš„é©—è­‰æ¡†æ¶å’Œå·¥å…·æ¨¡çµ„
- å¯¦ç¾çµ±ä¸€çš„è™•ç†å™¨æ¥å£
- æä¾›æ¸…æ½”çš„TLEæ•¸æ“šè¼¸å‡ºä¾›Stage 2ä½¿ç”¨

åŠŸèƒ½è®ŠåŒ–ï¼š
- âœ… ä¿ç•™: TLEæ•¸æ“šè¼‰å…¥ã€æ•¸æ“šé©—è­‰
- âŒ ç§»é™¤: SGP4è»Œé“è¨ˆç®—ï¼ˆç§»è‡³Stage 2ï¼‰
- âœ… æ–°å¢: æ™‚é–“åŸºæº–æ¨™æº–åŒ–ã€æ•¸æ“šå®Œæ•´æ€§æª¢æŸ¥
"""

import logging
from datetime import datetime, timezone, timedelta
from typing import Dict, Any, List, Optional
from pathlib import Path

# å…±äº«æ¨¡çµ„å°å…¥
from shared.base_processor import BaseStageProcessor
from shared.interfaces import ProcessingStatus, ProcessingResult, create_processing_result
from shared.validation_framework import ValidationEngine
from shared.utils import TimeUtils, FileUtils
from shared.constants import OrbitEngineConstantsManager
# ğŸš¨ ç§»é™¤æ¸¬è©¦æ¨¡çµ„æ´©æ¼ï¼šç”Ÿç”¢ä»£ç¢¼ä¸æ‡‰å¼•å…¥æ¸¬è©¦æ¨¡çµ„

# Stage 1å°ˆç”¨æ¨¡çµ„ (v2.0æ¨¡çµ„åŒ–æ¶æ§‹)
from .tle_data_loader import TLEDataLoader
from .data_validator import DataValidator
from .time_reference_manager import TimeReferenceManager

logger = logging.getLogger(__name__)


class Stage1DataLoadingProcessor(BaseStageProcessor):
    """
    Stage 1: æ•¸æ“šè¼‰å…¥å±¤è™•ç†å™¨ (é‡æ§‹ç‰ˆæœ¬)

    å°ˆè·è²¬ä»»ï¼š
    1. TLEæ•¸æ“šè¼‰å…¥å’Œè§£æ
    2. æ•¸æ“šæ ¼å¼é©—è­‰å’Œå®Œæ•´æ€§æª¢æŸ¥
    3. æ™‚é–“åŸºæº–æ¨™æº–åŒ–
    4. ç‚ºStage 2æä¾›æ¸…æ½”çš„æ•¸æ“šè¼¸å‡º
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        super().__init__(stage_number=1, stage_name="data_loading", config=config or {})

        # é…ç½®åƒæ•¸
        self.sample_mode = self.config.get('sample_mode', False)
        self.sample_size = self.config.get('sample_size', 100)
        self.validate_tle_epoch = self.config.get('validate_tle_epoch', True)

        # åˆå§‹åŒ–v2.0æ¨¡çµ„åŒ–çµ„ä»¶
        self.tle_loader = TLEDataLoader()
        self.data_validator = DataValidator(self.config)
        self.time_reference_manager = TimeReferenceManager(self.config)
        self.validation_engine = ValidationEngine('stage1')
        self.system_constants = OrbitEngineConstantsManager()

        # è™•ç†çµ±è¨ˆ
        self.processing_stats = {
            'total_files_scanned': 0,
            'total_satellites_loaded': 0,
            'validation_failures': 0,
            'time_reference_issues': 0
        }

        self.logger.info("Stage 1 æ•¸æ“šè¼‰å…¥è™•ç†å™¨å·²åˆå§‹åŒ– (v2.0æ¶æ§‹)")

    def process(self, input_data: Any) -> ProcessingResult:
        """
        ä¸»è¦è™•ç†æ–¹æ³•

        Args:
            input_data: è¼¸å…¥æ•¸æ“šï¼ˆå¯é¸çš„TLEæ•¸æ“šæˆ–é…ç½®ï¼‰

        Returns:
            ProcessingResult: è™•ç†çµæœï¼ŒåŒ…å«è¼‰å…¥çš„TLEæ•¸æ“š
        """
        from shared.interfaces.processor_interface import create_processing_result, ProcessingStatus
        
        start_time = datetime.now(timezone.utc)
        self.logger.info("ğŸš€ é–‹å§‹Stage 1æ•¸æ“šè¼‰å…¥è™•ç†...")

        try:
            # æª¢æŸ¥è¼¸å…¥æ•¸æ“šé¡å‹
            if input_data and isinstance(input_data, dict) and 'tle_data' in input_data:
                # ä½¿ç”¨æä¾›çš„TLEæ•¸æ“š
                self.logger.info("ğŸ“‹ ä½¿ç”¨è¼¸å…¥çš„TLEæ•¸æ“š...")
                tle_data_list = input_data['tle_data']
                loaded_data = self._process_input_tle_data(tle_data_list)
            else:
                # å¾æ–‡ä»¶è¼‰å…¥TLEæ•¸æ“š
                self.logger.info("ğŸ“ å¾æ–‡ä»¶è¼‰å…¥TLEæ•¸æ“š...")
                loaded_data = self._load_tle_data_from_files()

            # æ•¸æ“šé©—è­‰
            validation_result = self._validate_loaded_data(loaded_data)

            # æª¢æŸ¥é©—è­‰çµæœ
            if hasattr(validation_result, 'overall_status'):
                validation_status = validation_result.overall_status
                is_valid = validation_status == 'PASS' or (validation_status == 'PENDING' and len(loaded_data) > 0)
                # è½‰æ›ç‚ºå­—å…¸ç²å–è©³ç´°ä¿¡æ¯
                validation_dict = validation_result.to_dict()
                errors = [check['message'] for check in validation_dict['detailed_results']
                         if check['status'] == 'FAILURE']
                metrics = {'validation_summary': validation_dict}

                # å¦‚æœæ˜¯PENDINGç‹€æ…‹ä½†æœ‰æ•¸æ“šï¼Œæ·»åŠ åŸºæœ¬æª¢æŸ¥
                if validation_status == 'PENDING' and len(loaded_data) > 0:
                    validation_result.add_success(
                        "data_loaded",
                        f"æˆåŠŸè¼‰å…¥ {len(loaded_data)} é¡†è¡›æ˜Ÿæ•¸æ“š",
                        {'satellite_count': len(loaded_data)}
                    )
                    validation_result.finalize()
                    is_valid = True
            else:
                # å¦‚æœæ˜¯å­—å…¸æ ¼å¼
                is_valid = validation_result.get('is_valid', False)
                errors = validation_result.get('errors', [])
                metrics = validation_result.get('metrics', {})

            if not is_valid:
                # è¿”å›é©—è­‰å¤±æ•—çš„ProcessingResult
                return create_processing_result(
                    status=ProcessingStatus.VALIDATION_FAILED,
                    data={
                        'stage': 'data_loading',
                        'tle_data': [],
                        'processing_stats': self.processing_stats,
                        'quality_metrics': {},
                        'next_stage_ready': False
                    },
                    errors=errors,
                    metadata={
                        'processing_start_time': start_time.isoformat(),
                        'processing_end_time': datetime.now(timezone.utc).isoformat(),
                        'status': 'VALIDATION_FAILED'
                    },
                    message="Stage 1æ•¸æ“šé©—è­‰å¤±æ•—"
                )

            # æ™‚é–“åŸºæº–æ¨™æº–åŒ–
            standardized_data = self._standardize_time_reference(loaded_data)

            # æ•¸æ“šå®Œæ•´æ€§æª¢æŸ¥
            completeness_check = self._check_data_completeness(standardized_data)

            # æ§‹å»ºè¼¸å‡ºçµæœ
            processing_time = datetime.now(timezone.utc) - start_time
            result_data = {
                'stage': 'data_loading',
                'tle_data': standardized_data,
                'processing_stats': self.processing_stats,
                'quality_metrics': metrics,
                'next_stage_ready': True
            }

            metadata = {
                'processing_start_time': start_time.isoformat(),
                'processing_end_time': datetime.now(timezone.utc).isoformat(),
                'processing_duration_seconds': processing_time.total_seconds(),
                'total_satellites_loaded': len(standardized_data),
                'time_reference_standard': 'tle_epoch',
                'validation_passed': True,
                'completeness_score': completeness_check['score']
            }

            self.logger.info(f"âœ… Stage 1æ•¸æ“šè¼‰å…¥å®Œæˆï¼Œè¼‰å…¥{len(standardized_data)}é¡†è¡›æ˜Ÿæ•¸æ“š")

            # è¿”å›ProcessingResultç‰©ä»¶
            return create_processing_result(
                status=ProcessingStatus.SUCCESS,
                data=result_data,
                metadata=metadata,
                message=f"Stage 1æ•¸æ“šè¼‰å…¥æˆåŠŸï¼Œè™•ç†{len(standardized_data)}é¡†è¡›æ˜Ÿ"
            )

        except Exception as e:
            self.logger.error(f"âŒ Stage 1æ•¸æ“šè¼‰å…¥å¤±æ•—: {e}")
            # è¿”å›éŒ¯èª¤ç‹€æ…‹çš„ProcessingResult
            return create_processing_result(
                status=ProcessingStatus.FAILED,
                data={
                    'stage': 'data_loading',
                    'tle_data': [],
                    'processing_stats': self.processing_stats,
                    'quality_metrics': {},
                    'next_stage_ready': False
                },
                errors=[str(e)],
                metadata={
                    'processing_start_time': start_time.isoformat(),
                    'processing_end_time': datetime.now(timezone.utc).isoformat(),
                    'error': str(e),
                    'status': 'ERROR'
                },
                message=f"Stage 1æ•¸æ“šè¼‰å…¥å¤±æ•—: {e}"
            )

    def _process_input_tle_data(self, tle_data_list: List[Dict]) -> List[Dict]:
        """è™•ç†è¼¸å…¥çš„TLEæ•¸æ“š"""
        if not tle_data_list:
            self.logger.warning("è¼¸å…¥çš„TLEæ•¸æ“šç‚ºç©º")
            return []

        processed_data = []
        for tle_data in tle_data_list:
            # åŸºæœ¬æ ¼å¼æª¢æŸ¥
            if self._validate_tle_format(tle_data):
                processed_data.append(tle_data)
            else:
                self.processing_stats['validation_failures'] += 1

        self.processing_stats['total_satellites_loaded'] = len(processed_data)
        return processed_data

    def _load_tle_data_from_files(self) -> List[Dict]:
        """å¾æ–‡ä»¶è¼‰å…¥TLEæ•¸æ“š"""
        try:
            # æƒæTLEæ–‡ä»¶
            tle_files = self.tle_loader.scan_tle_data()
            self.processing_stats['total_files_scanned'] = len(tle_files)

            if not tle_files:
                self.logger.warning("æœªæ‰¾åˆ°TLEæ–‡ä»¶")
                return []

            # è¼‰å…¥æ‰€æœ‰TLEæ•¸æ“š
            all_tle_data = []
            for tle_file in tle_files:
                tle_data = self.tle_loader.load_satellite_data(tle_files)
                if tle_data:
                    all_tle_data.extend(tle_data)
                break  # load_satellite_data handles all files at once

            # æ¨£æœ¬æ¨¡å¼è™•ç†
            if self.sample_mode and len(all_tle_data) > self.sample_size:
                self.logger.info(f"æ¨£æœ¬æ¨¡å¼ï¼šå¾{len(all_tle_data)}é¡†è¡›æ˜Ÿä¸­é¸å–{self.sample_size}é¡†")
                all_tle_data = all_tle_data[:self.sample_size]

            self.processing_stats['total_satellites_loaded'] = len(all_tle_data)
            return all_tle_data

        except Exception as e:
            self.logger.error(f"è¼‰å…¥TLEæ–‡ä»¶å¤±æ•—: {e}")
            raise

    def _validate_tle_format(self, tle_data: Dict) -> bool:
        """é©—è­‰TLEæ•¸æ“šæ ¼å¼"""
        required_fields = ['satellite_id', 'line1', 'line2', 'name']

        for field in required_fields:
            if field not in tle_data:
                self.logger.warning(f"TLEæ•¸æ“šç¼ºå°‘å¿…è¦å­—æ®µ: {field}")
                return False

        # æª¢æŸ¥TLEè¡Œæ ¼å¼
        line1 = tle_data['line1']
        line2 = tle_data['line2']

        if len(line1) != 69 or len(line2) != 69:
            self.logger.warning("TLEè¡Œé•·åº¦ä¸æ­£ç¢º")
            return False

        if line1[0] != '1' or line2[0] != '2':
            self.logger.warning("TLEè¡Œæ¨™è­˜ç¬¦ä¸æ­£ç¢º")
            return False

        return True

    def _validate_loaded_data(self, loaded_data: List[Dict]) -> Dict[str, Any]:
        """ä½¿ç”¨v2.0æ¨¡çµ„åŒ–æ•¸æ“šé©—è­‰å™¨é©—è­‰è¼‰å…¥çš„æ•¸æ“š"""
        try:
            # ä½¿ç”¨æ–°çš„DataValidatorçµ„ä»¶é€²è¡Œå­¸è¡“ç´šé©—è­‰
            validation_result = self.data_validator.validate_tle_dataset(loaded_data)

            # æ›´æ–°çµ±è¨ˆä¿¡æ¯
            if not validation_result['is_valid']:
                self.processing_stats['validation_failures'] += len(validation_result['validation_details']['errors'])

            # è½‰æ›ç‚ºå…¼å®¹æ ¼å¼
            if validation_result['is_valid']:
                return {
                    'is_valid': True,
                    'errors': [],
                    'metrics': {
                        'validation_summary': validation_result,
                        'academic_grade': validation_result['overall_grade'],
                        'quality_metrics': validation_result['quality_metrics']
                    }
                }
            else:
                return {
                    'is_valid': False,
                    'errors': validation_result['validation_details']['errors'],
                    'metrics': {
                        'validation_summary': validation_result,
                        'academic_grade': validation_result['overall_grade']
                    }
                }

        except Exception as e:
            self.logger.error(f"æ•¸æ“šé©—è­‰å¤±æ•—: {e}")
            return {
                'is_valid': False,
                'errors': [str(e)],
                'metrics': {}
            }

    def _perform_tle_specific_validation(self, data: List[Dict]) -> Dict[str, Any]:
        """åŸ·è¡ŒTLEç‰¹å®šçš„é©—è­‰æª¢æŸ¥
        ğŸ“ Grade Aå­¸è¡“æ¨™æº–ï¼šåŸºæ–¼TLEæ•¸æ“šå…§åœ¨ç‰¹æ€§è©•ä¼°ï¼Œç¦æ­¢ä½¿ç”¨ç•¶å‰æ™‚é–“ä½œç‚ºè©•ä¼°åŸºæº–
        """
        metrics = {
            'unique_satellites': 0,
            'epoch_time_range_days': 0,
            'constellation_coverage': 0,
            'temporal_consistency_score': 0,  # æ”¹ç‚ºæ™‚é–“ä¸€è‡´æ€§è©•åˆ†
            'data_quality_grade': 'N/A'      # æ”¹ç‚ºå­¸è¡“å“è³ªç­‰ç´š
        }

        if not data:
            return metrics

        # æª¢æŸ¥è¡›æ˜ŸIDå”¯ä¸€æ€§
        satellite_ids = set()
        epochs = []
        constellations = set()

        for tle in data:
            satellite_ids.add(tle['satellite_id'])

            # è§£æTLEæ™‚é–“
            try:
                line1 = tle['line1']
                epoch_year = int(line1[18:20])
                epoch_day = float(line1[20:32])

                # è½‰æ›ç‚ºå®Œæ•´å¹´ä»½
                if epoch_year < 57:
                    full_year = 2000 + epoch_year
                else:
                    full_year = 1900 + epoch_year

                epoch_time = TimeUtils.parse_tle_epoch(full_year, epoch_day)
                epochs.append(epoch_time)

            except Exception as e:
                self.logger.warning(f"TLEæ™‚é–“è§£æå¤±æ•—: {e}")
                self.processing_stats['time_reference_issues'] += 1

        metrics['unique_satellites'] = len(satellite_ids)

        if epochs:
            time_range = max(epochs) - min(epochs)
            metrics['epoch_time_range_days'] = time_range.days

            # ğŸ“ å­¸è¡“æ¨™æº–åˆè¦ï¼šåŸºæ–¼æ•¸æ“šå…§åœ¨æ™‚é–“åˆ†ä½ˆè©•ä¼°å“è³ª
            metrics['temporal_consistency_score'] = self._calculate_temporal_consistency(epochs)
            metrics['data_quality_grade'] = self._assess_academic_data_quality(data, epochs)

        return metrics

    def _calculate_temporal_consistency(self, epochs: List[datetime]) -> float:
        """
        è¨ˆç®—TLEæ•¸æ“šçš„æ™‚é–“ä¸€è‡´æ€§è©•åˆ†
        ğŸ“ Grade Aå­¸è¡“æ¨™æº–ï¼šåŸºæ–¼æ•¸æ“šå…§åœ¨æ™‚é–“åˆ†ä½ˆç‰¹æ€§ï¼Œä¸ä¾è³´ç•¶å‰æ™‚é–“
        
        Args:
            epochs: TLE epochæ™‚é–“åˆ—è¡¨
            
        Returns:
            æ™‚é–“ä¸€è‡´æ€§è©•åˆ† (0-100)
        """
        if len(epochs) < 2:
            return 100.0  # å–®ä¸€æ•¸æ“šé»è¦–ç‚ºå®Œå…¨ä¸€è‡´
        
        epochs = sorted(epochs)
        time_gaps = []
        
        # è¨ˆç®—ç›¸é„°epochä¹‹é–“çš„æ™‚é–“é–“éš”
        for i in range(len(epochs) - 1):
            gap_hours = (epochs[i + 1] - epochs[i]).total_seconds() / 3600
            time_gaps.append(gap_hours)
        
        if not time_gaps:
            return 100.0
        
        # è¨ˆç®—æ™‚é–“é–“éš”çš„æ¨™æº–å·®
        mean_gap = sum(time_gaps) / len(time_gaps)
        variance = sum((gap - mean_gap) ** 2 for gap in time_gaps) / len(time_gaps)
        std_deviation = variance ** 0.5
        
        # å°‡æ¨™æº–å·®è½‰æ›ç‚ºä¸€è‡´æ€§è©•åˆ† (æ¨™æº–å·®è¶Šå°ï¼Œä¸€è‡´æ€§è¶Šé«˜)
        # å‡è¨­24å°æ™‚é–“éš”çš„æ¨™æº–å·®ç‚ºacceptable baseline
        if mean_gap == 0:
            return 100.0
        
        consistency_ratio = 1 - min(std_deviation / mean_gap, 1.0)
        return max(0, consistency_ratio * 100)

    def _assess_academic_data_quality(self, tle_data: List[Dict], epochs: List[datetime]) -> str:
        """
        è©•ä¼°TLEæ•¸æ“šçš„å­¸è¡“å“è³ªç­‰ç´š
        ğŸ“ Grade Aå­¸è¡“æ¨™æº–ï¼šåŸºæ–¼æ•¸æ“šå®Œæ•´æ€§ã€åƒæ•¸ç²¾åº¦ã€æ™‚é–“ä¸€è‡´æ€§
        
        Args:
            tle_data: TLEæ•¸æ“šåˆ—è¡¨
            epochs: epochæ™‚é–“åˆ—è¡¨
            
        Returns:
            å­¸è¡“å“è³ªç­‰ç´š (A+, A, A-, B+, B, B-, C)
        """
        try:
            # 1. æ•¸æ“šå®Œæ•´æ€§æª¢æŸ¥
            completeness_score = self._assess_data_completeness(tle_data)
            
            # 2. è»Œé“åƒæ•¸ç²¾åº¦æª¢æŸ¥
            parameter_precision = self._assess_orbital_parameter_precision(tle_data)
            
            # 3. æ™‚é–“ä¸€è‡´æ€§æª¢æŸ¥
            temporal_consistency = self._calculate_temporal_consistency(epochs)
            
            # 4. æ•¸æ“šé›†å¤§å°é©ç•¶æ€§
            dataset_adequacy = min(100, len(tle_data) / 100 * 100)  # 100å€‹è¡›æ˜Ÿç‚ºåŸºæº–
            
            # ç¶œåˆè©•åˆ† (å„é …æ¬Šé‡)
            overall_score = (
                completeness_score * 0.3 +
                parameter_precision * 0.3 +
                temporal_consistency * 0.2 +
                dataset_adequacy * 0.2
            )
            
            # æ ¹æ“šç¶œåˆè©•åˆ†åˆ†é…ç­‰ç´š
            if overall_score >= 95:
                return 'A+'
            elif overall_score >= 90:
                return 'A'
            elif overall_score >= 85:
                return 'A-'
            elif overall_score >= 80:
                return 'B+'
            elif overall_score >= 70:
                return 'B'
            elif overall_score >= 60:
                return 'B-'
            else:
                return 'C'
                
        except Exception as e:
            self.logger.warning(f"å­¸è¡“å“è³ªè©•ä¼°å¤±æ•—: {e}")
            return 'C'

    def _assess_data_completeness(self, tle_data: List[Dict]) -> float:
        """è©•ä¼°TLEæ•¸æ“šå®Œæ•´æ€§"""
        if not tle_data:
            return 0.0
        
        required_fields = ['satellite_id', 'line1', 'line2', 'name']
        complete_entries = 0
        
        for tle in tle_data:
            if all(field in tle and tle[field] for field in required_fields):
                # æª¢æŸ¥TLEæ ¼å¼æ­£ç¢ºæ€§
                try:
                    line1, line2 = tle['line1'], tle['line2']
                    if len(line1) == 69 and len(line2) == 69:
                        # æª¢æŸ¥checksum
                        if self._validate_tle_checksum(line1, line2):
                            complete_entries += 1
                except:
                    continue
        
        return (complete_entries / len(tle_data)) * 100

    def _assess_orbital_parameter_precision(self, tle_data: List[Dict]) -> float:
        """è©•ä¼°è»Œé“åƒæ•¸ç²¾åº¦"""
        precision_scores = []
        
        for tle in tle_data:
            try:
                line2 = tle['line2']
                
                # æª¢æŸ¥è»Œé“åƒæ•¸çš„åˆç†æ€§
                inclination = float(line2[8:16])
                eccentricity = float('0.' + line2[26:33])
                mean_motion = float(line2[52:63])
                
                param_score = 100.0
                
                # å‚¾è§’æª¢æŸ¥ (0-180åº¦)
                if not (0 <= inclination <= 180):
                    param_score -= 25
                
                # åå¿ƒç‡æª¢æŸ¥ (0-1)
                if not (0 <= eccentricity < 1):
                    param_score -= 25
                
                # å¹³å‡é‹å‹•æª¢æŸ¥ (åˆç†ç¯„åœ: 0.5-20 revs/day)
                if not (0.5 <= mean_motion <= 20):
                    param_score -= 25
                
                # æª¢æŸ¥æ•¸æ“šç²¾åº¦ (å°æ•¸ä½æ•¸)
                if '.' in line2[52:63]:
                    decimal_places = len(line2[52:63].split('.')[1])
                    if decimal_places < 8:  # æœŸæœ›è‡³å°‘8ä½å°æ•¸
                        param_score -= 25
                
                precision_scores.append(max(0, param_score))
                
            except Exception:
                precision_scores.append(0)
        
        return sum(precision_scores) / len(precision_scores) if precision_scores else 0

    def _validate_tle_checksum(self, line1: str, line2: str) -> bool:
        """é©—è­‰TLE checksum"""
        try:
            for line in [line1, line2]:
                checksum = 0
                for char in line[:-1]:  # æ’é™¤æœ€å¾Œçš„checksumä½
                    if char.isdigit():
                        checksum += int(char)
                    elif char == '-':
                        checksum += 1
                
                expected_checksum = int(line[-1])
                if (checksum % 10) != expected_checksum:
                    return False
            return True
        except:
            return False

    def _standardize_time_reference(self, loaded_data: List[Dict]) -> List[Dict]:
        """ä½¿ç”¨v2.0æ¨¡çµ„åŒ–æ™‚é–“åŸºæº–ç®¡ç†å™¨æ¨™æº–åŒ–æ™‚é–“åŸºæº–"""
        try:
            # ä½¿ç”¨æ–°çš„TimeReferenceManagerçµ„ä»¶å»ºç«‹æ™‚é–“åŸºæº–
            time_reference_result = self.time_reference_manager.establish_time_reference(loaded_data)

            if time_reference_result['time_reference_established']:
                standardized_data = time_reference_result['standardized_data']
                
                # æ›´æ–°çµ±è¨ˆä¿¡æ¯
                self.processing_stats['time_reference_issues'] = self.time_reference_manager.get_time_statistics()['parsing_errors']
                
                self.logger.info(f"âœ… æ™‚é–“åŸºæº–æ¨™æº–åŒ–å®Œæˆï¼Œè™•ç†{len(standardized_data)}ç­†è¨˜éŒ„")
                return standardized_data
            else:
                self.logger.warning("âš ï¸ æ™‚é–“åŸºæº–å»ºç«‹å¤±æ•—ï¼Œä½¿ç”¨å›é€€æ–¹æ¡ˆ")
                # å›é€€åˆ°åŸæœ‰çš„æ™‚é–“è™•ç†é‚è¼¯
                return self._fallback_time_standardization(loaded_data)

        except Exception as e:
            self.logger.error(f"æ™‚é–“åŸºæº–æ¨™æº–åŒ–å¤±æ•—: {e}")
            # å›é€€æ–¹æ¡ˆ
            return self._fallback_time_standardization(loaded_data)

    def _fallback_time_standardization(self, loaded_data: List[Dict]) -> List[Dict]:
        """å›é€€æ™‚é–“æ¨™æº–åŒ–æ–¹æ¡ˆï¼ˆä¿æŒå‘å¾Œå…¼å®¹ï¼‰"""
        standardized_data = []

        for tle_data in loaded_data:
            try:
                # è§£æTLE epochæ™‚é–“
                line1 = tle_data['line1']
                epoch_year = int(line1[18:20])
                epoch_day = float(line1[20:32])

                # è½‰æ›ç‚ºå®Œæ•´å¹´ä»½
                if epoch_year < 57:
                    full_year = 2000 + epoch_year
                else:
                    full_year = 1900 + epoch_year

                # æ¨™æº–åŒ–æ™‚é–“ä¿¡æ¯
                epoch_time = TimeUtils.parse_tle_epoch(full_year, epoch_day)

                # æ·»åŠ æ¨™æº–åŒ–æ™‚é–“å­—æ®µ
                enhanced_tle = tle_data.copy()
                enhanced_tle.update({
                    'epoch_datetime': epoch_time.isoformat(),
                    'epoch_year_full': full_year,
                    'epoch_day_decimal': epoch_day,
                    'time_reference_standard': 'tle_epoch_fallback'
                })

                standardized_data.append(enhanced_tle)

            except Exception as e:
                self.logger.error(f"æ™‚é–“æ¨™æº–åŒ–å¤±æ•— {tle_data.get('satellite_id', 'unknown')}: {e}")
                # ä¿ç•™åŸæ•¸æ“šä½†æ¨™è¨˜å•é¡Œ
                enhanced_tle = tle_data.copy()
                enhanced_tle['time_reference_error'] = str(e)
                standardized_data.append(enhanced_tle)
                self.processing_stats['time_reference_issues'] += 1

        return standardized_data

    def _check_data_completeness(self, data: List[Dict]) -> Dict[str, Any]:
        """æª¢æŸ¥æ•¸æ“šå®Œæ•´æ€§"""
        if not data:
            return {'score': 0, 'issues': ['ç„¡æ•¸æ“š']}

        total_satellites = len(data)
        complete_records = 0
        issues = []

        for tle in data:
            completeness_checks = [
                'satellite_id' in tle and tle['satellite_id'],
                'name' in tle and tle['name'],
                'line1' in tle and len(tle['line1']) == 69,
                'line2' in tle and len(tle['line2']) == 69,
                'epoch_datetime' in tle,
                'time_reference_error' not in tle
            ]

            if all(completeness_checks):
                complete_records += 1
            else:
                missing_fields = []
                if not completeness_checks[0]:
                    missing_fields.append('satellite_id')
                if not completeness_checks[1]:
                    missing_fields.append('name')
                if not completeness_checks[2]:
                    missing_fields.append('line1_format')
                if not completeness_checks[3]:
                    missing_fields.append('line2_format')
                if not completeness_checks[4]:
                    missing_fields.append('epoch_time')
                if not completeness_checks[5]:
                    missing_fields.append('time_parsing')

                if missing_fields:
                    issues.append(f"è¡›æ˜Ÿ {tle.get('satellite_id', 'unknown')}: {', '.join(missing_fields)}")

        completeness_score = (complete_records / total_satellites) * 100

        return {
            'score': completeness_score,
            'complete_records': complete_records,
            'total_records': total_satellites,
            'issues': issues[:10]  # é™åˆ¶å ±å‘Šå‰10å€‹å•é¡Œ
        }

    def validate_input(self, input_data: Any) -> Dict[str, Any]:
        """é©—è­‰è¼¸å…¥æ•¸æ“š"""
        errors = []
        warnings = []

        if input_data is None:
            # å…è¨±ç„¡è¼¸å…¥ï¼Œå°‡å¾æ–‡ä»¶è¼‰å…¥
            return {'valid': True, 'errors': errors, 'warnings': warnings}

        if isinstance(input_data, dict):
            if 'tle_data' in input_data:
                tle_data = input_data['tle_data']
                if not isinstance(tle_data, list):
                    errors.append("tle_dataå¿…é ˆæ˜¯åˆ—è¡¨æ ¼å¼")
                elif len(tle_data) == 0:
                    warnings.append("tle_dataç‚ºç©ºåˆ—è¡¨")
            return {'valid': len(errors) == 0, 'errors': errors, 'warnings': warnings}

        errors.append("è¼¸å…¥æ•¸æ“šæ ¼å¼ä¸æ­£ç¢º")
        return {'valid': False, 'errors': errors, 'warnings': warnings}

    def validate_output(self, output_data: Any) -> Dict[str, Any]:
        """é©—è­‰è¼¸å‡ºæ•¸æ“š"""
        errors = []
        warnings = []

        if not isinstance(output_data, dict):
            errors.append("è¼¸å‡ºæ•¸æ“šå¿…é ˆæ˜¯å­—å…¸æ ¼å¼")
            return {'valid': False, 'errors': errors, 'warnings': warnings}

        required_fields = ['stage', 'tle_data', 'metadata']
        for field in required_fields:
            if field not in output_data:
                errors.append(f"ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}")

        if output_data.get('stage') != 'data_loading':
            errors.append("éšæ®µæ¨™è­˜éŒ¯èª¤")

        # æª¢æŸ¥ TLE æ•¸æ“š
        tle_data = output_data.get('tle_data', {})
        if not isinstance(tle_data, list):
            errors.append("TLEæ•¸æ“šæ ¼å¼éŒ¯èª¤")
        elif len(tle_data) == 0:
            warnings.append("TLEæ•¸æ“šç‚ºç©º")

        return {
            'valid': len(errors) == 0,
            'errors': errors,
            'warnings': warnings
        }

    def extract_key_metrics(self, results: Dict[str, Any] = None) -> Dict[str, Any]:
        """æå–é—œéµæŒ‡æ¨™"""
        return {
            'stage': 'data_loading',
            'satellites_loaded': self.processing_stats['total_satellites_loaded'],
            'files_scanned': self.processing_stats['total_files_scanned'],
            'validation_failures': self.processing_stats['validation_failures'],
            'time_reference_issues': self.processing_stats['time_reference_issues'],
            'success_rate': (
                (self.processing_stats['total_satellites_loaded'] - self.processing_stats['validation_failures'])
                / max(1, self.processing_stats['total_satellites_loaded'])
            ) * 100
        }

    def run_validation_checks(self, data: Any) -> Dict[str, Any]:
        """åŸ·è¡ŒStage 1é©—è­‰æª¢æŸ¥
        
        ğŸ”§ ä¿®å¾©ï¼šèª¿æ•´æ•¸æ“šè¼‰å…¥éšæ®µçš„é©—è­‰é‚è¼¯
        - ä¸»è¦é—œæ³¨TLEæ•¸æ“šè¼‰å…¥çš„æˆåŠŸæ€§
        - æ™‚é–“åˆè¦æ€§ä½œç‚ºè­¦å‘Šè€ŒééŒ¯èª¤
        - ç¬¦åˆæ–°çš„æ•¸æ“šè¼‰å…¥å±¤æ¶æ§‹
        """
        try:
            # ä½¿ç”¨DataValidatoré€²è¡Œå®Œæ•´é©—è­‰
            if isinstance(data, dict) and 'tle_data' in data:
                tle_data_list = data['tle_data']
            elif isinstance(data, list):
                tle_data_list = data
            else:
                return {
                    'validation_status': 'failed',
                    'overall_status': 'FAIL',
                    'checks_performed': ['input_format_check'],
                    'stage_compliance': False,
                    'academic_standards': False,
                    'error_message': 'è¼¸å…¥æ•¸æ“šæ ¼å¼ä¸æ­£ç¢º',
                    'timestamp': datetime.now(timezone.utc).isoformat()
                }

            # åŸ·è¡Œæ•¸æ“šé©—è­‰
            validation_result = self.data_validator.validate_tle_dataset(tle_data_list)
            
            # åŸ·è¡Œæ™‚é–“åŸºæº–é©—è­‰
            time_reference_result = self.time_reference_manager.establish_time_reference(tle_data_list)
            time_compliance = self.time_reference_manager.validate_time_compliance(time_reference_result)

            # ğŸ”§ ä¿®å¾©ï¼šæ•¸æ“šè¼‰å…¥éšæ®µçš„é©—è­‰æ¨™æº–
            # ä¸»è¦æ¨™æº–ï¼šTLEæ•¸æ“šè¼‰å…¥æˆåŠŸ
            data_loading_successful = validation_result['is_valid'] and len(tle_data_list) > 0
            
            # å­¸è¡“æ¨™æº–ï¼šåŸºæ–¼æ•¸æ“šé©—è­‰çµæœ
            academic_standards_met = validation_result['overall_grade'] in ['A+', 'A', 'A-', 'B+', 'B']
            
            # ğŸš¨ é—œéµä¿®å¾©ï¼šoverall_statusé‚è¼¯èª¿æ•´
            # å°æ–¼æ•¸æ“šè¼‰å…¥éšæ®µï¼Œæ™‚é–“åˆè¦æ€§ä¸æ‡‰è©²æ˜¯å¤±æ•—æ¢ä»¶ï¼Œè€Œæ˜¯å“è³ªæŒ‡æ¨™
            overall_status = 'PASS' if data_loading_successful else 'FAIL'
            
            # æ§‹å»ºé©—è­‰å ±å‘Š
            validation_checks = {
                'validation_status': 'passed' if data_loading_successful else 'failed',
                'overall_status': overall_status,
                'checks_performed': [
                    'tle_format_validation',
                    'academic_grade_a_compliance', 
                    'time_reference_establishment',
                    'data_quality_assessment'
                ],
                'stage_compliance': data_loading_successful,
                'academic_standards': academic_standards_met,
                'detailed_results': {
                    'data_validation': validation_result,
                    'time_compliance': time_compliance,
                    'academic_grade': validation_result['overall_grade'],
                    'time_quality_grade': time_compliance.get('compliance_grade', 'C'),
                    'data_loading_metrics': {
                        'satellites_loaded': len(tle_data_list),
                        'loading_successful': data_loading_successful,
                        'time_reference_established': time_reference_result.get('time_reference_established', False)
                    }
                },
                'timestamp': datetime.now(timezone.utc).isoformat()
            }

            # æ·»åŠ æ™‚é–“åˆè¦æ€§è­¦å‘Šï¼ˆå¦‚æœéœ€è¦ï¼‰
            if not time_compliance.get('compliant', True):
                validation_checks['warnings'] = [
                    f"æ™‚é–“åˆè¦æ€§è©•ç´š: {time_compliance.get('compliance_grade', 'C')} - é€™ä¸å½±éŸ¿æ•¸æ“šè¼‰å…¥çš„æˆåŠŸæ€§"
                ]

            return validation_checks

        except Exception as e:
            self.logger.error(f"é©—è­‰æª¢æŸ¥å¤±æ•—: {e}")
            return {
                'validation_status': 'error',
                'overall_status': 'ERROR',
                'checks_performed': ['error_handling'],
                'stage_compliance': False,
                'academic_standards': False,
                'error_message': str(e),
                'timestamp': datetime.now(timezone.utc).isoformat()
            }

    def save_results(self, results: Dict[str, Any]) -> str:
        """ä¿å­˜Stage 1è™•ç†çµæœ"""
        try:
            # ä½¿ç”¨æ–°çš„æª”æ¡ˆå‘½åè¦ç¯„ï¼ˆç§»é™¤stageå‰ç¶´ï¼‰
            timestamp = datetime.now(timezone.utc).strftime('%Y%m%d_%H%M%S')
            output_filename = f"data_loading_output_{timestamp}.json"
            output_path = self.output_dir / output_filename

            # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
            self.output_dir.mkdir(parents=True, exist_ok=True)

            # ä¿å­˜çµæœ
            import json
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False, default=str)

            self.logger.info(f"âœ… Stage 1çµæœå·²ä¿å­˜è‡³: {output_path}")
            return str(output_path)

        except Exception as e:
            self.logger.error(f"ä¿å­˜çµæœå¤±æ•—: {e}")
            raise RuntimeError(f"Stage 1çµæœä¿å­˜å¤±æ•—: {e}")


def create_stage1_processor(config: Optional[Dict[str, Any]] = None) -> Stage1DataLoadingProcessor:
    """
    å‰µå»ºStage 1æ•¸æ“šè¼‰å…¥è™•ç†å™¨å¯¦ä¾‹

    Args:
        config: å¯é¸é…ç½®åƒæ•¸

    Returns:
        Stage 1è™•ç†å™¨å¯¦ä¾‹
    """
    return Stage1DataLoadingProcessor(config)