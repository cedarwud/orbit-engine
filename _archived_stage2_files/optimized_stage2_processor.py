#!/usr/bin/env python3
"""
å„ªåŒ–ç‰ˆéšæ®µäºŒè™•ç†å™¨ - æ•´åˆä¸¦è¡Œè¨ˆç®—å’ŒGPUåŠ é€Ÿ
ç›®æ¨™: 467ç§’ â†’ 60-90ç§’ (5-8å€æ€§èƒ½æå‡)

æ ¸å¿ƒå„ªåŒ–ç­–ç•¥:
1. ä¸¦è¡ŒSGP4è¨ˆç®— (GPU + CPUå¤šé€²ç¨‹)
2. ä¸¦è¡Œåº§æ¨™è½‰æ› (GPUåŠ é€Ÿ)
3. æ™ºèƒ½æ‰¹æ¬¡è™•ç†
4. è¨˜æ†¶é«”å„ªåŒ–
"""

import os
import sys
import time
import logging
from typing import Dict, List, Any, Optional
from datetime import datetime, timezone, timedelta
from pathlib import Path
import json

# å°å…¥åŸå§‹è™•ç†å™¨ä½œç‚ºåŸºç¤
from .stage2_orbital_computing_processor import Stage2OrbitalComputingProcessor

# å°å…¥å„ªåŒ–æ¨¡çµ„
from .parallel_sgp4_calculator import ParallelSGP4Calculator, ParallelConfig
from .sgp4_calculator import SGP4OrbitResult
from .gpu_coordinate_converter import GPUCoordinateConverter, check_gpu_availability

logger = logging.getLogger(__name__)
# å°å…¥è™•ç†çµæœå’Œç‹€æ…‹é¡å‹
try:
    from shared.interfaces.processor_interface import ProcessingResult, ProcessingStatus, create_processing_result
except ImportError:
    # å›é€€å°å…¥è·¯å¾‘
    import sys
    from pathlib import Path
    sys.path.append(str(Path(__file__).parent.parent.parent))
    from shared.interfaces.processor_interface import ProcessingResult, ProcessingStatus, create_processing_result

class OptimizedStage2Processor(Stage2OrbitalComputingProcessor):
    """
    å„ªåŒ–ç‰ˆéšæ®µäºŒè™•ç†å™¨

    åœ¨åŸå§‹è™•ç†å™¨åŸºç¤ä¸Šæ•´åˆ:
    - ä¸¦è¡ŒSGP4è¨ˆç®—
    - GPUåº§æ¨™è½‰æ›
    - æ™ºèƒ½æ‰¹æ¬¡è™•ç†
    - æ€§èƒ½ç›£æ§
    """

    def __init__(self, config_path: str = None, enable_optimization: bool = True):
        # åˆå§‹åŒ–åŸºç¤è™•ç†å™¨
        super().__init__(config_path)

        # ğŸš€ é‡æ–°å•Ÿç”¨ä¸¦è¡Œå„ªåŒ–ï¼ŒGPUç‰ˆæœ¬å•é¡Œå·²ä¿®å¾©
        self.enable_optimization = enable_optimization
        self.performance_metrics = {}

        # åˆå§‹åŒ–çµ±è¨ˆè³‡æ–™ï¼ˆç„¡è«–æ˜¯å¦å•Ÿç”¨å„ªåŒ–éƒ½è¦åˆå§‹åŒ–ï¼‰
        self.optimization_stats = {
            'sgp4_calculation_time': 0.0,
            'coordinate_conversion_time': 0.0,
            'visibility_analysis_time': 0.0,
            'total_satellites_processed': 0,
            'gpu_acceleration_used': False,
            'cpu_parallel_used': False
        }

        # æª¢æŸ¥GPUå¯ç”¨æ€§
        self.gpu_info = check_gpu_availability()
        self.logger.info(f"GPUç‹€æ…‹: {self.gpu_info}")

        # æ¸…ç†èˆŠçš„è¼¸å‡ºæª”æ¡ˆå’Œé©—è­‰å¿«ç…§
        self._cleanup_old_outputs()
        self._cleanup_validation_snapshots()

        # åˆå§‹åŒ–å„ªåŒ–çµ„ä»¶
        if self.enable_optimization:
            self._initialize_optimization_components()

        logger.info(f"ğŸš€ å„ªåŒ–ç‰ˆéšæ®µäºŒè™•ç†å™¨åˆå§‹åŒ–å®Œæˆ (å„ªåŒ–: {enable_optimization})")
        logger.info("ğŸ“Š æ•¸æ“šç²¾åº¦èªªæ˜: ä¿ç•™å®Œæ•´æ™‚é–“åºåˆ—æ•¸æ“šä»¥ç¢ºä¿è»Œé“é æ¸¬æº–ç¢ºæ€§")

    def _initialize_optimization_components(self):
        """åˆå§‹åŒ–å„ªåŒ–çµ„ä»¶"""
        try:
            # å„ªåŒ–çš„ä¸¦è¡ŒSGP4è¨ˆç®—å™¨é…ç½®
            parallel_config = ParallelConfig(
                enable_gpu=True,
                enable_multiprocessing=True,
                gpu_batch_size=5000,    # å¢åŠ æ‰¹æ¬¡å¤§å°æå‡ååé‡
                cpu_workers=min(16, max(8, os.cpu_count())),  # æœ€ä½³åŒ–å·¥ä½œé€²ç¨‹æ•¸
                memory_limit_gb=8.0     # å¢åŠ è¨˜æ†¶é«”é™åˆ¶
            )
            self.parallel_sgp4 = ParallelSGP4Calculator(parallel_config)

            # æ€§èƒ½åŸºç·šç›®æ¨™è¨­å®š (é”åˆ°æ–‡æª”æœ€ä½³é æœŸ)
            self.performance_targets = {
                'max_processing_time': 300,     # 5åˆ†é˜ç›®æ¨™ (å„ªæ–¼æ–‡æª”6åˆ†é˜)
                'min_throughput_per_sec': 30,   # æ¯ç§’è‡³å°‘30é¡†è¡›æ˜Ÿè™•ç†
                'max_memory_usage_gb': 1.5,     # è¨˜æ†¶é«”å„ªåŒ–åˆ°1.5GB
                'target_satellites': 8976,      # è™•ç†è¡›æ˜Ÿç¸½æ•¸
                'expected_feasible': 2200       # é æœŸå¯è¡Œè¡›æ˜Ÿæ•¸(å„ªåŒ–æå‡)
            }

            # GPUåº§æ¨™è½‰æ›å™¨
            enable_gpu = self.gpu_info.get('cupy_available', False)
            self.gpu_converter = GPUCoordinateConverter(
                observer_location=self.observer_location,
                enable_gpu=enable_gpu
            )

            # è¨˜éŒ„GPUä½¿ç”¨ç‹€æ…‹
            self.optimization_stats['gpu_acceleration_used'] = enable_gpu
            self.optimization_stats['cpu_parallel_used'] = True

            logger.info("âœ… å„ªåŒ–çµ„ä»¶åˆå§‹åŒ–æˆåŠŸ")
            logger.info(f"   GPUåº§æ¨™è½‰æ›: {'å•Ÿç”¨' if enable_gpu else 'ç¦ç”¨'}")

        except Exception as e:
            logger.warning(f"âš ï¸ å„ªåŒ–çµ„ä»¶åˆå§‹åŒ–å¤±æ•—ï¼Œå°‡ä½¿ç”¨æ¨™æº–è™•ç†: {e}")
            self.enable_optimization = False

    def _perform_modular_orbital_calculations(self, tle_data: List[Dict]) -> Dict[str, Any]:
        """
        é‡å¯«è»Œé“è¨ˆç®—æ–¹æ³•ï¼Œä½¿ç”¨ä¸¦è¡Œå„ªåŒ–ç‰ˆæœ¬
        """
        start_time = time.time()

        # ä¿å­˜TLEæ•¸æ“šä»¥ä¾›å¾ŒçºŒéšæ®µä½¿ç”¨ï¼ˆæ˜Ÿåº§è­˜åˆ¥ï¼‰
        self._current_tle_data = tle_data

        if self.enable_optimization and hasattr(self, 'parallel_sgp4'):
            logger.info("ğŸš€ ä½¿ç”¨ä¸¦è¡Œå„ªåŒ–SGP4è¨ˆç®—...")

            # æº–å‚™æ™‚é–“åºåˆ—
            time_series = self._prepare_time_series_for_parallel(tle_data)

            # ä¸¦è¡Œè¨ˆç®—
            orbital_results = self.parallel_sgp4.batch_calculate_parallel(tle_data, time_series)

            # ğŸ”§ ä¿®å¾©ï¼šæ›´æ–°SGP4è¨ˆç®—çµ±è¨ˆï¼Œè§£æ±ºé©—è­‰å¤±æ•—å•é¡Œ
            total_calculations = len(tle_data) * len(time_series)
            successful_calculations = sum(1 for result in orbital_results.values()
                                        if result and len(result.get('positions', [])) > 0)
            failed_calculations = total_calculations - successful_calculations

            # æ›´æ–°sgp4_calculatorçš„çµ±è¨ˆä¿¡æ¯
            self.sgp4_calculator.calculation_stats.update({
                'total_calculations': total_calculations,
                'successful_calculations': successful_calculations,
                'failed_calculations': failed_calculations
            })

            logger.info(f"ğŸ“Š SGP4ä¸¦è¡Œè¨ˆç®—çµ±è¨ˆæ›´æ–°: {successful_calculations}/{total_calculations} æˆåŠŸ")

            # ğŸ”§ ä¿®å¾©ï¼šæ›´æ–°processing_statsï¼Œè§£æ±º"è™•ç†0é¡†è¡›æ˜Ÿ"çš„å•é¡Œ
            self.processing_stats.update({
                'total_satellites_processed': len(tle_data),
                'successful_calculations': successful_calculations,
                'failed_calculations': failed_calculations
            })

            # æ›´æ–°çµ±è¨ˆ
            self.optimization_stats['sgp4_calculation_time'] = time.time() - start_time
            self.optimization_stats['gpu_acceleration_used'] = self.parallel_sgp4.gpu_available
            self.optimization_stats['cpu_parallel_used'] = True

        else:
            logger.info("ğŸ“Š ä½¿ç”¨æ¨™æº–SGP4è¨ˆç®—...")
            orbital_results = super()._perform_modular_orbital_calculations(tle_data)
            self.optimization_stats['sgp4_calculation_time'] = time.time() - start_time

        logger.info(f"â±ï¸ SGP4è¨ˆç®—è€—æ™‚: {self.optimization_stats['sgp4_calculation_time']:.2f}ç§’")
        return orbital_results

    def _prepare_time_series_for_parallel(self, tle_data: List[Dict]) -> List[datetime]:
        """ç‚ºä¸¦è¡Œè¨ˆç®—æº–å‚™æ™‚é–“åºåˆ—"""
        # ä½¿ç”¨ç¬¬ä¸€é¡†è¡›æ˜Ÿçš„é…ç½®ä¾†æ±ºå®šæ™‚é–“åºåˆ—
        if not tle_data:
            return []

        sample_satellite = tle_data[0]
        satellite_id = sample_satellite.get('satellite_id', 'unknown')
        constellation = self._determine_satellite_constellation(satellite_id, sample_satellite)

        # ç²å–æ™‚é–“åºåˆ— (å¾©ç”¨åŸå§‹é‚è¼¯) - é€™æœƒè¿”å›åˆ†é˜æ•¸çš„åˆ—è¡¨
        time_minutes_series = self._get_constellation_time_series(constellation, sample_satellite)

        # è½‰æ›ç‚ºdatetimeå°è±¡åˆ—è¡¨
        base_time_str = self._get_calculation_base_time([sample_satellite])
        base_time = datetime.fromisoformat(base_time_str.replace('Z', '+00:00'))
        datetime_series = []

        # ç¢ºä¿ time_minutes_series æ˜¯åˆ—è¡¨è€Œä¸æ˜¯å…¶ä»–é¡å‹
        if isinstance(time_minutes_series, (list, tuple)):
            for minutes in time_minutes_series:
                time_offset = timedelta(minutes=float(minutes))
                datetime_point = base_time + time_offset
                datetime_series.append(datetime_point)
        else:
            logger.error(f"âŒ time_minutes_series é¡å‹éŒ¯èª¤: {type(time_minutes_series)}")
            return []

        logger.info(f"âœ… æº–å‚™äº† {len(datetime_series)} å€‹æ™‚é–“é»ç”¨æ–¼ä¸¦è¡Œè¨ˆç®—")
        return datetime_series

    def _perform_coordinate_conversions(self, orbital_results: Dict[str, Any]) -> Dict[str, Any]:
        """
        é‡å†™åæ ‡è½¬æ¢æ–¹æ³•ï¼Œæ­£ç¡®å¤„ç†å­—å…¸æ ¼å¼çš„å¹¶è¡Œè®¡ç®—ç»“æœ
        æ”¯æŒå¤§æ•¸æ“šé›†åˆ†æ‰¹è™•ç†ä»¥é¿å…è¨˜æ†¶é«”å•é¡Œ
        """
        start_time = time.time()

        # ğŸ” èª¿è©¦ï¼šæª¢æŸ¥è¼¸å…¥æ•¸æ“š
        logger.info(f"ğŸ” åº§æ¨™è½‰æ›è¼¸å…¥æ•¸æ“šæª¢æŸ¥: {len(orbital_results)}é¡†è¡›æ˜Ÿ")
        if orbital_results:
            first_sat_id = list(orbital_results.keys())[0]
            first_sat_data = orbital_results[first_sat_id]
            logger.info(f"ğŸ” ç¤ºä¾‹è¡›æ˜Ÿ {first_sat_id} æ•¸æ“šéµ: {list(first_sat_data.keys())}")
            logger.info(f"ğŸ” ç¤ºä¾‹è¡›æ˜Ÿæ˜¯å¦æœ‰sgp4_positions: {'sgp4_positions' in first_sat_data}")
            if 'sgp4_positions' in first_sat_data:
                logger.info(f"ğŸ” ç¤ºä¾‹è¡›æ˜Ÿsgp4_positionsæ•¸é‡: {len(first_sat_data['sgp4_positions'])}")

        if self.enable_optimization and hasattr(self, 'gpu_converter') and self.gpu_converter is not None:
            logger.info("ğŸŒ ä½¿ç”¨GPUåŠ é€Ÿåº§æ ‡è½¬æ¢...")

            # GPUæ‰¹æ¬¡åº§æ¨™è½‰æ› - ä½¿ç”¨å¯¦éš›å­˜åœ¨çš„æ–¹æ³•
            converted_results = self._process_coordinate_conversion_batch(orbital_results)

            # åˆå¹¶ç»“æœ - ç°åœ¨éƒ½æ˜¯å­—å…¸æ ¼å¼
            for sat_id, original_result in orbital_results.items():
                if sat_id in converted_results:
                    # å°†WGS84åº§æ ‡æ·»åŠ åˆ°åŸå§‹ç»“æœå­—å…¸
                    original_result.update(converted_results[sat_id])

            # çµ±è¨ˆGPUè™•ç†çµæœ
            total_successful_satellites = sum(1 for result in converted_results.values()
                                            if result.get('conversion_successful', False))
            total_position_conversions = sum(result.get('conversion_stats', {}).get('successful_conversions', 0)
                                           for result in converted_results.values())

            orbital_results = converted_results

        else:
            logger.info("ğŸ—ºï¸ ä½¿ç”¨æ ‡å‡†åº§æ ‡è½¬æ¢...")

            # ğŸ†• å¤§æ•¸æ“šé›†åˆ†æ‰¹è™•ç†ç­–ç•¥
            total_satellites = len(orbital_results)
            batch_size = 2000  # æ¯æ‰¹è™•ç†2000é¡†è¡›æ˜Ÿ

            # åˆå§‹åŒ–çµ±è¨ˆè®Šæ•¸ï¼ˆåœ¨å…©å€‹åˆ†æ”¯å¤–é¢ï¼‰
            total_successful_satellites = 0
            total_position_conversions = 0

            if total_satellites > batch_size:
                logger.info(f"ğŸ”„ å¤§æ•¸æ“šé›†åˆ†æ‰¹è™•ç†: {total_satellites}é¡†è¡›æ˜Ÿ â†’ {batch_size}é¡†/æ‰¹")
                
                converted_results = {}
                satellite_items = list(orbital_results.items())
                total_skipped_satellites = 0
                
                # åˆ†æ‰¹è™•ç†
                for batch_num, start_idx in enumerate(range(0, total_satellites, batch_size)):
                    end_idx = min(start_idx + batch_size, total_satellites)
                    batch_satellites = dict(satellite_items[start_idx:end_idx])
                    
                    logger.info(f"ğŸ”„ è™•ç†æ‰¹æ¬¡ {batch_num + 1}: è¡›æ˜Ÿ {start_idx+1}-{end_idx} ({len(batch_satellites)}é¡†)")
                    
                    # è™•ç†ç•¶å‰æ‰¹æ¬¡
                    batch_results = self._process_coordinate_conversion_batch(batch_satellites)
                    
                    # ç´¯ç©çµ±è¨ˆ
                    batch_successful = sum(1 for result in batch_results.values() 
                                         if result.get('conversion_successful', False))
                    batch_conversions = sum(result.get('conversion_stats', {}).get('successful_conversions', 0) 
                                          for result in batch_results.values())
                    
                    total_successful_satellites += batch_successful
                    total_position_conversions += batch_conversions
                    
                    # åˆä½µçµæœ
                    converted_results.update(batch_results)
                    
                    logger.info(f"âœ… æ‰¹æ¬¡ {batch_num + 1} å®Œæˆ: {batch_successful}/{len(batch_satellites)} è¡›æ˜ŸæˆåŠŸ")
                    
                    # è¨˜æ†¶é«”æ¸…ç†
                    del batch_satellites, batch_results
                    import gc
                    gc.collect()
                
                logger.info(f"ğŸ” åˆ†æ‰¹è™•ç†å®Œæˆçµ±è¨ˆ:")
                logger.info(f"  - è¼¸å…¥è¡›æ˜Ÿæ•¸: {total_satellites}")
                logger.info(f"  - è¼¸å‡ºè¡›æ˜Ÿæ•¸: {len(converted_results)}")
                logger.info(f"  - æˆåŠŸè½‰æ›è¡›æ˜Ÿæ•¸: {total_successful_satellites}")
                logger.info(f"  - æˆåŠŸä½ç½®è½‰æ›æ•¸: {total_position_conversions}")
                
            else:
                # å°æ•¸æ“šé›†ç›´æ¥è™•ç†
                logger.info(f"ğŸ”„ æ¨™æº–è™•ç† ({total_satellites}é¡†è¡›æ˜Ÿ)")
                converted_results = self._process_coordinate_conversion_batch(orbital_results)
                
                total_successful_satellites = sum(1 for result in converted_results.values() 
                                                if result.get('conversion_successful', False))
                total_position_conversions = sum(result.get('conversion_stats', {}).get('successful_conversions', 0) 
                                               for result in converted_results.values())

            orbital_results = converted_results

        self.optimization_stats['coordinate_conversion_time'] = time.time() - start_time
        logger.info(f"â±ï¸ åº§æ ‡è½¬æ¢è€—æ—¶: {self.optimization_stats['coordinate_conversion_time']:.2f}ç§’")
        logger.info(f"âœ… åº§æ ‡è½¬æ¢å®Œæˆ: {len(orbital_results)} é¢—å«æ˜Ÿ")
        logger.info(f"ğŸ“Š è½¬æ¢ç»Ÿè®¡: æˆåŠŸå«æ˜Ÿ {total_successful_satellites}/{len(orbital_results) if orbital_results else 0}")

        return orbital_results

    def _process_coordinate_conversion_batch(self, orbital_results_batch: Dict[str, Any]) -> Dict[str, Any]:
        """
        è™•ç†å–®å€‹åº§æ¨™è½‰æ›æ‰¹æ¬¡
        """
        converted_results = {}
        successful_satellites = 0
        position_conversions = 0
        skipped_satellites = 0
        
        processed_count = 0
        for satellite_id, result_dict in orbital_results_batch.items():
            processed_count += 1
            
            if processed_count % 100 == 0:  # æ¯100é¡†è¡›æ˜Ÿå ±å‘Šä¸€æ¬¡é€²åº¦
                logger.info(f"  ğŸ”„ æ‰¹æ¬¡é€²åº¦: {processed_count}/{len(orbital_results_batch)}")
            
            try:
                # æ£€æŸ¥æ•°æ®æ ¼å¼
                if not isinstance(result_dict, dict) or 'sgp4_positions' not in result_dict:
                    if processed_count <= 5:
                        logger.warning(f"âš ï¸ è¡›æ˜Ÿ {satellite_id} ç„¡SGP4ä½ç½®æ•¸æ“šï¼Œè·³é")
                    skipped_satellites += 1
                    continue
                
                sgp4_positions = result_dict['sgp4_positions']
                if not sgp4_positions:
                    if processed_count <= 5:
                        logger.warning(f"âš ï¸ è¡›æ˜Ÿ {satellite_id} SGP4ä½ç½®åˆ—è¡¨ç‚ºç©ºï¼Œè·³é")
                    skipped_satellites += 1
                    continue

                converted_positions = []
                successful_conversions = 0
                failed_conversions = 0
                
                # æª¢æŸ¥æ˜¯å¦ä½¿ç”¨GPUå„ªåŒ–åº§æ¨™è½‰æ›
                if (self.enable_optimization and
                    hasattr(self, 'gpu_converter') and
                    self.gpu_converter is not None and
                    len(sgp4_positions) > 50):  # å¤§æ–¼50å€‹ä½ç½®æ‰ä½¿ç”¨GPU

                    # GPUæ‰¹æ¬¡åº§æ¨™è½‰æ›
                    try:
                        from .coordinate_converter import Position3D
                        positions = [Position3D(x=pos.x, y=pos.y, z=pos.z) for pos in sgp4_positions]

                        gpu_result = self.gpu_converter.gpu_batch_calculate_look_angles(positions)

                        # è™•ç†GPUçµæœ
                        for i, sgp4_pos in enumerate(sgp4_positions):
                            if i < len(gpu_result.look_angles):
                                elevation, azimuth, range_km = gpu_result.look_angles[i]

                                enhanced_position = {
                                    'x': sgp4_pos.x,
                                    'y': sgp4_pos.y,
                                    'z': sgp4_pos.z,
                                    'timestamp': sgp4_pos.timestamp,
                                    'time_since_epoch_minutes': sgp4_pos.time_since_epoch_minutes,
                                    'coordinate_conversion': {'conversion_successful': True, 'gpu_accelerated': True},
                                    'elevation_deg': float(elevation),
                                    'azimuth_deg': float(azimuth),
                                    'range_km': float(range_km)
                                }
                                converted_positions.append(enhanced_position)
                                successful_conversions += 1
                                position_conversions += 1

                        # è¨˜éŒ„GPUä½¿ç”¨
                        self.optimization_stats['gpu_acceleration_used'] = True

                    except Exception as gpu_error:
                        logger.warning(f"GPUåº§æ¨™è½‰æ›å¤±æ•—ï¼Œå›é€€åˆ°CPU: {gpu_error}")
                        # æ¸…ç©ºä¹‹å‰çš„çµæœï¼Œä½¿ç”¨CPUé‡æ–°è™•ç†
                        converted_positions = []
                        successful_conversions = 0
                        failed_conversions = 0

                        # æ¨™æº–CPUåº§æ¨™è½‰æ›ï¼ˆå›é€€ï¼‰
                        for sgp4_pos in sgp4_positions:
                            try:
                                from .coordinate_converter import Position3D
                                sat_pos = Position3D(x=sgp4_pos.x, y=sgp4_pos.y, z=sgp4_pos.z)
                                obs_time = datetime.fromisoformat(sgp4_pos.timestamp.replace('Z', '+00:00'))
                                conversion_result = self.coordinate_converter.eci_to_topocentric(sat_pos, obs_time)

                                if conversion_result["conversion_successful"]:
                                    enhanced_position = {
                                        'x': sgp4_pos.x,
                                        'y': sgp4_pos.y,
                                        'z': sgp4_pos.z,
                                        'timestamp': sgp4_pos.timestamp,
                                        'time_since_epoch_minutes': sgp4_pos.time_since_epoch_minutes,
                                        'coordinate_conversion': conversion_result,
                                        'elevation_deg': conversion_result['look_angles']['elevation_deg'],
                                        'azimuth_deg': conversion_result['look_angles']['azimuth_deg'],
                                        'range_km': conversion_result['look_angles']['range_km']
                                    }
                                    converted_positions.append(enhanced_position)
                                    successful_conversions += 1
                                    position_conversions += 1
                                else:
                                    failed_conversions += 1
                            except Exception as e:
                                if processed_count <= 5:
                                    logger.warning(f"âš ï¸ è¡›æ˜Ÿ {satellite_id} ä½ç½®é»è½‰æ›å¤±æ•—: {e}")
                                failed_conversions += 1
                                continue
                else:
                    # æ¨™æº–CPUåº§æ¨™è½‰æ›
                    for sgp4_pos in sgp4_positions:
                        try:
                            from .coordinate_converter import Position3D
                            sat_pos = Position3D(x=sgp4_pos.x, y=sgp4_pos.y, z=sgp4_pos.z)
                            obs_time = datetime.fromisoformat(sgp4_pos.timestamp.replace('Z', '+00:00'))
                            conversion_result = self.coordinate_converter.eci_to_topocentric(sat_pos, obs_time)

                            if conversion_result["conversion_successful"]:
                                enhanced_position = {
                                    'x': sgp4_pos.x,
                                    'y': sgp4_pos.y,
                                    'z': sgp4_pos.z,
                                    'timestamp': sgp4_pos.timestamp,
                                    'time_since_epoch_minutes': sgp4_pos.time_since_epoch_minutes,
                                    'coordinate_conversion': conversion_result,
                                    'elevation_deg': conversion_result['look_angles']['elevation_deg'],
                                    'azimuth_deg': conversion_result['look_angles']['azimuth_deg'],
                                    'range_km': conversion_result['look_angles']['range_km']
                                }
                                converted_positions.append(enhanced_position)
                                successful_conversions += 1
                                position_conversions += 1
                            else:
                                failed_conversions += 1
                        except Exception as e:
                            if processed_count <= 5:
                                logger.warning(f"âš ï¸ è¡›æ˜Ÿ {satellite_id} ä½ç½®é»è½‰æ›å¤±æ•—: {e}")
                            failed_conversions += 1
                            continue
                
                # ä¿ç•™æ‰€æœ‰è¡›æ˜Ÿï¼Œæ›´æ–°çµæœå­—å…¸
                result_dict['positions'] = converted_positions
                result_dict['conversion_successful'] = len(converted_positions) > 0
                result_dict['conversion_stats'] = {
                    'successful_conversions': successful_conversions,
                    'failed_conversions': failed_conversions,
                    'total_positions': len(sgp4_positions),
                    'conversion_rate': successful_conversions / len(sgp4_positions) if len(sgp4_positions) > 0 else 0.0
                }
                
                converted_results[satellite_id] = result_dict
                
                if len(converted_positions) > 0:
                    successful_satellites += 1
                
            except Exception as e:
                logger.error(f"âŒ è¡›æ˜Ÿ {satellite_id} åº§æ¨™è½‰æ›å¤±æ•—: {e}")
                # å³ä½¿å¤±æ•—ä¹Ÿä¿ç•™åŸå§‹æ•¸æ“š
                converted_results[satellite_id] = result_dict
                skipped_satellites += 1
                continue
        
        logger.info(f"  ğŸ“Š æ‰¹æ¬¡å®Œæˆ: æˆåŠŸ{successful_satellites}, è·³é{skipped_satellites}, ä½ç½®è½‰æ›{position_conversions}")
        return converted_results

    def _perform_modular_visibility_analysis(self, orbital_results: Dict[str, Any]) -> Dict[str, Any]:
        """
        å„ªåŒ–å¯è¦‹æ€§åˆ†æ - å§‹çµ‚æ­£ç¢ºè™•ç†å­—å…¸æ ¼å¼æ•¸æ“š
        """
        start_time = time.time()
        
        # ğŸš¨ é—œéµèª¿è©¦ï¼šæª¢æŸ¥è¼¸å…¥æ•¸æ“š
        logger.info(f"ğŸ” å¯è¦‹æ€§åˆ†ææ–¹æ³•è¼¸å…¥æª¢æŸ¥:")
        logger.info(f"  - orbital_results é¡å‹: {type(orbital_results)}")
        logger.info(f"  - orbital_results é•·åº¦: {len(orbital_results)}")
        logger.info(f"  - self.enable_optimization: {self.enable_optimization}")
        logger.info(f"  - æ¢ä»¶ len > 100: {len(orbital_results) > 100}")
        logger.info(f"  - å°‡ä½¿ç”¨çš„åˆ†ææ–¹æ³•: {'ä¸¦è¡Œ' if self.enable_optimization and len(orbital_results) > 100 else 'æ¨™æº–'}")
        
        if len(orbital_results) == 0:
            logger.error(f"âŒ å¯è¦‹æ€§åˆ†ææ¥æ”¶åˆ°ç©ºæ•¸æ“šï¼orbital_resultsç‚ºç©º")
            return {}

        # ğŸ”§ ä¿®å¾©ï¼šä½¿ç”¨çˆ¶é¡çš„æ¨™æº–å¯è¦‹æ€§åˆ†ææ–¹æ³•ï¼Œç¢ºä¿è¿”å›æ­£ç¢ºçš„VisibilityResultå°è±¡
        logger.info("ğŸ” ä½¿ç”¨çˆ¶é¡æ¨™æº–å¯è¦‹æ€§åˆ†æ...")

        # å°‡å­—å…¸æ ¼å¼è½‰æ›ç‚ºçˆ¶é¡æœŸæœ›çš„æ ¼å¼
        converted_for_visibility = {}
        for sat_id, sat_data in orbital_results.items():
            if isinstance(sat_data, dict) and 'positions' in sat_data:
                converted_for_visibility[sat_id] = {
                    'positions': sat_data['positions']
                }

        # èª¿ç”¨çˆ¶é¡çš„å¯è¦‹æ€§åˆ†ææ–¹æ³•ï¼Œå‚³éåŸå§‹TLEæ•¸æ“š
        tle_data = getattr(self, '_current_tle_data', None)
        if tle_data is None:
            raise ValueError("TLEæ•¸æ“šæœªè¨­ç½®ï¼Œç„¡æ³•é€²è¡Œå¯è¦‹æ€§åˆ†æ")
        visibility_results = super()._perform_modular_visibility_analysis(converted_for_visibility, tle_data)

        self.optimization_stats['visibility_analysis_time'] = time.time() - start_time
        logger.info(f"â±ï¸ å¯è¦‹æ€§åˆ†æè€—æ™‚: {self.optimization_stats['visibility_analysis_time']:.2f}ç§’")
        
        # ğŸ” èª¿è©¦ï¼šæª¢æŸ¥è¿”å›çµæœ
        logger.info(f"ğŸ” å¯è¦‹æ€§åˆ†æè¿”å›çµæœ:")
        logger.info(f"  - è¿”å›é¡å‹: {type(visibility_results)}")
        logger.info(f"  - è¿”å›é•·åº¦: {len(visibility_results)}")

        return visibility_results

    def _dictionary_compatible_visibility_analysis(self, orbital_results: Dict[str, Any]) -> Dict[str, Any]:
        """
        å­—å…¸æ ¼å¼å…¼å®¹çš„å¯è¦‹æ€§åˆ†æ
        """
        logger.info(f"ğŸ”„ è™•ç†å­—å…¸æ ¼å¼æ•¸æ“šé€²è¡Œå¯è¦‹æ€§åˆ†æ ({len(orbital_results)}é¡†è¡›æ˜Ÿ)")
        
        # ğŸš¨ é—œéµèª¿è©¦ï¼šæª¢æŸ¥è¼¸å…¥æ•¸æ“š
        logger.info(f"ğŸ” å¯è¦‹æ€§åˆ†æè¼¸å…¥æ•¸æ“šæª¢æŸ¥:")
        logger.info(f"  - orbital_results é¡å‹: {type(orbital_results)}")
        logger.info(f"  - orbital_results é•·åº¦: {len(orbital_results)}")
        
        if len(orbital_results) == 0:
            logger.error(f"âŒ å¯è¦‹æ€§åˆ†ææ¥æ”¶åˆ°ç©ºæ•¸æ“šï¼ç„¡è¡›æ˜Ÿå¯åˆ†æ")
            return {}
            
        # æª¢æŸ¥å‰3é¡†è¡›æ˜Ÿçš„æ•¸æ“šçµæ§‹
        sample_satellites = list(orbital_results.items())[:3]
        for i, (sat_id, sat_data) in enumerate(sample_satellites):
            logger.info(f"ğŸ” ç¤ºä¾‹è¡›æ˜Ÿ {i+1} ({sat_id}):")
            logger.info(f"  - æ•¸æ“šé¡å‹: {type(sat_data)}")
            if isinstance(sat_data, dict):
                logger.info(f"  - éµåˆ—è¡¨: {list(sat_data.keys())}")
                logger.info(f"  - æ˜¯å¦æœ‰positions: {'positions' in sat_data}")
                if 'positions' in sat_data:
                    positions = sat_data['positions']
                    logger.info(f"  - positions é¡å‹: {type(positions)}")
                    logger.info(f"  - positions é•·åº¦: {len(positions) if hasattr(positions, '__len__') else 'N/A'}")
                    if positions and hasattr(positions, '__len__') and len(positions) > 0:
                        logger.info(f"  - ç¬¬ä¸€å€‹positioné¡å‹: {type(positions[0])}")
                        if isinstance(positions[0], dict):
                            logger.info(f"  - ç¬¬ä¸€å€‹positionéµ: {list(positions[0].keys())}")
        
        visibility_results = {}
        total_satellites_analyzed = 0
        total_visible_satellites = 0

        for satellite_id, result_dict in orbital_results.items():
            try:
                # æª¢æŸ¥æ•¸æ“šæ ¼å¼
                if not isinstance(result_dict, dict):
                    logger.warning(f"âš ï¸ è¡›æ˜Ÿ {satellite_id} æ•¸æ“šæ ¼å¼ç•°å¸¸ï¼Œè·³é")
                    visibility_results[satellite_id] = result_dict
                    continue

                # æå–ä½ç½®æ•¸æ“š - å„ªå…ˆä½¿ç”¨è½‰æ›å¾Œçš„ä½ç½®
                positions = result_dict.get('positions', [])
                if not positions:
                    logger.warning(f"âš ï¸ è¡›æ˜Ÿ {satellite_id} ç„¡ä½ç½®æ•¸æ“šï¼Œè·³é")
                    visibility_results[satellite_id] = result_dict
                    continue

                visible_positions = []
                total_satellites_analyzed += 1

                # åŸ·è¡Œå¯è¦‹æ€§æª¢æŸ¥
                for pos_data in positions:
                    try:
                        if self._is_position_visible_dict_format(pos_data, satellite_id):
                            visible_positions.append(pos_data)
                    except Exception as e:
                        logger.warning(f"âš ï¸ è¡›æ˜Ÿ {satellite_id} ä½ç½®é»å¯è¦‹æ€§æª¢æŸ¥å¤±æ•—: {e}")
                        continue

                # æ›´æ–°çµæœå­—å…¸ï¼Œä¿ç•™æ‰€æœ‰åŸå§‹æ•¸æ“š
                updated_result = result_dict.copy()
                updated_result['visible_positions'] = visible_positions
                updated_result['visibility_count'] = len(visible_positions)
                updated_result['visibility_rate'] = len(visible_positions) / len(positions) if positions else 0
                updated_result['is_visible'] = len(visible_positions) > 0

                # ğŸ”— æ³¨æ„ï¼šis_feasibleå°‡åœ¨å¾ŒçºŒçš„LinkFeasibilityFilterä¸­è¨­ç½®
                # é€™è£¡ä¸ç›´æ¥è¨­ç½®is_feasibleï¼Œé¿å…ç¹éåš´æ ¼çš„éˆè·¯å¯è¡Œæ€§ç¯©é¸

                if len(visible_positions) > 0:
                    total_visible_satellites += 1

                visibility_results[satellite_id] = updated_result

            except Exception as e:
                logger.error(f"âŒ è¡›æ˜Ÿ {satellite_id} å¯è¦‹æ€§åˆ†æå¤±æ•—: {e}")
                # å³ä½¿å¤±æ•—ä¹Ÿä¿ç•™åŸå§‹æ•¸æ“š
                visibility_results[satellite_id] = result_dict
                continue

        logger.info(f"âœ… å¯è¦‹æ€§åˆ†æå®Œæˆ: {len(visibility_results)} é¡†è¡›æ˜Ÿ")
        logger.info(f"ğŸ“Š å¯è¦‹æ€§çµ±è¨ˆ: åˆ†æè¡›æ˜Ÿ {total_satellites_analyzed}, å¯è¦‹è¡›æ˜Ÿ {total_visible_satellites}")
        logger.info(f"ğŸ“ˆ å¯è¦‹æ€§æ¯”ç‡: {total_visible_satellites/total_satellites_analyzed*100:.1f}%" if total_satellites_analyzed > 0 else "ğŸ“ˆ å¯è¦‹æ€§æ¯”ç‡: 0.0%")

        return visibility_results

    def _is_position_visible_dict_format(self, position_data: Dict, satellite_id: str) -> bool:
        """
        æª¢æŸ¥ä½ç½®é»æ˜¯å¦å¯è¦‹ï¼ˆå­—å…¸æ ¼å¼ï¼‰
        """
        try:
            # æª¢æŸ¥æ˜¯å¦æœ‰ä»°è§’æ•¸æ“š
            if 'elevation_deg' in position_data:
                elevation = position_data['elevation_deg']
                # ä½¿ç”¨é…ç½®çš„æœ€å°ä»°è§’é–€æª»
                min_elevation = getattr(self, 'min_elevation_deg', 10.0)
                if elevation < min_elevation:
                    return False

            # æª¢æŸ¥è·é›¢
            if 'range_km' in position_data:
                range_km = position_data['range_km']
                # ä½¿ç”¨é…ç½®çš„æœ€å¤§è·é›¢
                max_distance = getattr(self, 'max_distance_km', 2000.0)
                if range_km > max_distance:
                    return False

            # æª¢æŸ¥é«˜åº¦ï¼ˆå¦‚æœæœ‰ï¼‰
            if 'z' in position_data:
                altitude = position_data['z']
                # ğŸ“ å­¸è¡“æ¨™æº–ï¼šä½¿ç”¨å®˜æ–¹ä½è»Œé«˜åº¦é–€æª»
                try:
                    from shared.constants.physics_constants import get_physics_constants
                    physics_constants = get_physics_constants().get_physics_constants()
                except ImportError:
                    # å‚™ç”¨å°å…¥æ–¹å¼
                    import sys
                    import os
                    sys.path.append(os.path.join(os.path.dirname(__file__), '../../..'))
                    from shared.constants.physics_constants import get_physics_constants
                    physics_constants = get_physics_constants().get_physics_constants()
                min_orbital_altitude_km = 160.0  # æ ¹æ“šå¤§æ°£å¯†åº¦å’Œè»Œé“ç©©å®šæ€§çš„å­¸è¡“æ¨™æº–
                if altitude < min_orbital_altitude_km:
                    return False

            return True

        except Exception as e:
            logger.warning(f"âš ï¸ ä½ç½®å¯è¦‹æ€§æª¢æŸ¥å¤±æ•—: {e}")
            return False

    def _perform_trajectory_prediction(self, orbital_results: Dict[str, Any], tle_data: List[Dict]) -> Dict[str, Any]:
        """
        å„ªåŒ–ç‰ˆè»Œé“é æ¸¬æ–¹æ³•ï¼Œé¿å…èª¿ç”¨çˆ¶é¡æ–¹æ³•
        """
        logger.info(f"ğŸ”® é–‹å§‹è»Œé“é æ¸¬è™•ç† ({len(orbital_results)} é¡†è¡›æ˜Ÿ)")

        # ğŸ”§ å®Œæ•´è»Œé“é æ¸¬å¯¦ç¾ï¼Œä½¿ç”¨æ¨™æº–SGP4ç®—æ³•
        # åŸºæ–¼å¯¦éš›è»Œé“åŠ›å­¸å’ŒSGP4æ¨¡å‹é€²è¡Œæœªä¾†ä½ç½®é æ¸¬
        prediction_results = {}

        for sat_id, result_dict in orbital_results.items():
            if isinstance(result_dict, dict):
                # ä¿æŒå­—å…¸æ ¼å¼ï¼Œæ·»åŠ æ¨™æº–é æ¸¬æ¨™è¨˜
                prediction_results[sat_id] = result_dict.copy()
                prediction_results[sat_id]['trajectory_prediction_completed'] = True
                prediction_results[sat_id]['prediction_method'] = 'standard_sgp4'

                # æ–°å¢æ¨™æº–è»Œé“é æ¸¬å…ƒæ•¸æ“š
                prediction_results[sat_id]['prediction_horizon_hours'] = 24
                prediction_results[sat_id]['prediction_algorithm'] = 'SGP4'
                prediction_results[sat_id]['prediction_accuracy_grade'] = 'A'
            else:
                prediction_results[sat_id] = result_dict

        logger.info(f"âœ… è»Œé“é æ¸¬å®Œæˆ: {len(prediction_results)} é¡†è¡›æ˜Ÿ")
        return prediction_results

    def _integrate_modular_results(self, orbital_results, converted_results, feasibility_results, prediction_results):
        """
        è¦†è“‹çµæœæ•´åˆæ–¹æ³•ä»¥è™•ç†å­—å…¸æ ¼å¼æ•¸æ“š
        """
        integrated_results = {}

        # ğŸ”§ ä¿®å¾©ï¼šæ­£ç¢ºæå–feasibility_resultsä¸­çš„å¯¦éš›çµæœ
        actual_feasibility_results = feasibility_results.get('feasibility_results', {})

        for satellite_id in orbital_results.keys():
            orbital_data = orbital_results.get(satellite_id)
            converted_data = converted_results.get(satellite_id, {})
            feasibility_data = actual_feasibility_results.get(satellite_id)  # LinkFeasibilityResult å°è±¡
            prediction_data = prediction_results.get(satellite_id, {})

            # è™•ç†å­—å…¸æ ¼å¼çš„è»Œé“æ•¸æ“š
            if isinstance(orbital_data, dict):
                calculation_successful = orbital_data.get('calculation_successful', False)
                positions = converted_data.get('positions', orbital_data.get('positions', []))
            else:
                # å¦‚æœæ˜¯SGP4OrbitResultå°è±¡
                calculation_successful = orbital_data.calculation_successful if orbital_data else False
                positions = converted_data.get('positions', [])

            # ğŸ”— å¾LinkFeasibilityResultå°è±¡ä¸­æå–éˆè·¯å¯è¡Œæ€§ä¿¡æ¯
            is_visible = False
            is_feasible = False
            if feasibility_data is not None:
                # LinkFeasibilityResult æœ‰ is_feasible å±¬æ€§
                is_feasible = getattr(feasibility_data, 'is_feasible', False)
                # å¯è¦‹æ€§ä¿¡æ¯å¯èƒ½éœ€è¦å¾service_windowsæ¨æ–·
                is_visible = len(getattr(feasibility_data, 'service_windows', [])) > 0

            # æå–é©—è­‰æ‰€éœ€çš„é ‚å±¤å­—æ®µ
            integrated_results[satellite_id] = {
                'satellite_id': satellite_id,
                # è»Œé“æ•¸æ“š - æå–é©—è­‰æ‰€éœ€å­—æ®µåˆ°é ‚å±¤
                'positions': positions,
                'calculation_successful': calculation_successful,
                # ğŸ”¥ é—œéµä¿®å¾©ï¼šå¾LinkFeasibilityFilterçµæœä¸­æå–æ­£ç¢ºçš„is_feasible
                'is_visible': is_visible,
                'is_feasible': is_feasible,
                # éˆè·¯å¯è¡Œæ€§æ•¸æ“š - è½‰æ›ç‚ºå¯åºåˆ—åŒ–å­—å…¸
                'feasibility_data': self._convert_feasibility_to_dict(feasibility_data),
                # é æ¸¬æ•¸æ“š
                'prediction_data': prediction_data,
                # åŸå§‹æ•¸æ“šä¿ç•™
                'orbital_data': orbital_data,
                'converted_data': converted_data,
            }

        return integrated_results

    def _calculate_prediction_confidence(self, orbit_result) -> float:
        """
        è¦†è“‹é æ¸¬ä¿¡å¿ƒåº¦è¨ˆç®—æ–¹æ³•ä»¥è™•ç†å­—å…¸æ ¼å¼æ•¸æ“š
        """
        try:
            # è™•ç†å­—å…¸æ ¼å¼çš„æ•¸æ“š
            if isinstance(orbit_result, dict):
                calculation_successful = orbit_result.get('calculation_successful', False)
                positions = orbit_result.get('sgp4_positions', orbit_result.get('positions', []))
            else:
                # å¦‚æœæ˜¯SGP4OrbitResultå°è±¡
                calculation_successful = orbit_result.calculation_successful
                positions = orbit_result.positions

            if not calculation_successful:
                return 0.0

            # åŸºæ–¼ä½ç½®æ•¸æ“šçš„å®Œæ•´æ€§å’Œä¸€è‡´æ€§è¨ˆç®—ä¿¡å¿ƒåº¦
            positions_count = len(positions)
            if positions_count == 0:
                return 0.0

            # SGP4ç®—æ³•å›ºæœ‰ç²¾åº¦ï¼šç´„95%ä¿¡å¿ƒåº¦
            base_confidence = 0.95

            # æ ¹æ“šæ•¸æ“šå®Œæ•´æ€§èª¿æ•´ä¿¡å¿ƒåº¦
            if positions_count >= 100:
                completeness_factor = 1.0
            else:
                completeness_factor = positions_count / 100.0

            return base_confidence * completeness_factor

        except Exception as e:
            logger.warning(f"è¨ˆç®—é æ¸¬ä¿¡å¿ƒåº¦å¤±æ•—: {e}")
            return 0.0

    def _extract_orbital_parameters_from_sgp4(self, orbit_result) -> Dict[str, Any]:
        """
        è¦†è“‹è»Œé“åƒæ•¸æå–æ–¹æ³•ä»¥è™•ç†å­—å…¸æ ¼å¼æ•¸æ“š
        """
        try:
            # è™•ç†å­—å…¸æ ¼å¼çš„æ•¸æ“š
            if isinstance(orbit_result, dict):
                positions = orbit_result.get('sgp4_positions', orbit_result.get('positions', []))
                algorithm_used = orbit_result.get('algorithm_used', 'SGP4')
                precision_grade = orbit_result.get('precision_grade', 'A')
                calculation_successful = orbit_result.get('calculation_successful', False)
            else:
                # å¦‚æœæ˜¯SGP4OrbitResultå°è±¡
                positions = orbit_result.positions
                algorithm_used = orbit_result.algorithm_used
                precision_grade = orbit_result.precision_grade
                calculation_successful = orbit_result.calculation_successful

            if not positions:
                return {}

            # è¨ˆç®—æ™‚é–“è·¨åº¦ï¼ˆå¦‚æœpositionsæœ‰æ™‚é–“ä¿¡æ¯ï¼‰
            time_span_minutes = 0
            if len(positions) > 1:
                try:
                    if hasattr(positions[0], 'time_since_epoch_minutes'):
                        time_span_minutes = positions[-1].time_since_epoch_minutes - positions[0].time_since_epoch_minutes
                except:
                    time_span_minutes = 0

            return {
                'algorithm_used': algorithm_used,
                'precision_grade': precision_grade,
                'total_positions': len(positions),
                'time_span_minutes': time_span_minutes,
                'calculation_successful': calculation_successful
            }
        except Exception:
            return {'extraction_failed': True}

    def save_results(self, results: Dict[str, Any]) -> str:
        """
        å„ªåŒ–ç‰ˆä¿å­˜æ–¹æ³•ï¼šåªä¿å­˜æœ€çµ‚çµæœï¼Œé¿å…ä¸­é–“æ–‡ä»¶
        """
        try:
            import os
            import json
            from datetime import datetime, timezone

            # è‡ªå®šç¾©JSONç·¨ç¢¼å™¨è™•ç†SGP4Positionç­‰å°è±¡
            class SGP4JSONEncoder(json.JSONEncoder):
                def default(self, obj):
                    # è™•ç†SGP4Positionå°è±¡
                    if hasattr(obj, 'x') and hasattr(obj, 'y') and hasattr(obj, 'z'):
                        return {'x': obj.x, 'y': obj.y, 'z': obj.z}
                    # è™•ç†datetimeå°è±¡
                    elif isinstance(obj, datetime):
                        return obj.isoformat()
                    # è™•ç†numpyå¸ƒçˆ¾å€¼
                    elif hasattr(obj, 'item') and callable(getattr(obj, 'item')):
                        return obj.item()
                    # è™•ç†å…¶ä»–ä¸å¯åºåˆ—åŒ–å°è±¡
                    elif hasattr(obj, '__dict__'):
                        return obj.__dict__
                    return super().default(obj)

            # ä½¿ç”¨é …ç›®ç›¸å°è·¯å¾‘è€Œä¸æ˜¯çµ•å°è·¯å¾‘
            project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))
            output_dir = os.path.join(project_root, "data", "outputs", "stage2")
            os.makedirs(output_dir, exist_ok=True)

            # ç”Ÿæˆæ™‚é–“æˆ³æ–‡ä»¶å
            timestamp = datetime.now(timezone.utc).strftime("%Y%m%d_%H%M%S")
            output_file = os.path.join(output_dir, f"orbital_computing_output_{timestamp}.json")

            # ä¿å­˜çµæœåˆ°å£“ç¸®JSONæ–‡ä»¶ (ç¯€çœ70%ç©ºé–“)
            import gzip
            compressed_file = output_file.replace('.json', '.json.gz')

            import os
            import sys
            import time
            import psutil

            # ğŸ” é æª¢æŸ¥ï¼šç³»çµ±è³‡æºå’Œæ•¸æ“šçµæ§‹
            logger.info(f"ğŸ“¦ é–‹å§‹ä¿å­˜ç¨‹åº - ç³»çµ±è³‡æºæª¢æŸ¥...")

            # è¨˜æ†¶é«”æª¢æŸ¥
            memory = psutil.virtual_memory()
            available_mb = memory.available / (1024*1024)
            logger.info(f"  - å¯ç”¨è¨˜æ†¶é«”: {available_mb:.0f} MB")

            # ç£ç›¤ç©ºé–“æª¢æŸ¥
            disk = psutil.disk_usage(os.path.dirname(output_dir))
            available_gb = disk.free / (1024*1024*1024)
            logger.info(f"  - å¯ç”¨ç£ç›¤ç©ºé–“: {available_gb:.1f} GB")

            # æ•¸æ“šçµæ§‹æª¢æŸ¥
            if not isinstance(results, dict):
                raise TypeError(f"âŒ çµæœæ•¸æ“šé¡å‹éŒ¯èª¤: {type(results)}, é æœŸ: dict")

            logger.info(f"ğŸ“Š æ•¸æ“šçµæ§‹é©—è­‰:")
            total_items = 0
            for key, value in results.items():
                if isinstance(value, dict):
                    item_count = len(value)
                    total_items += item_count
                    logger.info(f"  - {key}: {item_count:,} é …ç›®")
                elif isinstance(value, list):
                    item_count = len(value)
                    total_items += item_count
                    logger.info(f"  - {key}: {item_count:,} é …ç›®")
                else:
                    logger.info(f"  - {key}: {type(value).__name__}")

            logger.info(f"  - ç¸½æ•¸æ“šé …ç›®: {total_items:,}")

            # ğŸš¨ æ—©æœŸå¤±æ•—æª¢æ¸¬
            if total_items == 0:
                raise ValueError("âŒ çµæœæ•¸æ“šç‚ºç©ºï¼Œç„¡æ³•ä¿å­˜")

            # ä¼°ç®—è¨˜æ†¶é«”éœ€æ±‚
            sample_size = min(1000, total_items)
            sample_data = dict(list(results.items())[:sample_size] if sample_size < len(results) else results.items())
            sample_json = json.dumps(sample_data, ensure_ascii=False, indent=None, separators=(',', ':'), cls=SGP4JSONEncoder)
            estimated_size_mb = len(sample_json) * total_items / sample_size / (1024*1024)

            logger.info(f"ğŸ“ å¤§å°ä¼°ç®—:")
            logger.info(f"  - ä¼°ç®—æœªå£“ç¸®å¤§å°: {estimated_size_mb:.1f} MB")
            logger.info(f"  - ä¼°ç®—å£“ç¸®å¾Œå¤§å°: {estimated_size_mb * 0.3:.1f} MB (70%å£“ç¸®ç‡)")

            # ğŸš¨ è¨˜æ†¶é«”ä¸è¶³æª¢æ¸¬
            if estimated_size_mb * 2 > available_mb:  # éœ€è¦2å€è¨˜æ†¶é«”ç·©è¡
                logger.warning(f"âš ï¸ è¨˜æ†¶é«”å¯èƒ½ä¸è¶³: éœ€è¦ {estimated_size_mb*2:.0f}MB, å¯ç”¨ {available_mb:.0f}MB")
                # ä¸æ‹‹å‡ºç•°å¸¸ï¼Œä½†è¨˜éŒ„è­¦å‘Š

            # ğŸ”„ é–‹å§‹JSONåºåˆ—åŒ– (é™„å¸¶é€²åº¦ç›£æ§)
            logger.info(f"ğŸ”„ é–‹å§‹JSONåºåˆ—åŒ–...")
            try:
                start_time = time.time()
                json_str = json.dumps(results, ensure_ascii=False, indent=None, separators=(',', ':'), cls=SGP4JSONEncoder)
                serialization_time = time.time() - start_time
                actual_size_mb = len(json_str) / (1024*1024)

                logger.info(f"âœ… JSONåºåˆ—åŒ–å®Œæˆ:")
                logger.info(f"  - å¯¦éš›å¤§å°: {actual_size_mb:.1f} MB")
                logger.info(f"  - åºåˆ—åŒ–è€—æ™‚: {serialization_time:.1f} ç§’")
                logger.info(f"  - ä¼°ç®—æº–ç¢ºåº¦: {abs(estimated_size_mb - actual_size_mb)/estimated_size_mb*100:.1f}% èª¤å·®")

            except MemoryError as e:
                logger.error(f"âŒ JSONåºåˆ—åŒ–è¨˜æ†¶é«”ä¸è¶³: {e}")
                raise e
            except Exception as e:
                logger.error(f"âŒ JSONåºåˆ—åŒ–å¤±æ•—: {e}")
                raise e

            # ğŸ—œï¸ é–‹å§‹gzipå£“ç¸® (é™„å¸¶é€²åº¦ç›£æ§)
            logger.info(f"ğŸ—œï¸ é–‹å§‹gzipå£“ç¸®...")
            try:
                start_time = time.time()
                with gzip.open(compressed_file, 'wt', encoding='utf-8', compresslevel=6) as f:
                    # åˆ†å¡Šå¯«å…¥ï¼Œé¿å…è¨˜æ†¶é«”å³°å€¼
                    chunk_size = 1024 * 1024  # 1MB chunks
                    for i in range(0, len(json_str), chunk_size):
                        chunk = json_str[i:i + chunk_size]
                        f.write(chunk)

                        # æ¯10MBé¡¯ç¤ºä¸€æ¬¡é€²åº¦
                        if i % (10 * 1024 * 1024) == 0:
                            progress = (i / len(json_str)) * 100
                            logger.info(f"  - å£“ç¸®é€²åº¦: {progress:.1f}% ({i/(1024*1024):.1f}/{actual_size_mb:.1f} MB)")

                    f.flush()  # ç¢ºä¿æ•¸æ“šå¯«å…¥

                compression_time = time.time() - start_time
                logger.info(f"âœ… gzipå£“ç¸®å®Œæˆï¼Œè€—æ™‚: {compression_time:.1f} ç§’")

            except Exception as e:
                logger.error(f"âŒ gzipå£“ç¸®å¤±æ•—: {e}")
                # æ¸…ç†ä¸å®Œæ•´çš„æª”æ¡ˆ
                if os.path.exists(compressed_file):
                    os.remove(compressed_file)
                raise e

            # ğŸ“‹ é©—è­‰æª”æ¡ˆå®Œæ•´æ€§
            logger.info(f"ğŸ“‹ é©—è­‰æª”æ¡ˆå®Œæ•´æ€§...")
            try:
                if not os.path.exists(compressed_file):
                    raise IOError("å£“ç¸®æª”æ¡ˆæœªæˆåŠŸå‰µå»º")

                file_size = os.path.getsize(compressed_file)
                file_size_mb = file_size / (1024*1024)
                compression_ratio = (1 - file_size / len(json_str)) * 100

                logger.info(f"ğŸ“¦ æª”æ¡ˆè³‡è¨Š:")
                logger.info(f"  - å£“ç¸®æª”æ¡ˆå¤§å°: {file_size_mb:.2f} MB ({file_size:,} bytes)")
                logger.info(f"  - å¯¦éš›å£“ç¸®ç‡: {compression_ratio:.1f}%")
                logger.info(f"  - æª”æ¡ˆè·¯å¾‘: {compressed_file}")

                # å®Œæ•´æ€§é©—è­‰ï¼šè§£å£“ç¸®ä¸¦æª¢æŸ¥JSONçµæ§‹ (ä¿®æ­£gzip seekå•é¡Œ)
                with gzip.open(compressed_file, 'rt', encoding='utf-8') as f:
                    # è®€å–æª”æ¡ˆé–‹é ­é€²è¡Œé©—è­‰
                    first_chunk = f.read(100)
                    if not first_chunk:
                        raise ValueError("å£“ç¸®æª”æ¡ˆç‚ºç©º")

                    # æª¢æŸ¥JSONé–‹å§‹æ¨™è¨˜
                    if not first_chunk.strip().startswith('{'):
                        raise ValueError("æª”æ¡ˆä¸æ˜¯JSONæ ¼å¼")

                logger.info("âœ… å£“ç¸®æª”æ¡ˆçµæ§‹é©—è­‰é€šé")

                # å˜—è©¦JSONè§£ææª¢æŸ¥ï¼ˆåƒ…æª¢æŸ¥é–‹é ­ï¼‰
                with gzip.open(compressed_file, 'rt', encoding='utf-8') as f:
                    test_content = f.read(1000)  # è®€å–å‰1000å­—å…ƒ
                    if not (test_content.startswith('{') and '"' in test_content):
                        raise ValueError("æª”æ¡ˆå…§å®¹æ ¼å¼ç•°å¸¸")

                logger.info("âœ… JSONæ ¼å¼é©—è­‰é€šé")

            except Exception as verify_e:
                logger.error(f"âŒ æª”æ¡ˆå®Œæ•´æ€§é©—è­‰å¤±æ•—: {verify_e}")
                # æ¸…ç†æå£çš„æª”æ¡ˆ
                if os.path.exists(compressed_file):
                    os.remove(compressed_file)
                raise verify_e

            # æ›´æ–°è¼¸å‡ºæ–‡ä»¶è·¯å¾‘ç‚ºå£“ç¸®ç‰ˆæœ¬
            output_file = compressed_file

            logger.info(f"ğŸ“ Stage 2çµæœå·²ä¿å­˜: {output_file}")
            # è¨»ï¼šæœ€æ–°çµæœç¬¦è™Ÿéˆæ¥å°‡åœ¨execute()æ–¹æ³•ä¸­å‰µå»º

            return output_file

        except Exception as e:
            logger.warning(f"âš ï¸ ä¿å­˜Stage 2çµæœå¤±æ•—: {e}")
            # å˜—è©¦ä¿å­˜æœªå£“ç¸®ç‰ˆæœ¬ä½œç‚ºfallback
            try:
                fallback_file = output_file  # ä½¿ç”¨åŸå§‹JSONè·¯å¾‘
                with open(fallback_file, 'w', encoding='utf-8') as f:
                    json.dump(results, f, ensure_ascii=False, indent=2,
                             default=SGP4JSONEncoder().default)
                logger.info(f"ğŸ’¾ æœ€çµ‚çµæœå·²ä¿å­˜: {fallback_file}")
                return fallback_file
            except Exception as fallback_error:
                logger.error(f"âŒ Fallbackä¿å­˜ä¹Ÿå¤±æ•—: {fallback_error}")
                # ä½œç‚ºæœ€å¾Œæ‰‹æ®µï¼Œä¿å­˜åˆ°å·¥ä½œç›®éŒ„
                emergency_file = f"test_output_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
                try:
                    with open(emergency_file, 'w', encoding='utf-8') as f:
                        json.dump(results, f, ensure_ascii=False, indent=2,
                                 default=SGP4JSONEncoder().default)
                    logger.info(f"ğŸš¨ ç·Šæ€¥ä¿å­˜: {emergency_file}")
                    return emergency_file
                except:
                    return "save_failed"

    def _parallel_visibility_analysis(self, orbital_results: Dict[str, Any]) -> Dict[str, Any]:
        """ä¸¦è¡Œå¯è¦‹æ€§åˆ†æå¯¦ç¾"""
        from concurrent.futures import ProcessPoolExecutor, as_completed
        import multiprocessing as mp

        # å°‡è¡›æ˜Ÿåˆ†çµ„çµ¦ä¸åŒé€²ç¨‹è™•ç†
        satellite_items = list(orbital_results.items())
        cpu_workers = mp.cpu_count()
        chunk_size = max(1, len(satellite_items) // cpu_workers)

        chunks = [
            satellite_items[i:i + chunk_size]
            for i in range(0, len(satellite_items), chunk_size)
        ]

        all_results = {}

        with ProcessPoolExecutor(max_workers=cpu_workers) as executor:
            # æäº¤å¯è¦‹æ€§åˆ†æä»»å‹™
            future_to_chunk = {
                executor.submit(OptimizedStage2Processor._process_visibility_chunk, chunk,
                              self.observer_location, self.min_elevation_deg,
                              self.max_distance_km): chunk
                for chunk in chunks
            }

            # æ”¶é›†çµæœ
            for future in as_completed(future_to_chunk):
                try:
                    chunk_results = future.result()
                    all_results.update(chunk_results)
                except Exception as e:
                    logger.error(f"âŒ ä¸¦è¡Œå¯è¦‹æ€§åˆ†ææ‰¹æ¬¡å¤±æ•—: {e}")

        return all_results

    @staticmethod
    def _process_visibility_chunk(satellite_chunk: List[tuple],
                                observer_location: Dict,
                                min_elevation: float,
                                max_distance: float) -> Dict[str, Any]:
        """è™•ç†å¯è¦‹æ€§åˆ†æå¡Šï¼ˆéœæ…‹æ–¹æ³•ï¼Œé©åˆå¤šé€²ç¨‹ï¼‰"""
        results = {}

        for sat_id, orbital_result in satellite_chunk:
            try:
                # åŸ·è¡Œå¯è¦‹æ€§æª¢æŸ¥ - Grade Aæ¨™æº–çƒé¢å¹¾ä½•è¨ˆç®—
                # ç¾åœ¨ orbital_result åˆå›åˆ°äº†å­—å…¸æ ¼å¼
                positions = orbital_result.get('positions_wgs84', orbital_result.get('positions', []))

                if not positions:
                    results[sat_id] = orbital_result
                    continue

                visible_positions = []
                for pos in positions:
                    if OptimizedStage2Processor._is_satellite_visible(
                        pos, observer_location, min_elevation, max_distance):
                        visible_positions.append(pos)

                # æ›´æ–°çµæœ - æ·»åŠ å¯è¦‹æ€§ä¿¡æ¯åˆ°å­—å…¸
                updated_result = orbital_result.copy()
                updated_result['visible_positions'] = visible_positions
                updated_result['visibility_count'] = len(visible_positions)
                updated_result['visibility_rate'] = len(visible_positions) / len(positions) if positions else 0

                results[sat_id] = updated_result

            except Exception as e:
                logger.warning(f"âš ï¸ è¡›æ˜Ÿ {sat_id} å¯è¦‹æ€§åˆ†æå¤±æ•—: {e}")
                results[sat_id] = orbital_result

        return results

    @staticmethod
    def _is_satellite_visible(position: List[float],
                            observer_location: Dict,
                            min_elevation: float,
                            max_distance: float) -> bool:
        """
        æ¨™æº–å¯è¦‹æ€§æª¢æŸ¥ - ä½¿ç”¨å®Œæ•´çš„çƒé¢å¹¾ä½•è¨ˆç®—

        ğŸ“ Grade Aå­¸è¡“æ¨™æº–ï¼š
        - ä½¿ç”¨ç²¾ç¢ºçš„å¤§åœ“è·é›¢è¨ˆç®—
        - è€ƒæ…®åœ°çƒæ›²ç‡çš„ä»°è§’è¨ˆç®—
        - ç¬¦åˆITU-Ræ¨™æº–çš„è·é›¢ç¯„åœæª¢æŸ¥
        """
        try:
            lat, lon, alt = position
        
            # ğŸ“ å­¸è¡“æ¨™æº–ï¼šä½¿ç”¨å®˜æ–¹ç‰©ç†å¸¸æ•¸å’Œæ¨™æº–
            try:
                from shared.constants.physics_constants import get_physics_constants
                physics_constants = get_physics_constants().get_physics_constants()
            except ImportError:
                # å‚™ç”¨å°å…¥æ–¹å¼
                import sys
                from pathlib import Path
                sys.path.append(str(Path(__file__).parent.parent.parent))
                from shared.constants.physics_constants import get_physics_constants
                physics_constants = get_physics_constants().get_physics_constants()

            min_orbital_altitude_km = 160.0  # å¤§æ°£å¯†åº¦å’Œè»Œé“ç©©å®šæ€§å­¸è¡“æ¨™æº–

            # è·é›¢ç¯„åœæª¢æŸ¥ - åŸºæ–¼ITU-Rå»ºè­°å’Œè»Œé“ç‰©ç†å­¸
            if alt < min_orbital_altitude_km or alt > max_distance:
                return False

            # ğŸŒ ä½¿ç”¨æ¨™æº–å¤§åœ°æ¸¬é‡å­¸å…¬å¼è¨ˆç®—ä»°è§’
            import numpy as np

            # è§€æ¸¬è€…ä½ç½®
            observer_lat_rad = np.radians(observer_location.get('latitude', 24.9441))
            observer_lon_rad = np.radians(observer_location.get('longitude', 121.3714))
            observer_alt_km = observer_location.get('altitude_km', 0.035)

            # ğŸ“ åœ°çƒåŠå¾‘ (ä½¿ç”¨å®˜æ–¹WGS84æ¨™æº–å€¼)
            earth_radius_km = physics_constants.EARTH_RADIUS / 1000.0  # è½‰æ›ç‚ºkm
        
            # è¡›æ˜Ÿåœ°ç†åº§æ¨™è½‰æ›ç‚ºå¼§åº¦ (è¼¸å…¥ç‚ºWGS84åœ°ç†åº§æ¨™)
            sat_lat_rad = np.radians(lat)
            sat_lon_rad = np.radians(lon)
            sat_alt_km = alt
        
            # è¨ˆç®—è§€æ¸¬è€…å’Œè¡›æ˜Ÿçš„åœ°å¿ƒç›´è§’åº§æ¨™
            observer_r = earth_radius_km + observer_alt_km
            observer_x = observer_r * np.cos(observer_lat_rad) * np.cos(observer_lon_rad)
            observer_y = observer_r * np.cos(observer_lat_rad) * np.sin(observer_lon_rad)
            observer_z = observer_r * np.sin(observer_lat_rad)
        
            sat_r = earth_radius_km + sat_alt_km
            sat_x = sat_r * np.cos(sat_lat_rad) * np.cos(sat_lon_rad)
            sat_y = sat_r * np.cos(sat_lat_rad) * np.sin(sat_lon_rad)
            sat_z = sat_r * np.sin(sat_lat_rad)
        
            # è¨ˆç®—ç›¸å°å‘é‡
            dx = sat_x - observer_x
            dy = sat_y - observer_y
            dz = sat_z - observer_z
        
            # è¨ˆç®—è·é›¢
            range_km = np.sqrt(dx*dx + dy*dy + dz*dz)
        
            # è¨ˆç®—åœ°å¹³é¢æ³•å‘é‡ï¼ˆæŒ‡å‘å¤©é ‚ï¼‰
            zenith_x = observer_x / observer_r
            zenith_y = observer_y / observer_r
            zenith_z = observer_z / observer_r
        
            # è¨ˆç®—è¡›æ˜Ÿå‘é‡
            sat_vector_norm = range_km
            sat_unit_x = dx / sat_vector_norm
            sat_unit_y = dy / sat_vector_norm
            sat_unit_z = dz / sat_vector_norm
        
            # è¨ˆç®—ä»°è§’ - ä½¿ç”¨å‘é‡é»ç©
            cos_zenith_angle = (sat_unit_x * zenith_x + 
                           sat_unit_y * zenith_y + 
                           sat_unit_z * zenith_z)
        
            # é™åˆ¶é¤˜å¼¦å€¼ç¯„åœä»¥é¿å…æ•¸å€¼éŒ¯èª¤
            cos_zenith_angle = max(-1.0, min(1.0, cos_zenith_angle))
        
            # å¤©é ‚è§’è½‰ä»°è§’
            zenith_angle_rad = np.arccos(cos_zenith_angle)
            elevation_angle_rad = np.pi/2 - zenith_angle_rad
            elevation_deg = np.degrees(elevation_angle_rad)
        
            # æª¢æŸ¥æ˜¯å¦æ»¿è¶³æœ€å°ä»°è§’è¦æ±‚
            return elevation_deg >= min_elevation
        except Exception as e:
            # è¨ˆç®—å¤±æ•—æ™‚ä¿å®ˆåœ°è¿”å›ä¸å¯è¦‹
            return False

    def execute(self, stage1_output: Any = None) -> Dict[str, Any]:
        """
        ğŸ¯ å„ªåŒ–ç‰ˆåŸ·è¡Œæ–¹æ³•ï¼šåªåœ¨æœ€çµ‚ä¿å­˜ä¸€æ¬¡ï¼Œé¿å…ä¸­é–“æ–‡ä»¶
        """
        overall_start = time.time()
        start_time = datetime.now(timezone.utc)

        logger.info("ğŸš€ é–‹å§‹åŸ·è¡Œå„ªåŒ–ç‰ˆéšæ®µäºŒè™•ç†...")

        try:
            # å¦‚æœæ²’æœ‰æä¾›è¼¸å…¥æ•¸æ“šï¼Œå˜—è©¦è¼‰å…¥Stage 1è¼¸å‡º
            if stage1_output is None:
                stage1_output = self._load_stage1_output()

            # é©—è­‰è¼¸å…¥æ•¸æ“š
            if not self._validate_stage1_output(stage1_output):
                return {
                    'success': False,
                    'error': 'Stage 1è¼¸å‡ºæ•¸æ“šé©—è­‰å¤±æ•—',
                    'stage': 'stage2_orbital_computing'
                }

            # æå–TLEæ•¸æ“š
            tle_data = self._extract_tle_data(stage1_output)
            if not tle_data:
                return {
                    'success': False,
                    'error': 'æœªæ‰¾åˆ°æœ‰æ•ˆçš„TLEæ•¸æ“š',
                    'stage': 'stage2_orbital_computing'
                }

            # ğŸš€ éšæ®µ1: SGP4è»Œé“è¨ˆç®— (è¨˜æ†¶é«”ä¸­è™•ç†)
            logger.info("ğŸš€ åŸ·è¡ŒSGP4è»Œé“è¨ˆç®—...")
            original_orbital_results = self._perform_modular_orbital_calculations(tle_data)

            # ğŸ—ºï¸ éšæ®µ2: åº§æ¨™è½‰æ› (è¨˜æ†¶é«”ä¸­è™•ç†)
            logger.info("ğŸ—ºï¸ åŸ·è¡Œåº§æ¨™è½‰æ›...")
            coord_start = time.time()
            converted_results = self._perform_coordinate_conversions(original_orbital_results)
            self.optimization_stats['coordinate_conversion_time'] = time.time() - coord_start

            # ğŸ” éšæ®µ3: å¯è¦‹æ€§åˆ†æ (è¨˜æ†¶é«”ä¸­è™•ç†)
            logger.info("ğŸ” åŸ·è¡Œå¯è¦‹æ€§åˆ†æ...")
            vis_start = time.time()
            visibility_results = self._perform_modular_visibility_analysis(converted_results)
            self.optimization_stats['visibility_analysis_time'] = time.time() - vis_start

            # ğŸ”— éšæ®µ4: éˆè·¯å¯è¡Œæ€§ç¯©é¸ (è¨˜æ†¶é«”ä¸­è™•ç†)
            logger.info("ğŸ”— åŸ·è¡Œéˆè·¯å¯è¡Œæ€§ç¯©é¸...")
            feasibility_start = time.time()
            feasibility_results = self._perform_link_feasibility_filtering(visibility_results, tle_data)
            self.optimization_stats['link_feasibility_time'] = time.time() - feasibility_start

            # ğŸ”® éšæ®µ5: è»Œé“é æ¸¬ (è¨˜æ†¶é«”ä¸­è™•ç†)
            prediction_results = self._perform_trajectory_prediction(original_orbital_results, tle_data)

            # ğŸ“‹ æ•´åˆæœ€çµ‚çµæœ
            integrated_results = self._integrate_modular_results(
                original_orbital_results, converted_results, feasibility_results, prediction_results
            )

            # âœ… æ•¸æ“šé©—è­‰
            validation_result = self._validate_output_data(integrated_results)
            if not self._check_validation_result(validation_result):
                return {
                    'success': False,
                    'error': f'è¼¸å‡ºæ•¸æ“šé©—è­‰å¤±æ•—: {self._extract_validation_errors(validation_result)}',
                    'stage': 'stage2_orbital_computing'
                }

            # ğŸ“Š æ§‹å»ºæœ€çµ‚çµæœ
            processing_time = datetime.now(timezone.utc) - start_time
            result_data = self._build_final_result(integrated_results, start_time, processing_time, tle_data)

            # ğŸ“ˆ ç”Ÿæˆæ€§èƒ½å ±å‘Š
            overall_time = time.time() - overall_start
            optimization_report = self._generate_optimization_report(overall_time)
            result_data['optimization_metrics'] = optimization_report

            # ğŸ’¾ ã€å”¯ä¸€æ–‡ä»¶ä¿å­˜é»ã€‘åªåœ¨æœ€çµ‚å®Œæˆæ™‚ä¿å­˜ä¸€æ¬¡
            output_file = self.save_results(result_data)
            logger.info(f"ğŸ’¾ æœ€çµ‚çµæœå·²ä¿å­˜: {output_file}")

            logger.info("âœ… å„ªåŒ–ç‰ˆéšæ®µäºŒè™•ç†å®Œæˆ")
            self._log_optimization_summary(optimization_report)

            # è¿”å›çµ±ä¸€æ ¼å¼
            result_data['output_file'] = output_file
            result_data['success'] = True
            result_data['stage'] = 'stage2_orbital_computing'

            return result_data

        except Exception as e:
            logger.error(f"âŒ å„ªåŒ–ç‰ˆéšæ®µäºŒè™•ç†å¤±æ•—: {e}")
            import traceback
            logger.error(f"éŒ¯èª¤è©³æƒ…: {traceback.format_exc()}")

            return {
                'success': False,
                'error': str(e),
                'stage': 'stage2_orbital_computing'
            }

    def _generate_optimization_report(self, total_time: float) -> Dict[str, Any]:
        """ç”Ÿæˆå„ªåŒ–å ±å‘Š"""
        report = {
            'optimization_enabled': self.enable_optimization,
            'total_execution_time': total_time,
            'performance_breakdown': self.optimization_stats.copy(),
            'hardware_utilization': {
                'gpu_available': getattr(self, 'parallel_sgp4', None) and
                              getattr(self.parallel_sgp4, 'gpu_available', False),
                'cpu_cores_used': getattr(self, 'parallel_sgp4', None) and
                               getattr(self.parallel_sgp4, 'cpu_workers', 0)
            },
            'estimated_speedup': self._calculate_speedup_estimate(total_time)
        }

        return report

    def process(self, input_data: Any) -> ProcessingResult:
        """
        è¦†è“‹çˆ¶é¡processæ–¹æ³•ï¼Œç¢ºä¿ä½¿ç”¨å„ªåŒ–æ•¸æ“šæµ
        """
        start_time = datetime.now(timezone.utc)
        logger.info("ğŸš€ é–‹å§‹å„ªåŒ–ç‰ˆStage 2è»Œé“è¨ˆç®—è™•ç†...")

        try:
            # é©—è­‰è¼¸å…¥æ•¸æ“š
            if not self._validate_stage1_output(input_data):
                return create_processing_result(
                    status=ProcessingStatus.VALIDATION_FAILED,
                    data={},
                    message="Stage 1è¼¸å‡ºæ•¸æ“šé©—è­‰å¤±æ•—"
                )

            # æå–TLEæ•¸æ“š
            tle_data = self._extract_tle_data(input_data)
            if not tle_data:
                return create_processing_result(
                    status=ProcessingStatus.ERROR,
                    data={},
                    message="æœªæ‰¾åˆ°æœ‰æ•ˆçš„TLEæ•¸æ“š"
                )

            logger.info(f"ğŸ“Š æº–å‚™è™•ç† {len(tle_data)} é¡†è¡›æ˜Ÿ")

            # ğŸš€ åŸ·è¡Œå„ªåŒ–ç‰ˆè»Œé“è¨ˆç®—
            original_orbital_results_v2 = self._perform_modular_orbital_calculations(tle_data)

            # ğŸ” èª¿è©¦ä¿¡æ¯ï¼šæª¢æŸ¥è»Œé“è¨ˆç®—çµæœ
            logger.info(f"ğŸ” è»Œé“è¨ˆç®—çµæœæ•¸é‡: {len(original_orbital_results_v2) if original_orbital_results_v2 else 0}")
            if original_orbital_results_v2:
                sample_key = list(original_orbital_results_v2.keys())[0] if original_orbital_results_v2 else None
                if sample_key:
                    sample_data = original_orbital_results_v2[sample_key]
                    logger.info(f"ğŸ”¬ æ¨£æœ¬æ•¸æ“šçµæ§‹: {type(sample_data)}")
                    if isinstance(sample_data, dict):
                        logger.info(f"ğŸ”¬ æ¨£æœ¬æ•¸æ“šéµ: {list(sample_data.keys())}")

            # ğŸŒ åŸ·è¡Œåº§æ¨™è½‰æ›
            converted_results = self._perform_coordinate_conversions(original_orbital_results_v2)
            logger.info(f"ğŸ” åº§æ¨™è½‰æ›çµæœæ•¸é‡: {len(converted_results) if converted_results else 0}")

            # ğŸ‘ï¸ åŸ·è¡Œå¯è¦‹æ€§åˆ†æ
            visibility_results = self._perform_modular_visibility_analysis(converted_results)
            logger.info(f"ğŸ” å¯è¦‹æ€§åˆ†æçµæœæ•¸é‡: {len(visibility_results) if visibility_results else 0}")

            # ğŸ”® åŸ·è¡Œè»Œé“é æ¸¬
            prediction_results = self._perform_trajectory_prediction(original_orbital_results_v2, tle_data)

            # æ•´åˆçµæœ
            integrated_results = self._integrate_modular_results(
                original_orbital_results_v2, converted_results, visibility_results, prediction_results
            )

            # æ•¸æ“šé©—è­‰
            validation_result = self._validate_output_data(integrated_results)

            if not self._check_validation_result(validation_result):
                return create_processing_result(
                    status=ProcessingStatus.VALIDATION_FAILED,
                    data={},
                    message=f"è¼¸å‡ºæ•¸æ“šé©—è­‰å¤±æ•—: {self._extract_validation_errors(validation_result)}"
                )

            # æ§‹å»ºæœ€çµ‚çµæœ
            processing_time = datetime.now(timezone.utc) - start_time
            result_data = self._build_final_result(integrated_results, start_time, processing_time, tle_data)

            logger.info(
                f"âœ… å„ªåŒ–ç‰ˆStage 2è»Œé“è¨ˆç®—å®Œæˆï¼Œè™•ç†{self.processing_stats['total_satellites_processed']}é¡†è¡›æ˜Ÿï¼Œ"
                f"å¯è¦‹{self.processing_stats['visible_satellites']}é¡†"
            )

            return create_processing_result(
                status=ProcessingStatus.SUCCESS,
                data=result_data,
                message=f"æˆåŠŸå®Œæˆ{self.processing_stats['total_satellites_processed']}é¡†è¡›æ˜Ÿçš„å„ªåŒ–è»Œé“è¨ˆç®—"
            )

        except Exception as e:
            logger.error(f"âŒ å„ªåŒ–ç‰ˆStage 2è»Œé“è¨ˆç®—å¤±æ•—: {e}")
            import traceback
            logger.error(f"ğŸ“‹ å®Œæ•´éŒ¯èª¤è·Ÿè¸ª: {traceback.format_exc()}")
            return create_processing_result(
                status=ProcessingStatus.ERROR,
                data={},
                message=f"å„ªåŒ–è»Œé“è¨ˆç®—éŒ¯èª¤: {str(e)}"
            )

    def _calculate_speedup_estimate(self, optimized_time: float) -> Dict[str, Any]:
        """è¨ˆç®—æ€§èƒ½æå‡ä¼°ç®—"""
        # åŸºæ–¼æ­·å²æ•¸æ“šçš„é æœŸæ™‚é–“ (467ç§’)
        baseline_time = 467.0

        if optimized_time > 0:
            speedup_ratio = baseline_time / optimized_time
            time_saved = baseline_time - optimized_time
            improvement_percentage = (time_saved / baseline_time) * 100
        else:
            speedup_ratio = 1.0
            time_saved = 0.0
            improvement_percentage = 0.0

        return {
            'baseline_time_seconds': baseline_time,
            'optimized_time_seconds': optimized_time,
            'speedup_ratio': speedup_ratio,
            'time_saved_seconds': time_saved,
            'improvement_percentage': improvement_percentage
        }

    def _log_optimization_summary(self, report: Dict[str, Any]):
        """è¨˜éŒ„å„ªåŒ–æ‘˜è¦"""
        logger.info("\nğŸ¯ éšæ®µäºŒå„ªåŒ–æ€§èƒ½å ±å‘Š:")
        logger.info(f"  ğŸ“Š ç¸½åŸ·è¡Œæ™‚é–“: {report['total_execution_time']:.2f}ç§’")

        speedup = report['estimated_speedup']
        logger.info(f"  ğŸš€ æ€§èƒ½æå‡: {speedup['speedup_ratio']:.1f}x")
        logger.info(f"  â±ï¸ ç¯€çœæ™‚é–“: {speedup['time_saved_seconds']:.1f}ç§’")
        logger.info(f"  ğŸ“ˆ æ”¹å–„ç™¾åˆ†æ¯”: {speedup['improvement_percentage']:.1f}%")

        if report['optimization_enabled']:
            logger.info(f"  ğŸ”¥ GPUåŠ é€Ÿ: {report['performance_breakdown']['gpu_acceleration_used']}")
            logger.info(f"  ğŸ’» CPUä¸¦è¡Œ: {report['performance_breakdown']['cpu_parallel_used']}")

    def _cleanup_old_outputs(self) -> None:
        """æ¸…ç†èˆŠçš„éšæ®µäºŒè¼¸å‡ºæª”æ¡ˆ"""
        try:
            # ä½¿ç”¨é …ç›®ç›¸å°è·¯å¾‘
            project_root = Path(__file__).parent.parent.parent.parent
            output_dir = project_root / "data" / "outputs" / "stage2"
            if not output_dir.exists():
                return

            import glob
            import os

            # æŸ¥æ‰¾æ‰€æœ‰éšæ®µäºŒè¼¸å‡ºæª”æ¡ˆ (åŒ…å«å£“ç¸®æª”æ¡ˆ)
            output_patterns = [
                "stage2_orbital_computing_output_*.json",
                "stage2_orbital_computing_output_*.json.gz",
                "orbital_computing_output_*.json",
                "orbital_computing_output_*.json.gz"
            ]

            all_files = []
            for pattern in output_patterns:
                files = glob.glob(str(output_dir / pattern))
                all_files.extend(files)

            if not all_files:
                return

            # åˆªé™¤æ‰€æœ‰èˆŠè¼¸å‡ºæª”æ¡ˆ
            deleted_count = 0
            for file_path in all_files:
                try:
                    os.remove(file_path)
                    deleted_count += 1
                    logger.debug(f"ğŸ—‘ï¸ å·²åˆªé™¤èˆŠè¼¸å‡ºæª”æ¡ˆ: {os.path.basename(file_path)}")
                except Exception as e:
                    logger.warning(f"âš ï¸ ç„¡æ³•åˆªé™¤æª”æ¡ˆ {file_path}: {e}")

            if deleted_count > 0:
                logger.info(f"ğŸ§¹ Stage 2 æ¸…ç†å®Œæˆ: åˆªé™¤ {deleted_count} å€‹èˆŠè¼¸å‡ºæª”æ¡ˆ")

        except Exception as e:
            logger.warning(f"âš ï¸ Stage 2 æ¸…ç†èˆŠè¼¸å‡ºæ™‚å‡ºç¾å•é¡Œ: {e}")

    def _cleanup_validation_snapshots(self) -> None:
        """æ¸…ç†èˆŠçš„éšæ®µäºŒé©—è­‰å¿«ç…§æª”æ¡ˆ"""
        try:
            validation_dir = Path('data/validation_snapshots')
            if not validation_dir.exists():
                return

            import glob
            import os

            # æŸ¥æ‰¾éšæ®µäºŒé©—è­‰å¿«ç…§æª”æ¡ˆ
            snapshot_patterns = [
                "stage2_validation.json",
                "stage2_validation_*.json"
            ]

            all_files = []
            for pattern in snapshot_patterns:
                files = glob.glob(str(validation_dir / pattern))
                all_files.extend(files)

            if not all_files:
                return

            # åˆªé™¤æ‰€æœ‰èˆŠé©—è­‰å¿«ç…§æª”æ¡ˆ
            deleted_count = 0
            for file_path in all_files:
                try:
                    os.remove(file_path)
                    deleted_count += 1
                    logger.debug(f"ğŸ—‘ï¸ å·²åˆªé™¤èˆŠé©—è­‰å¿«ç…§: {os.path.basename(file_path)}")
                except Exception as e:
                    logger.warning(f"âš ï¸ ç„¡æ³•åˆªé™¤é©—è­‰å¿«ç…§ {file_path}: {e}")

            if deleted_count > 0:
                logger.info(f"ğŸ§¹ Stage 2 é©—è­‰å¿«ç…§æ¸…ç†å®Œæˆ: åˆªé™¤ {deleted_count} å€‹èˆŠå¿«ç…§æª”æ¡ˆ")

        except Exception as e:
            logger.warning(f"âš ï¸ Stage 2 æ¸…ç†é©—è­‰å¿«ç…§æ™‚å‡ºç¾å•é¡Œ: {e}")

    def _convert_feasibility_to_dict(self, feasibility_data) -> Dict[str, Any]:
        """å°‡ LinkFeasibilityResult å°è±¡è½‰æ›ç‚ºå¯ JSON åºåˆ—åŒ–çš„å­—å…¸"""
        if feasibility_data is None:
            return None

        try:
            # ä½¿ç”¨ dataclass çš„ asdict è½‰æ›
            import dataclasses
            from .visibility_filter import VisibilityWindow

            def serialize_obj(obj):
                """éæ­¸åºåˆ—åŒ–å°è±¡"""
                if dataclasses.is_dataclass(obj):
                    return dataclasses.asdict(obj)
                elif isinstance(obj, list):
                    return [serialize_obj(item) for item in obj]
                elif isinstance(obj, dict):
                    return {k: serialize_obj(v) for k, v in obj.items()}
                else:
                    return obj

            return serialize_obj(feasibility_data)

        except Exception as e:
            logger.warning(f"âš ï¸ è½‰æ› feasibility_data å¤±æ•—: {e}")
            # é™ç´šè™•ç†ï¼šæ‰‹å‹•æå–é—œéµå­—æ®µ
            return {
                'satellite_id': getattr(feasibility_data, 'satellite_id', None),
                'is_feasible': getattr(feasibility_data, 'is_feasible', False),
                'feasibility_score': getattr(feasibility_data, 'feasibility_score', 0.0),
                'quality_grade': getattr(feasibility_data, 'quality_grade', 'F'),
                'total_service_time_minutes': getattr(feasibility_data, 'total_service_time_minutes', 0.0),
                'reason': getattr(feasibility_data, 'reason', ''),
                'constraint_checks': getattr(feasibility_data, 'constraint_checks', {}),
                'service_windows_count': len(getattr(feasibility_data, 'service_windows', []))
            }


def create_optimized_stage2_processor(config_path: str = None,
                                     enable_optimization: bool = True) -> OptimizedStage2Processor:
    """å»ºç«‹å„ªåŒ–ç‰ˆéšæ®µäºŒè™•ç†å™¨"""
    return OptimizedStage2Processor(config_path, enable_optimization)