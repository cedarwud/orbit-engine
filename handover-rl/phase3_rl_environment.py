#!/usr/bin/env python3
"""
Phase 3: RL ç’°å¢ƒè¨­è¨ˆèˆ‡é©—è­‰

åŠŸèƒ½ï¼š
1. å¯¦ä½œ Gymnasium ç’°å¢ƒï¼ˆç¬¦åˆ OpenAI Gym æ¨™æº–ï¼‰
2. å®šç¾©ç‹€æ…‹ç©ºé–“ã€å‹•ä½œç©ºé–“ã€çå‹µå‡½æ•¸
3. é©—è­‰ç’°å¢ƒæ­£ç¢ºæ€§
4. æ¸¬è©¦éš¨æ©Ÿç­–ç•¥æ€§èƒ½

åŸ·è¡Œï¼š
    python phase3_rl_environment.py

è¼¸å‡ºï¼š
    é©—è­‰ç’°å¢ƒæ­£ç¢ºæ€§
    æ¸¬è©¦éš¨æ©Ÿç­–ç•¥ baseline
"""

import json
import yaml
import numpy as np
import gymnasium as gym
from gymnasium import spaces
from typing import Dict, List, Tuple, Optional
from pathlib import Path


class Episode:
    """
    Episode æ•¸æ“šåŒ…è£é¡ï¼ˆç›¸å®¹æ€§è™•ç†ï¼‰

    ç”¨æ–¼åŒ…è£å¾ pickle è¼‰å…¥çš„å­—å…¸æ•¸æ“šï¼Œæä¾›çµ±ä¸€çš„å±¬æ€§è¨ªå•æ¥å£ã€‚

    ç›¸å®¹æ€§è™•ç†ï¼š
    - time_series (Phase 1 ä¿å­˜çš„éµå) â†’ time_points (Phase 3 æœŸæœ›çš„å±¬æ€§å)
    - å­—å…¸æ ¼å¼ â†’ å°è±¡å±¬æ€§è¨ªå•

    SOURCE: rl.md Line 242-267 (Episode åŒ…è£é¡è¨­è¨ˆ)
    """

    def __init__(self, data):
        """
        Args:
            data: Episode æ•¸æ“šï¼ˆå­—å…¸æˆ– Episode å°è±¡ï¼‰
        """
        # å¦‚æœå·²ç¶“æ˜¯ Episode å°è±¡ï¼Œç›´æ¥è¤‡è£½å±¬æ€§
        if isinstance(data, Episode):
            self.satellite_id = data.satellite_id
            self.constellation = data.constellation
            self.time_points = data.time_points
            self.gpp_events = data.gpp_events
            self.episode_length = data.episode_length
            return

        # å¾å­—å…¸å‰µå»ºå°è±¡
        self.satellite_id = data['satellite_id']
        self.constellation = data['constellation']

        # ç›¸å®¹æ€§è™•ç†ï¼šæ”¯æŒ time_seriesï¼ˆPhase 1ï¼‰å’Œ time_pointsï¼ˆæœªä¾†å¯èƒ½ï¼‰
        if 'time_points' in data:
            self.time_points = data['time_points']
        elif 'time_series' in data:
            self.time_points = data['time_series']  # â† é—œéµè½‰æ›
        else:
            self.time_points = []

        self.gpp_events = data.get('gpp_events', [])
        self.episode_length = data.get('episode_length', len(self.time_points))


class HandoverEnvironment(gym.Env):
    """
    è¡›æ˜Ÿæ›æ‰‹æ±ºç­– RL ç’°å¢ƒ

    ç‹€æ…‹ç©ºé–“ (12 ç¶­ - å®Œæ•´ç‰¹å¾µé›†):
        ä¿¡è™Ÿå“è³ª (3 ç¶­):
            - RSRP (dBm): Reference Signal Received Power
            - RSRQ (dB): Reference Signal Received Quality
            - SINR (dB): Signal-to-Interference-plus-Noise Ratio

        ç‰©ç†åƒæ•¸ (7 ç¶­):
            - Distance (km): è¡›æ˜Ÿèˆ‡åœ°é¢ç«™è·é›¢
            - Elevation (deg): ä»°è§’
            - Doppler Shift (Hz): éƒ½åœå‹’é »ç§»
            - Radial Velocity (m/s): å¾‘å‘é€Ÿåº¦
            - Atmospheric Loss (dB): å¤§æ°£è¡°æ¸›
            - Path Loss (dB): è·¯å¾‘æè€—
            - Propagation Delay (ms): å‚³æ’­å»¶é²

        3GPP åç§»é‡ (2 ç¶­):
            - Offset MO (dB): Measurement offset
            - Cell Offset (dB): Cell-specific offset

    å‹•ä½œç©ºé–“ (2 å€‹é›¢æ•£å‹•ä½œ):
        0: maintain (ä¿æŒç•¶å‰æœå‹™è¡›æ˜Ÿ)
        1: handover (æ›æ‰‹åˆ°æœ€ä½³é„°å±…)

    çå‹µå‡½æ•¸:
        reward = w1 * qos_improvement
                - w2 * handover_penalty
                + w3 * signal_quality
                - w4 * ping_pong_penalty

    SOURCE:
    - 3GPP TS 38.331 v18.5.1 (äº‹ä»¶å®šç¾©)
    - 3GPP TS 38.215 v18.1.0 Section 5.1 (ä¿¡è™Ÿå“è³ªæ¸¬é‡)
    - docs/final.md Line 139 (çå‹µå‡½æ•¸è¨­è¨ˆ)
    """

    metadata = {'render_modes': ['human']}

    def __init__(self,
                 episodes: List,  # List[Episode] or List[Dict] from phase1_data_loader_v2.py
                 config: Dict,
                 mode: str = 'train'):
        """
        Args:
            episodes: æ›æ‰‹æ±ºç­– Episode åˆ—è¡¨ï¼ˆä¾†è‡ª phase1_data_loader_v2.pyï¼‰
                     å¯ä»¥æ˜¯å­—å…¸åˆ—è¡¨æˆ– Episode å°è±¡åˆ—è¡¨
            config: RL é…ç½®
            mode: 'train' or 'eval'
        """
        super().__init__()

        # ç›¸å®¹æ€§è™•ç†ï¼šå°‡å­—å…¸åˆ—è¡¨è½‰æ›ç‚º Episode å°è±¡åˆ—è¡¨
        # SOURCE: rl.md Line 268-279 (ç›¸å®¹æ€§è½‰æ›é‚è¼¯)
        if episodes and len(episodes) > 0 and isinstance(episodes[0], dict):
            self.episodes = [Episode(ep) for ep in episodes]
            print(f"   âœ… å·²è½‰æ› {len(episodes)} å€‹å­—å…¸ç‚º Episode å°è±¡")
        else:
            self.episodes = episodes

        self.config = config
        self.mode = mode

        # ç‹€æ…‹ç©ºé–“ï¼šBox(12,) - å®Œæ•´ç‰¹å¾µé›†
        # [rsrp, rsrq, sinr, distance, elevation, doppler, velocity,
        #  atmospheric_loss, path_loss, propagation_delay, offset_mo, cell_offset]
        self.observation_space = spaces.Box(
            low=np.array([
                -150.0,  # rsrp_dbm (SOURCE: 3GPP TS 38.215 v18.1.0 Table 5.1.1-1)
                -30.0,   # rsrq_db (SOURCE: 3GPP TS 38.215 v18.1.0 Table 5.1.3-1)
                -10.0,   # rs_sinr_db (SOURCE: 3GPP TS 38.215 v18.1.0 Table 5.1.5-1)
                500.0,   # distance_km (SOURCE: LEO orbit altitude range)
                0.0,     # elevation_deg (SOURCE: 3GPP TR 38.821 Section 6.1.2)
                -50000.0,  # doppler_shift_hz (SOURCE: ITU-R M.1184 Annex 1)
                -8000.0,   # radial_velocity_ms (SOURCE: LEO orbital velocity)
                0.0,     # atmospheric_loss_db (SOURCE: ITU-R P.676-13)
                120.0,   # path_loss_db (SOURCE: 3GPP TR 38.811 Section 6.6.3)
                0.0,     # propagation_delay_ms
                -10.0,   # offset_mo_db (SOURCE: 3GPP TS 38.331 Section 5.5.4)
                -10.0    # cell_offset_db (SOURCE: 3GPP TS 38.331 Section 5.5.4)
            ], dtype=np.float32),
            high=np.array([
                -30.0,   # rsrp_dbm
                -3.0,    # rsrq_db
                30.0,    # rs_sinr_db
                3000.0,  # distance_km
                90.0,    # elevation_deg
                50000.0,   # doppler_shift_hz
                8000.0,    # radial_velocity_ms
                30.0,    # atmospheric_loss_db
                200.0,   # path_loss_db
                50.0,    # propagation_delay_ms
                10.0,    # offset_mo_db
                10.0     # cell_offset_db
            ], dtype=np.float32),
            dtype=np.float32
        )

        # å‹•ä½œç©ºé–“ï¼šDiscrete(2)
        # 0=maintain, 1=handover
        self.action_space = spaces.Discrete(2)

        # çå‹µæ¬Šé‡ (with SOURCE comments)
        reward_config = config['environment']['reward_weights']
        self.w_qos = reward_config['qos_improvement']        # SOURCE: Zhang et al. 2021 (RL handover weight)
        self.w_handover = reward_config['handover_penalty']  # SOURCE: 3GPP TS 36.839 v11.1.0 Section 6.2.3
        self.w_signal = reward_config['signal_quality']      # SOURCE: Chen et al. 2020 (QoS weight)
        self.w_ping_pong = reward_config['ping_pong_penalty']  # SOURCE: 3GPP TS 36.839 v11.1.0 Section 6.2.3

        # ç’°å¢ƒç‹€æ…‹ (Episode-based)
        self.current_episode_idx = 0
        self.current_time_step = 0
        self.total_handovers = 0
        self.last_handover_time = -1
        self.episode_rewards = []

        # ç•¶å‰ Episode
        self.current_episode = None

        # é‡ç½®ç’°å¢ƒ
        self.reset()

    def reset(self, seed: Optional[int] = None, options: Optional[Dict] = None) -> Tuple[np.ndarray, Dict]:
        """é‡ç½®ç’°å¢ƒï¼ˆé–‹å§‹æ–° Episodeï¼‰"""
        super().reset(seed=seed)

        self.current_time_step = 0
        self.total_handovers = 0
        self.last_handover_time = -1
        self.episode_rewards = []

        # éš¨æ©Ÿé¸æ“‡ Episodeï¼ˆè¨“ç·´æ¨¡å¼ï¼‰æˆ–å¾é ­é–‹å§‹ï¼ˆè©•ä¼°æ¨¡å¼ï¼‰
        if self.mode == 'train' and seed is None:
            self.current_episode_idx = np.random.randint(0, len(self.episodes))
        else:
            self.current_episode_idx = 0

        # è¼‰å…¥ç•¶å‰ Episode
        if len(self.episodes) > 0:
            self.current_episode = self.episodes[self.current_episode_idx]
        else:
            self.current_episode = None

        # ç²å–åˆå§‹ç‹€æ…‹
        state = self._get_state()
        info = {
            'episode_idx': self.current_episode_idx,
            'time_step': self.current_time_step,
            'satellite_id': self.current_episode.satellite_id if self.current_episode else None
        }

        return state, info

    def step(self, action: int) -> Tuple[np.ndarray, float, bool, bool, Dict]:
        """
        åŸ·è¡Œä¸€æ­¥

        Args:
            action: 0=maintain, 1=handover

        Returns:
            observation: ä¸‹ä¸€ç‹€æ…‹
            reward: çå‹µ
            terminated: æ˜¯å¦çµæŸï¼ˆepisode å®Œæˆï¼‰
            truncated: æ˜¯å¦æˆªæ–·ï¼ˆè¶…æ™‚ï¼‰
            info: é¡å¤–ä¿¡æ¯
        """
        # æª¢æŸ¥ Episode æ˜¯å¦çµæŸ
        if self.current_episode is None or self.current_time_step >= len(self.current_episode.time_points):
            # Episode çµæŸ
            terminated = True
            truncated = False
            reward = 0.0
            next_state = self._get_state()
            info = {
                'episode_idx': self.current_episode_idx,
                'time_step': self.current_time_step,
                'total_handovers': self.total_handovers,
                'episode_reward': sum(self.episode_rewards)
            }
            return next_state, reward, terminated, truncated, info

        # ç²å–ç•¶å‰æ™‚é–“é»
        current_time_point = self.current_episode.time_points[self.current_time_step]

        # è¨ˆç®—çå‹µ
        reward = self._calculate_reward(action, current_time_point)
        self.episode_rewards.append(reward)

        # æ›´æ–°ç‹€æ…‹
        if action == 1:  # handover
            self.total_handovers += 1
            self.last_handover_time = self.current_time_step

        # ç§»å‹•åˆ°ä¸‹ä¸€æ™‚é–“æ­¥
        self.current_time_step += 1

        # ç²å–ä¸‹ä¸€ç‹€æ…‹
        next_state = self._get_state()

        # æª¢æŸ¥æ˜¯å¦çµæŸ
        terminated = self.current_time_step >= len(self.current_episode.time_points)
        truncated = False

        info = {
            'episode_idx': self.current_episode_idx,
            'time_step': self.current_time_step,
            'total_handovers': self.total_handovers,
            'action_taken': action,
            'reward_components': self._get_reward_components(action, current_time_point)
        }

        return next_state, reward, terminated, truncated, info

    def _get_state(self) -> np.ndarray:
        """
        ç²å–ç•¶å‰ç‹€æ…‹ (12 ç¶­å®Œæ•´ç‰¹å¾µ)

        Returns:
            state: [rsrp, rsrq, sinr, distance, elevation, doppler, velocity,
                    atmospheric_loss, path_loss, propagation_delay, offset_mo, cell_offset]
        """
        # å¦‚æœ Episode çµæŸæˆ–ç„¡æ•ˆï¼Œè¿”å›é›¶ç‹€æ…‹
        if self.current_episode is None or self.current_time_step >= len(self.current_episode.time_points):
            return np.zeros(12, dtype=np.float32)

        # ç²å–ç•¶å‰æ™‚é–“é»çš„ç‰¹å¾µ
        time_point = self.current_episode.time_points[self.current_time_step]

        # æå– 12 ç¶­ç‰¹å¾µ
        state = np.array([
            time_point.get('rsrp_dbm', -999.0),
            time_point.get('rsrq_db', -999.0),
            time_point.get('rs_sinr_db', -999.0),
            time_point.get('distance_km', -999.0),
            time_point.get('elevation_deg', -999.0),
            time_point.get('doppler_shift_hz', 0.0),
            time_point.get('radial_velocity_ms', 0.0),
            time_point.get('atmospheric_loss_db', 0.0),
            time_point.get('path_loss_db', 0.0),
            time_point.get('propagation_delay_ms', 0.0),
            time_point.get('offset_mo_db', 0.0),
            time_point.get('cell_offset_db', 0.0),
        ], dtype=np.float32)

        # è™•ç†ç„¡æ•ˆå€¼ï¼ˆç”¨é‚Šç•Œå€¼æ›¿æ›ï¼‰
        state = np.nan_to_num(state, nan=-999.0, posinf=999.0, neginf=-999.0)

        return state

    def _calculate_reward(self, action: int, time_point: Dict) -> float:
        """
        è¨ˆç®—çå‹µ

        Reward = w1 * QoS_improvement
                - w2 * handover_penalty
                + w3 * signal_quality
                - w4 * ping_pong_penalty

        Args:
            action: åŸ·è¡Œçš„å‹•ä½œ
            time_point: ç•¶å‰æ™‚é–“é»ç‰¹å¾µ

        Returns:
            reward: ç¸½çå‹µ
        """
        # çµ„ä»¶ 1: QoS æ”¹å–„ï¼ˆåŸºæ–¼ RSRPï¼‰
        qos_improvement = 0.0
        if action == 1:  # handover
            rsrp = time_point.get('rsrp_dbm', -999.0)
            # å‡è¨­æ›æ‰‹å¾Œå¯æ”¹å–„ RSRPï¼ˆç°¡åŒ–æ¨¡å‹ï¼Œå¯¦éš›æ‡‰æ¯”è¼ƒé„°å±…è¡›æ˜Ÿï¼‰
            # é€™è£¡ä½¿ç”¨ç•¶å‰ RSRP ä½œç‚ºåŸºæº–
            if rsrp != -999.0:
                # æ­£è¦åŒ–åˆ° [-1, 1]
                qos_improvement = (rsrp + 90.0) / 60.0  # å‡è¨­ç¯„åœ -150 to -30
                qos_improvement = np.clip(qos_improvement, -1.0, 1.0)

        # çµ„ä»¶ 2: æ›æ‰‹æ‡²ç½°
        handover_penalty = 1.0 if action == 1 else 0.0

        # çµ„ä»¶ 3: ä¿¡è™Ÿå“è³ªçå‹µï¼ˆåŸºæ–¼ RSRP é–€æª»ï¼‰
        signal_quality = 0.0
        rsrp = time_point.get('rsrp_dbm', -999.0)
        if rsrp != -999.0:
            # SOURCE: 3GPP TS 38.133 v18.3.0 Table 10.1.19.2-1
            if rsrp > -90:  # è‰¯å¥½ä¿¡è™Ÿ
                signal_quality = 0.5
            elif rsrp < -110:  # å·®ä¿¡è™Ÿ
                signal_quality = -0.5

        # çµ„ä»¶ 4: Ping-Pong æ‡²ç½°ï¼ˆçŸ­æ™‚é–“å…§é€£çºŒæ›æ‰‹ï¼‰
        ping_pong_penalty = 0.0
        if action == 1 and self.last_handover_time >= 0:
            # SOURCE: 3GPP TS 36.839 v11.1.0 Section 6.2.3.2 (TTT = 1s typical)
            time_since_last_handover = self.current_time_step - self.last_handover_time
            if time_since_last_handover < 10:  # < 10 time steps (~50s at 5s interval)
                ping_pong_penalty = 1.0

        # ç¸½çå‹µ
        total_reward = (
            self.w_qos * qos_improvement
            - self.w_handover * handover_penalty
            + self.w_signal * signal_quality
            - self.w_ping_pong * ping_pong_penalty
        )

        return total_reward

    def _get_reward_components(self, action: int, time_point: Dict) -> Dict:
        """ç²å–çå‹µçµ„ä»¶ï¼ˆç”¨æ–¼åˆ†æï¼‰"""
        qos = 0.0
        handover_pen = 0.0
        signal_qual = 0.0
        ping_pong_pen = 0.0

        rsrp = time_point.get('rsrp_dbm', -999.0)

        if action == 1:
            if rsrp != -999.0:
                qos = (rsrp + 90.0) / 60.0
                qos = np.clip(qos, -1.0, 1.0)
            handover_pen = 1.0

        if rsrp > -90:
            signal_qual = 0.5
        elif rsrp < -110:
            signal_qual = -0.5

        if action == 1 and self.last_handover_time >= 0:
            time_since_last = self.current_time_step - self.last_handover_time
            if time_since_last < 10:
                ping_pong_pen = 1.0

        return {
            'qos_improvement': qos,
            'handover_penalty': handover_pen,
            'signal_quality': signal_qual,
            'ping_pong_penalty': ping_pong_pen
        }

    def render(self):
        """æ¸²æŸ“ç’°å¢ƒï¼ˆå¯é¸ï¼‰"""
        if self.current_episode and self.current_time_step < len(self.current_episode.time_points):
            time_point = self.current_episode.time_points[self.current_time_step]
            rsrp = time_point.get('rsrp_dbm', -999.0)
            elevation = time_point.get('elevation_deg', -999.0)
            distance = time_point.get('distance_km', -999.0)

            print(f"Episode {self.current_episode_idx} Step {self.current_time_step}: "
                  f"Satellite={self.current_episode.satellite_id}, "
                  f"RSRP={rsrp:.2f} dBm, "
                  f"Elevation={elevation:.1f}Â°, "
                  f"Distance={distance:.1f} km, "
                  f"Handovers={self.total_handovers}")


def test_environment():
    """æ¸¬è©¦ç’°å¢ƒæ­£ç¢ºæ€§"""
    import pickle

    print("=" * 70)
    print("Phase 3: RL ç’°å¢ƒé©—è­‰")
    print("=" * 70)

    # è¼‰å…¥é…ç½®
    print("\nğŸ“¥ è¼‰å…¥é…ç½®...")
    with open("config/rl_config.yaml", 'r') as f:
        config = yaml.safe_load(f)

    # è¼‰å…¥è¨“ç·´æ•¸æ“šï¼ˆEpisode æ ¼å¼ï¼‰
    print("ğŸ“¥ è¼‰å…¥è¨“ç·´ Episodes...")
    data_path = Path("data")

    try:
        with open(data_path / "train_episodes.pkl", 'rb') as f:
            train_episodes_raw = pickle.load(f)
        print(f"   è¨“ç·´ Episodes: {len(train_episodes_raw)}")

        # è½‰æ›ç‚º Episode å°è±¡ï¼ˆå¦‚æœæ˜¯å­—å…¸åˆ—è¡¨ï¼‰
        if len(train_episodes_raw) > 0:
            if isinstance(train_episodes_raw[0], dict):
                train_episodes = [Episode(ep) for ep in train_episodes_raw]
                print(f"   âœ… å·²è½‰æ›ç‚º Episode å°è±¡")
            else:
                train_episodes = train_episodes_raw

            # é¡¯ç¤ºç¬¬ä¸€å€‹ Episode ä¿¡æ¯
            first_ep = train_episodes[0]
            print(f"   ç¬¬ä¸€å€‹ Episode: è¡›æ˜Ÿ {first_ep.satellite_id}, "
                  f"{len(first_ep.time_points)} æ™‚é–“é»")
    except FileNotFoundError:
        print("   âŒ æ‰¾ä¸åˆ° train_episodes.pkl")
        print("   è«‹å…ˆé‹è¡Œ phase1_data_loader_v2.py ç”Ÿæˆæ•¸æ“š")
        return

    # å‰µå»ºç’°å¢ƒ
    print("\nğŸ”¨ å‰µå»º RL ç’°å¢ƒ...")
    env = HandoverEnvironment(train_episodes, config, mode='train')
    print(f"   âœ… ç’°å¢ƒå‰µå»ºæˆåŠŸ")
    print(f"   ç‹€æ…‹ç©ºé–“: {env.observation_space}")
    print(f"   å‹•ä½œç©ºé–“: {env.action_space}")

    # æ¸¬è©¦ç’°å¢ƒé‡ç½®
    print("\nğŸ§ª æ¸¬è©¦ç’°å¢ƒé‡ç½®...")
    state, info = env.reset()
    print(f"   åˆå§‹ç‹€æ…‹å½¢ç‹€: {state.shape}")
    print(f"   åˆå§‹ç‹€æ…‹å‰ 3 ç¶­ (RSRP/RSRQ/SINR): {state[:3]}")
    assert state.shape == (12,), f"ç‹€æ…‹ç¶­åº¦éŒ¯èª¤: æœŸæœ› (12,), å¯¦éš› {state.shape}"
    print(f"   âœ… é‡ç½®æ¸¬è©¦é€šé")

    # æ¸¬è©¦å‹•ä½œåŸ·è¡Œ
    print("\nğŸ§ª æ¸¬è©¦å‹•ä½œåŸ·è¡Œ...")
    actions = [0, 1, 0, 1, 0]  # maintain, handover, maintain, handover, maintain
    for i, action in enumerate(actions):
        next_state, reward, terminated, truncated, info = env.step(action)
        print(f"   Step {i+1}: action={action}, reward={reward:.3f}, "
              f"handovers={info['total_handovers']}")

        assert next_state.shape == (12,), f"ç‹€æ…‹ç¶­åº¦éŒ¯èª¤: {next_state.shape}"
        assert isinstance(reward, (int, float)), "çå‹µé¡å‹éŒ¯èª¤"

    print(f"   âœ… å‹•ä½œåŸ·è¡Œæ¸¬è©¦é€šé")

    # æ¸¬è©¦éš¨æ©Ÿç­–ç•¥
    print("\nğŸ² æ¸¬è©¦éš¨æ©Ÿç­–ç•¥ï¼ˆ100 stepsï¼‰...")
    env.reset()
    total_reward = 0
    handover_count = 0

    for step in range(100):
        action = env.action_space.sample()  # éš¨æ©Ÿå‹•ä½œ
        next_state, reward, terminated, truncated, info = env.step(action)
        total_reward += reward

        if action == 1:
            handover_count += 1

        if terminated:
            print(f"   Episode åœ¨ {step+1} æ­¥çµæŸ")
            break

    print(f"   ç¸½çå‹µ: {total_reward:.3f}")
    print(f"   æ›æ‰‹æ¬¡æ•¸: {handover_count}")
    avg_reward = total_reward / (step + 1) if step > 0 else 0
    print(f"   å¹³å‡çå‹µ: {avg_reward:.3f}")
    print(f"   âœ… éš¨æ©Ÿç­–ç•¥æ¸¬è©¦é€šé")

    # å®Œæ•´ episode æ¸¬è©¦
    print("\nğŸ¯ æ¸¬è©¦å®Œæ•´ Episode...")
    if len(train_episodes) > 0:
        test_episodes = train_episodes[:3]  # æ¸¬è©¦å‰ 3 å€‹ episodes
        env = HandoverEnvironment(test_episodes, config, mode='eval')
        state, info = env.reset()

        episode_reward = 0
        steps = 0

        while True:
            action = env.action_space.sample()
            next_state, reward, terminated, truncated, info = env.step(action)
            episode_reward += reward
            steps += 1

            if terminated or truncated:
                break

        print(f"   Episode å®Œæˆ")
        print(f"   ç¸½æ­¥æ•¸: {steps}")
        print(f"   ç¸½çå‹µ: {episode_reward:.3f}")
        print(f"   ç¸½æ›æ‰‹æ¬¡æ•¸: {info['total_handovers']}")
        print(f"   âœ… Episode æ¸¬è©¦é€šé")

    print("\n" + "=" * 70)
    print("âœ… Phase 3 å®Œæˆï¼ç’°å¢ƒé©—è­‰é€šé")
    print("=" * 70)
    print("\nä¸‹ä¸€æ­¥: é‹è¡Œ Phase 4 (RL è¨“ç·´)")
    print("  python phase4_rl_training.py")


if __name__ == "__main__":
    test_environment()
