#!/usr/bin/env python3
"""
Phase 3: RL ç’°å¢ƒè¨­è¨ˆèˆ‡é©—è­‰ï¼ˆâœ… ä½¿ç”¨çœŸå¯¦é„°å±…æ•¸æ“š - ç„¡ç°¡åŒ–å‡è¨­ï¼‰

âœ¨ é—œéµæ”¹é€²ï¼š
1. âœ… çå‹µå‡½æ•¸ä½¿ç”¨çœŸå¯¦é„°å±… RSRP æ¯”è¼ƒï¼ˆæ¶ˆé™¤"å‡è¨­æ›æ‰‹å¾Œå¯æ”¹å–„"çš„ç°¡åŒ–ï¼‰
2. âœ… åŸºæ–¼æ™‚é–“æˆ³ç´¢å¼•æŸ¥æ‰¾åŒæ™‚å¯è¦‹çš„çœŸå¯¦é„°å±…è¡›æ˜Ÿï¼ˆ10-15 é¡†ï¼‰
3. âœ… å®Œå…¨åŸºæ–¼ Stage 5 çœŸå¯¦è§€æ¸¬æ•¸æ“šï¼Œç„¡ä¼°ç®—æˆ–æ¨¡æ“¬

åŠŸèƒ½ï¼š
1. å¯¦ä½œ Gymnasium ç’°å¢ƒï¼ˆç¬¦åˆ OpenAI Gym æ¨™æº–ï¼‰
2. å®šç¾©ç‹€æ…‹ç©ºé–“ã€å‹•ä½œç©ºé–“ã€âœ… çœŸå¯¦é„°å±…çå‹µå‡½æ•¸
3. é©—è­‰ç’°å¢ƒæ­£ç¢ºæ€§
4. æ¸¬è©¦éš¨æ©Ÿç­–ç•¥æ€§èƒ½

åŸ·è¡Œï¼š
    python phase3_rl_environment.py

è¼¸å‡ºï¼š
    é©—è­‰ç’°å¢ƒæ­£ç¢ºæ€§
    æ¸¬è©¦éš¨æ©Ÿç­–ç•¥ baseline

SOURCE:
- Stage 5 signal_analysis çœŸå¯¦ RSRP æ•¸æ“š
- 3GPP TS 38.331 v18.5.1 Section 5.5.4.4 (A3 event)
"""

import json
import yaml
import numpy as np
import gymnasium as gym
from gymnasium import spaces
from typing import Dict, List, Tuple, Optional
from pathlib import Path


class Episode:
    """
    Episode æ•¸æ“šåŒ…è£é¡ï¼ˆç›¸å®¹æ€§è™•ç†ï¼‰

    ç”¨æ–¼åŒ…è£å¾ pickle è¼‰å…¥çš„å­—å…¸æ•¸æ“šï¼Œæä¾›çµ±ä¸€çš„å±¬æ€§è¨ªå•æ¥å£ã€‚

    ç›¸å®¹æ€§è™•ç†ï¼š
    - time_series (Phase 1 ä¿å­˜çš„éµå) â†’ time_points (Phase 3 æœŸæœ›çš„å±¬æ€§å)
    - å­—å…¸æ ¼å¼ â†’ å°è±¡å±¬æ€§è¨ªå•

    SOURCE: rl.md Line 242-267 (Episode åŒ…è£é¡è¨­è¨ˆ)
    """

    def __init__(self, data):
        """
        Args:
            data: Episode æ•¸æ“šï¼ˆå­—å…¸æˆ– Episode å°è±¡ï¼‰
        """
        # å¦‚æœå·²ç¶“æ˜¯ Episode å°è±¡ï¼Œç›´æ¥è¤‡è£½å±¬æ€§
        if isinstance(data, Episode):
            self.satellite_id = data.satellite_id
            self.constellation = data.constellation
            self.time_points = data.time_points
            self.gpp_events = data.gpp_events
            self.episode_length = data.episode_length
            return

        # å¾å­—å…¸å‰µå»ºå°è±¡
        self.satellite_id = data['satellite_id']
        self.constellation = data['constellation']

        # ç›¸å®¹æ€§è™•ç†ï¼šæ”¯æŒ time_seriesï¼ˆPhase 1ï¼‰å’Œ time_pointsï¼ˆæœªä¾†å¯èƒ½ï¼‰
        if 'time_points' in data:
            self.time_points = data['time_points']
        elif 'time_series' in data:
            self.time_points = data['time_series']  # â† é—œéµè½‰æ›
        else:
            self.time_points = []

        self.gpp_events = data.get('gpp_events', [])
        self.episode_length = data.get('episode_length', len(self.time_points))


class HandoverEnvironment(gym.Env):
    """
    è¡›æ˜Ÿæ›æ‰‹æ±ºç­– RL ç’°å¢ƒ

    ç‹€æ…‹ç©ºé–“ (12 ç¶­ - å®Œæ•´ç‰¹å¾µé›†):
        ä¿¡è™Ÿå“è³ª (3 ç¶­):
            - RSRP (dBm): Reference Signal Received Power
            - RSRQ (dB): Reference Signal Received Quality
            - SINR (dB): Signal-to-Interference-plus-Noise Ratio

        ç‰©ç†åƒæ•¸ (7 ç¶­):
            - Distance (km): è¡›æ˜Ÿèˆ‡åœ°é¢ç«™è·é›¢
            - Elevation (deg): ä»°è§’
            - Doppler Shift (Hz): éƒ½åœå‹’é »ç§»
            - Radial Velocity (m/s): å¾‘å‘é€Ÿåº¦
            - Atmospheric Loss (dB): å¤§æ°£è¡°æ¸›
            - Path Loss (dB): è·¯å¾‘æè€—
            - Propagation Delay (ms): å‚³æ’­å»¶é²

        3GPP åç§»é‡ (2 ç¶­):
            - Offset MO (dB): Measurement offset
            - Cell Offset (dB): Cell-specific offset

    å‹•ä½œç©ºé–“ (2 å€‹é›¢æ•£å‹•ä½œ):
        0: maintain (ä¿æŒç•¶å‰æœå‹™è¡›æ˜Ÿ)
        1: handover (æ›æ‰‹åˆ°æœ€ä½³é„°å±…)

    çå‹µå‡½æ•¸:
        reward = w1 * qos_improvement
                - w2 * handover_penalty
                + w3 * signal_quality
                - w4 * ping_pong_penalty

    SOURCE:
    - 3GPP TS 38.331 v18.5.1 (äº‹ä»¶å®šç¾©)
    - 3GPP TS 38.215 v18.1.0 Section 5.1 (ä¿¡è™Ÿå“è³ªæ¸¬é‡)
    - docs/final.md Line 139 (çå‹µå‡½æ•¸è¨­è¨ˆ)
    """

    metadata = {'render_modes': ['human']}

    def __init__(self,
                 episodes: List,  # List[Episode] or List[Dict] from phase1_data_loader_v2.py
                 config: Dict,
                 timestamp_index: Dict = None,  # âœ… æ–°å¢ï¼šç”¨æ–¼çœŸå¯¦é„°å±…æŸ¥æ‰¾
                 mode: str = 'train'):
        """
        Args:
            episodes: æ›æ‰‹æ±ºç­– Episode åˆ—è¡¨ï¼ˆä¾†è‡ª phase1_data_loader_v2.pyï¼‰
                     å¯ä»¥æ˜¯å­—å…¸åˆ—è¡¨æˆ– Episode å°è±¡åˆ—è¡¨
            config: RL é…ç½®
            timestamp_index: âœ… æ™‚é–“æˆ³ç´¢å¼• - ç”¨æ–¼çœŸå¯¦é„°å±… RSRP æŸ¥æ‰¾
                            æ ¼å¼: {timestamp: {sat_id: features}}
            mode: 'train' or 'eval'
        """
        super().__init__()

        # ç›¸å®¹æ€§è™•ç†ï¼šå°‡å­—å…¸åˆ—è¡¨è½‰æ›ç‚º Episode å°è±¡åˆ—è¡¨
        # SOURCE: rl.md Line 268-279 (ç›¸å®¹æ€§è½‰æ›é‚è¼¯)
        if episodes and len(episodes) > 0 and isinstance(episodes[0], dict):
            self.episodes = [Episode(ep) for ep in episodes]
            print(f"   âœ… å·²è½‰æ› {len(episodes)} å€‹å­—å…¸ç‚º Episode å°è±¡")
        else:
            self.episodes = episodes

        self.config = config
        self.mode = mode

        # âœ… æ™‚é–“æˆ³ç´¢å¼• - ç”¨æ–¼çœŸå¯¦é„°å±…æŸ¥æ‰¾ï¼ˆæ¶ˆé™¤ç°¡åŒ–å‡è¨­ï¼‰
        self.timestamp_index = timestamp_index if timestamp_index is not None else {}
        if self.timestamp_index:
            print(f"   âœ… å·²è¼‰å…¥æ™‚é–“æˆ³ç´¢å¼•ï¼ˆ{len(self.timestamp_index)} å€‹æ™‚é–“æˆ³ï¼‰")

        # ç‹€æ…‹ç©ºé–“ï¼šBox(12,) - å®Œæ•´ç‰¹å¾µé›†
        # [rsrp, rsrq, sinr, distance, elevation, doppler, velocity,
        #  atmospheric_loss, path_loss, propagation_delay, offset_mo, cell_offset]
        self.observation_space = spaces.Box(
            low=np.array([
                -150.0,  # rsrp_dbm (SOURCE: 3GPP TS 38.215 v18.1.0 Table 5.1.1-1)
                -30.0,   # rsrq_db (SOURCE: 3GPP TS 38.215 v18.1.0 Table 5.1.3-1)
                -10.0,   # rs_sinr_db (SOURCE: 3GPP TS 38.215 v18.1.0 Table 5.1.5-1)
                500.0,   # distance_km (SOURCE: LEO orbit altitude range)
                0.0,     # elevation_deg (SOURCE: 3GPP TR 38.821 Section 6.1.2)
                -50000.0,  # doppler_shift_hz (SOURCE: ITU-R M.1184 Annex 1)
                -8000.0,   # radial_velocity_ms (SOURCE: LEO orbital velocity)
                0.0,     # atmospheric_loss_db (SOURCE: ITU-R P.676-13)
                120.0,   # path_loss_db (SOURCE: 3GPP TR 38.811 Section 6.6.3)
                0.0,     # propagation_delay_ms
                -10.0,   # offset_mo_db (SOURCE: 3GPP TS 38.331 Section 5.5.4)
                -10.0    # cell_offset_db (SOURCE: 3GPP TS 38.331 Section 5.5.4)
            ], dtype=np.float32),
            high=np.array([
                -30.0,   # rsrp_dbm
                -3.0,    # rsrq_db
                30.0,    # rs_sinr_db
                3000.0,  # distance_km
                90.0,    # elevation_deg
                50000.0,   # doppler_shift_hz
                8000.0,    # radial_velocity_ms
                30.0,    # atmospheric_loss_db
                200.0,   # path_loss_db
                50.0,    # propagation_delay_ms
                10.0,    # offset_mo_db
                10.0     # cell_offset_db
            ], dtype=np.float32),
            dtype=np.float32
        )

        # å‹•ä½œç©ºé–“ï¼šDiscrete(2)
        # 0=maintain, 1=handover
        self.action_space = spaces.Discrete(2)

        # çå‹µæ¬Šé‡ (with SOURCE comments)
        reward_config = config['environment']['reward_weights']
        self.w_qos = reward_config['qos_improvement']        # SOURCE: Zhang et al. 2021 (RL handover weight)
        self.w_handover = reward_config['handover_penalty']  # SOURCE: 3GPP TS 36.839 v11.1.0 Section 6.2.3
        self.w_signal = reward_config['signal_quality']      # SOURCE: Chen et al. 2020 (QoS weight)
        self.w_ping_pong = reward_config['ping_pong_penalty']  # SOURCE: 3GPP TS 36.839 v11.1.0 Section 6.2.3

        # ç’°å¢ƒç‹€æ…‹ (Episode-based)
        self.current_episode_idx = 0
        self.current_time_step = 0
        self.total_handovers = 0
        self.last_handover_time = -1
        self.episode_rewards = []

        # ç•¶å‰ Episode
        self.current_episode = None
        self.current_satellite_id = None  # âœ… ç”¨æ–¼çœŸå¯¦é„°å±…æŸ¥æ‰¾æ™‚æ’é™¤è‡ªå·±

        # é‡ç½®ç’°å¢ƒ
        self.reset()

    def reset(self, seed: Optional[int] = None, options: Optional[Dict] = None) -> Tuple[np.ndarray, Dict]:
        """é‡ç½®ç’°å¢ƒï¼ˆé–‹å§‹æ–° Episodeï¼‰"""
        super().reset(seed=seed)

        self.current_time_step = 0
        self.total_handovers = 0
        self.last_handover_time = -1
        self.episode_rewards = []

        # éš¨æ©Ÿé¸æ“‡ Episodeï¼ˆè¨“ç·´æ¨¡å¼ï¼‰æˆ–å¾é ­é–‹å§‹ï¼ˆè©•ä¼°æ¨¡å¼ï¼‰
        if self.mode == 'train' and seed is None:
            self.current_episode_idx = np.random.randint(0, len(self.episodes))
        else:
            self.current_episode_idx = 0

        # è¼‰å…¥ç•¶å‰ Episode
        if len(self.episodes) > 0:
            self.current_episode = self.episodes[self.current_episode_idx]
            self.current_satellite_id = self.current_episode.satellite_id  # âœ… è¨­ç½®ç•¶å‰æœå‹™è¡›æ˜Ÿ ID
        else:
            self.current_episode = None
            self.current_satellite_id = None

        # ç²å–åˆå§‹ç‹€æ…‹
        state = self._get_state()
        info = {
            'episode_idx': self.current_episode_idx,
            'time_step': self.current_time_step,
            'satellite_id': self.current_satellite_id
        }

        return state, info

    def step(self, action: int) -> Tuple[np.ndarray, float, bool, bool, Dict]:
        """
        åŸ·è¡Œä¸€æ­¥

        Args:
            action: 0=maintain, 1=handover

        Returns:
            observation: ä¸‹ä¸€ç‹€æ…‹
            reward: çå‹µ
            terminated: æ˜¯å¦çµæŸï¼ˆepisode å®Œæˆï¼‰
            truncated: æ˜¯å¦æˆªæ–·ï¼ˆè¶…æ™‚ï¼‰
            info: é¡å¤–ä¿¡æ¯
        """
        # æª¢æŸ¥ Episode æ˜¯å¦çµæŸ
        if self.current_episode is None or self.current_time_step >= len(self.current_episode.time_points):
            # Episode çµæŸ
            terminated = True
            truncated = False
            reward = 0.0
            next_state = self._get_state()
            info = {
                'episode_idx': self.current_episode_idx,
                'time_step': self.current_time_step,
                'total_handovers': self.total_handovers,
                'episode_reward': sum(self.episode_rewards)
            }
            return next_state, reward, terminated, truncated, info

        # ç²å–ç•¶å‰æ™‚é–“é»
        current_time_point = self.current_episode.time_points[self.current_time_step]

        # è¨ˆç®—çå‹µ
        reward = self._calculate_reward(action, current_time_point)
        self.episode_rewards.append(reward)

        # æ›´æ–°ç‹€æ…‹
        if action == 1:  # handover
            self.total_handovers += 1
            self.last_handover_time = self.current_time_step

        # ç§»å‹•åˆ°ä¸‹ä¸€æ™‚é–“æ­¥
        self.current_time_step += 1

        # ç²å–ä¸‹ä¸€ç‹€æ…‹
        next_state = self._get_state()

        # æª¢æŸ¥æ˜¯å¦çµæŸ
        terminated = self.current_time_step >= len(self.current_episode.time_points)
        truncated = False

        info = {
            'episode_idx': self.current_episode_idx,
            'time_step': self.current_time_step,
            'total_handovers': self.total_handovers,
            'action_taken': action,
            'reward_components': self._get_reward_components(action, current_time_point)
        }

        return next_state, reward, terminated, truncated, info

    def _get_state(self) -> np.ndarray:
        """
        ç²å–ç•¶å‰ç‹€æ…‹ (12 ç¶­å®Œæ•´ç‰¹å¾µ)

        Returns:
            state: [rsrp, rsrq, sinr, distance, elevation, doppler, velocity,
                    atmospheric_loss, path_loss, propagation_delay, offset_mo, cell_offset]
        """
        # å¦‚æœ Episode çµæŸæˆ–ç„¡æ•ˆï¼Œè¿”å›é›¶ç‹€æ…‹
        if self.current_episode is None or self.current_time_step >= len(self.current_episode.time_points):
            return np.zeros(12, dtype=np.float32)

        # ç²å–ç•¶å‰æ™‚é–“é»çš„ç‰¹å¾µ
        time_point = self.current_episode.time_points[self.current_time_step]

        # æå– 12 ç¶­ç‰¹å¾µ
        state = np.array([
            time_point.get('rsrp_dbm', -999.0),
            time_point.get('rsrq_db', -999.0),
            time_point.get('rs_sinr_db', -999.0),
            time_point.get('distance_km', -999.0),
            time_point.get('elevation_deg', -999.0),
            time_point.get('doppler_shift_hz', 0.0),
            time_point.get('radial_velocity_ms', 0.0),
            time_point.get('atmospheric_loss_db', 0.0),
            time_point.get('path_loss_db', 0.0),
            time_point.get('propagation_delay_ms', 0.0),
            time_point.get('offset_mo_db', 0.0),
            time_point.get('cell_offset_db', 0.0),
        ], dtype=np.float32)

        # è™•ç†ç„¡æ•ˆå€¼ï¼ˆç”¨é‚Šç•Œå€¼æ›¿æ›ï¼‰
        state = np.nan_to_num(state, nan=-999.0, posinf=999.0, neginf=-999.0)

        return state

    def _calculate_reward(self, action: int, time_point: Dict) -> float:
        """
        è¨ˆç®—çå‹µ - âœ… ä½¿ç”¨çœŸå¯¦é„°å±…è¡›æ˜Ÿæ¯”è¼ƒï¼ˆç„¡ç°¡åŒ–å‡è¨­ï¼‰

        Reward = w1 * QoS_improvement
                - w2 * handover_penalty
                + w3 * signal_quality
                - w4 * ping_pong_penalty

        âœ… æ¶ˆé™¤ç°¡åŒ–å‡è¨­çš„é—œéµæ”¹é€²ï¼š
        - å¾æ™‚é–“æˆ³ç´¢å¼•ä¸­æ‰¾åˆ°çœŸå¯¦é„°å±…è¡›æ˜Ÿ
        - åŸºæ–¼çœŸå¯¦ RSRP å·®ç•°è¨ˆç®— QoS æ”¹å–„
        - å®Œå…¨åŸºæ–¼ Stage 5 çœŸå¯¦è§€æ¸¬æ•¸æ“š

        SOURCE:
        - 3GPP TS 38.331 v18.5.1 Section 5.5.4.4 (A3 event: neighbour better than serving)
        - Stage 5 signal_analysis çœŸå¯¦ RSRP æ•¸æ“š

        Args:
            action: åŸ·è¡Œçš„å‹•ä½œ
            time_point: ç•¶å‰æ™‚é–“é»ç‰¹å¾µ

        Returns:
            reward: ç¸½çå‹µ
        """
        # ç²å–ç•¶å‰æœå‹™è¡›æ˜Ÿ RSRP
        serving_rsrp = time_point.get('rsrp_dbm', -999.0)
        timestamp = time_point.get('timestamp', '')

        # çµ„ä»¶ 1: QoS æ”¹å–„ï¼ˆâœ… åŸºæ–¼çœŸå¯¦é„°å±… RSRP æ¯”è¼ƒï¼‰
        qos_improvement = 0.0
        if action == 1:  # handover
            # âœ… å¾æ™‚é–“æˆ³ç´¢å¼•æ‰¾åˆ°çœŸå¯¦é„°å±…
            neighbors = self.timestamp_index.get(timestamp, {})

            # æ‰¾åˆ°æœ€ä½³é„°å±…ï¼ˆRSRP æœ€é«˜ä¸”å„ªæ–¼æœå‹™è¡›æ˜Ÿï¼‰
            best_neighbor_rsrp = -999.0
            for neighbor_id, neighbor_data in neighbors.items():
                # æ’é™¤ç•¶å‰æœå‹™è¡›æ˜Ÿè‡ªå·±
                if neighbor_id == self.current_satellite_id:
                    continue

                neighbor_rsrp = neighbor_data.get('rsrp_dbm', -999.0)
                if neighbor_rsrp > best_neighbor_rsrp:
                    best_neighbor_rsrp = neighbor_rsrp

            # âœ… è¨ˆç®—çœŸå¯¦çš„ QoS æ”¹å–„ï¼ˆåŸºæ–¼é„°å±…å’Œæœå‹™è¡›æ˜Ÿ RSRP å·®ç•°ï¼‰
            if best_neighbor_rsrp != -999.0 and serving_rsrp != -999.0:
                # RSRP å·®ç•°ï¼ˆæ­£å€¼è¡¨ç¤ºé„°å±…æ›´å¥½ï¼Œè² å€¼è¡¨ç¤ºæœå‹™è¡›æ˜Ÿæ›´å¥½ï¼‰
                rsrp_difference = best_neighbor_rsrp - serving_rsrp
                # æ­£è¦åŒ–åˆ° [-1, 1]ï¼ˆå‡è¨­æœ€å¤§å·®ç•° Â±60 dBï¼‰
                # SOURCE: 3GPP TS 38.215 v18.1.0 Table 5.1.1-1 (RSRP range: -156 to -31 dBm)
                qos_improvement = rsrp_difference / 60.0
                qos_improvement = np.clip(qos_improvement, -1.0, 1.0)
            else:
                # ç„¡æœ‰æ•ˆé„°å±…æ•¸æ“šæ™‚ï¼Œä¸çµ¦äºˆ QoS æ”¹å–„çå‹µ
                qos_improvement = 0.0

        # çµ„ä»¶ 2: æ›æ‰‹æ‡²ç½°
        handover_penalty = 1.0 if action == 1 else 0.0

        # çµ„ä»¶ 3: ä¿¡è™Ÿå“è³ªçå‹µï¼ˆåŸºæ–¼ RSRP é–€æª»ï¼‰
        signal_quality = 0.0
        if serving_rsrp != -999.0:
            # SOURCE: 3GPP TS 38.133 v18.3.0 Table 10.1.19.2-1
            if serving_rsrp > -90:  # è‰¯å¥½ä¿¡è™Ÿ
                signal_quality = 0.5
            elif serving_rsrp < -110:  # å·®ä¿¡è™Ÿ
                signal_quality = -0.5

        # çµ„ä»¶ 4: Ping-Pong æ‡²ç½°ï¼ˆçŸ­æ™‚é–“å…§é€£çºŒæ›æ‰‹ï¼‰
        ping_pong_penalty = 0.0
        if action == 1 and self.last_handover_time >= 0:
            # SOURCE: 3GPP TS 36.839 v11.1.0 Section 6.2.3.2 (TTT = 1s typical)
            time_since_last_handover = self.current_time_step - self.last_handover_time
            if time_since_last_handover < 10:  # < 10 time steps (~50s at 5s interval)
                ping_pong_penalty = 1.0

        # ç¸½çå‹µ
        total_reward = (
            self.w_qos * qos_improvement
            - self.w_handover * handover_penalty
            + self.w_signal * signal_quality
            - self.w_ping_pong * ping_pong_penalty
        )

        return total_reward

    def _get_reward_components(self, action: int, time_point: Dict) -> Dict:
        """
        ç²å–çå‹µçµ„ä»¶ï¼ˆç”¨æ–¼åˆ†æï¼‰ - âœ… ä½¿ç”¨çœŸå¯¦é„°å±…è¡›æ˜Ÿæ¯”è¼ƒ

        èˆ‡ _calculate_reward() ä¿æŒå®Œå…¨ä¸€è‡´
        """
        serving_rsrp = time_point.get('rsrp_dbm', -999.0)
        timestamp = time_point.get('timestamp', '')

        qos = 0.0
        handover_pen = 0.0
        signal_qual = 0.0
        ping_pong_pen = 0.0

        # âœ… çµ„ä»¶ 1: QoS æ”¹å–„ï¼ˆåŸºæ–¼çœŸå¯¦é„°å±… RSRP æ¯”è¼ƒï¼‰
        if action == 1:
            neighbors = self.timestamp_index.get(timestamp, {})
            best_neighbor_rsrp = -999.0

            for neighbor_id, neighbor_data in neighbors.items():
                if neighbor_id == self.current_satellite_id:
                    continue
                neighbor_rsrp = neighbor_data.get('rsrp_dbm', -999.0)
                if neighbor_rsrp > best_neighbor_rsrp:
                    best_neighbor_rsrp = neighbor_rsrp

            if best_neighbor_rsrp != -999.0 and serving_rsrp != -999.0:
                rsrp_difference = best_neighbor_rsrp - serving_rsrp
                qos = rsrp_difference / 60.0
                qos = np.clip(qos, -1.0, 1.0)

            handover_pen = 1.0

        # çµ„ä»¶ 3: ä¿¡è™Ÿå“è³ª
        if serving_rsrp > -90:
            signal_qual = 0.5
        elif serving_rsrp < -110:
            signal_qual = -0.5

        # çµ„ä»¶ 4: Ping-Pong
        if action == 1 and self.last_handover_time >= 0:
            time_since_last = self.current_time_step - self.last_handover_time
            if time_since_last < 10:
                ping_pong_pen = 1.0

        return {
            'qos_improvement': qos,
            'handover_penalty': handover_pen,
            'signal_quality': signal_qual,
            'ping_pong_penalty': ping_pong_pen
        }

    def render(self):
        """æ¸²æŸ“ç’°å¢ƒï¼ˆå¯é¸ï¼‰"""
        if self.current_episode and self.current_time_step < len(self.current_episode.time_points):
            time_point = self.current_episode.time_points[self.current_time_step]
            rsrp = time_point.get('rsrp_dbm', -999.0)
            elevation = time_point.get('elevation_deg', -999.0)
            distance = time_point.get('distance_km', -999.0)

            print(f"Episode {self.current_episode_idx} Step {self.current_time_step}: "
                  f"Satellite={self.current_episode.satellite_id}, "
                  f"RSRP={rsrp:.2f} dBm, "
                  f"Elevation={elevation:.1f}Â°, "
                  f"Distance={distance:.1f} km, "
                  f"Handovers={self.total_handovers}")


def test_environment():
    """æ¸¬è©¦ç’°å¢ƒæ­£ç¢ºæ€§"""
    import pickle

    print("=" * 70)
    print("Phase 3: RL ç’°å¢ƒé©—è­‰ï¼ˆâœ… ä½¿ç”¨çœŸå¯¦é„°å±…æ•¸æ“šï¼‰")
    print("=" * 70)

    # è¼‰å…¥é…ç½®
    print("\nğŸ“¥ è¼‰å…¥é…ç½®...")
    with open("config/rl_config.yaml", 'r') as f:
        config = yaml.safe_load(f)

    # è¼‰å…¥è¨“ç·´æ•¸æ“šï¼ˆEpisode æ ¼å¼ï¼‰
    print("ğŸ“¥ è¼‰å…¥è¨“ç·´ Episodes...")
    data_path = Path("data")

    try:
        with open(data_path / "train_episodes.pkl", 'rb') as f:
            train_episodes_raw = pickle.load(f)
        print(f"   è¨“ç·´ Episodes: {len(train_episodes_raw)}")

        # è½‰æ›ç‚º Episode å°è±¡ï¼ˆå¦‚æœæ˜¯å­—å…¸åˆ—è¡¨ï¼‰
        if len(train_episodes_raw) > 0:
            if isinstance(train_episodes_raw[0], dict):
                train_episodes = [Episode(ep) for ep in train_episodes_raw]
                print(f"   âœ… å·²è½‰æ›ç‚º Episode å°è±¡")
            else:
                train_episodes = train_episodes_raw

            # é¡¯ç¤ºç¬¬ä¸€å€‹ Episode ä¿¡æ¯
            first_ep = train_episodes[0]
            print(f"   ç¬¬ä¸€å€‹ Episode: è¡›æ˜Ÿ {first_ep.satellite_id}, "
                  f"{len(first_ep.time_points)} æ™‚é–“é»")
    except FileNotFoundError:
        print("   âŒ æ‰¾ä¸åˆ° train_episodes.pkl")
        print("   è«‹å…ˆé‹è¡Œ phase1_data_loader_v2.py ç”Ÿæˆæ•¸æ“š")
        return

    # âœ… è¼‰å…¥æ™‚é–“æˆ³ç´¢å¼•ï¼ˆç”¨æ–¼çœŸå¯¦é„°å±…æŸ¥æ‰¾ï¼‰
    print("\nğŸ“¥ è¼‰å…¥æ™‚é–“æˆ³ç´¢å¼•...")
    try:
        with open(data_path / "timestamp_index.pkl", 'rb') as f:
            timestamp_index = pickle.load(f)
        print(f"   âœ… æ™‚é–“æˆ³ç´¢å¼•: {len(timestamp_index)} å€‹æ™‚é–“æˆ³")
    except FileNotFoundError:
        print("   âš ï¸  æ‰¾ä¸åˆ° timestamp_index.pklï¼Œå°‡ä¸ä½¿ç”¨çœŸå¯¦é„°å±…æ¯”è¼ƒ")
        print("   è«‹é‡æ–°é‹è¡Œ phase1_data_loader_v2.py ç”Ÿæˆæ™‚é–“æˆ³ç´¢å¼•")
        timestamp_index = {}

    # å‰µå»ºç’°å¢ƒï¼ˆâœ… å‚³å…¥æ™‚é–“æˆ³ç´¢å¼•ï¼‰
    print("\nğŸ”¨ å‰µå»º RL ç’°å¢ƒ...")
    env = HandoverEnvironment(train_episodes, config, timestamp_index=timestamp_index, mode='train')
    print(f"   âœ… ç’°å¢ƒå‰µå»ºæˆåŠŸ")
    print(f"   ç‹€æ…‹ç©ºé–“: {env.observation_space}")
    print(f"   å‹•ä½œç©ºé–“: {env.action_space}")

    # æ¸¬è©¦ç’°å¢ƒé‡ç½®
    print("\nğŸ§ª æ¸¬è©¦ç’°å¢ƒé‡ç½®...")
    state, info = env.reset()
    print(f"   åˆå§‹ç‹€æ…‹å½¢ç‹€: {state.shape}")
    print(f"   åˆå§‹ç‹€æ…‹å‰ 3 ç¶­ (RSRP/RSRQ/SINR): {state[:3]}")
    assert state.shape == (12,), f"ç‹€æ…‹ç¶­åº¦éŒ¯èª¤: æœŸæœ› (12,), å¯¦éš› {state.shape}"
    print(f"   âœ… é‡ç½®æ¸¬è©¦é€šé")

    # æ¸¬è©¦å‹•ä½œåŸ·è¡Œ
    print("\nğŸ§ª æ¸¬è©¦å‹•ä½œåŸ·è¡Œ...")
    actions = [0, 1, 0, 1, 0]  # maintain, handover, maintain, handover, maintain
    for i, action in enumerate(actions):
        next_state, reward, terminated, truncated, info = env.step(action)
        print(f"   Step {i+1}: action={action}, reward={reward:.3f}, "
              f"handovers={info['total_handovers']}")

        assert next_state.shape == (12,), f"ç‹€æ…‹ç¶­åº¦éŒ¯èª¤: {next_state.shape}"
        assert isinstance(reward, (int, float)), "çå‹µé¡å‹éŒ¯èª¤"

    print(f"   âœ… å‹•ä½œåŸ·è¡Œæ¸¬è©¦é€šé")

    # æ¸¬è©¦éš¨æ©Ÿç­–ç•¥
    print("\nğŸ² æ¸¬è©¦éš¨æ©Ÿç­–ç•¥ï¼ˆ100 stepsï¼‰...")
    env.reset()
    total_reward = 0
    handover_count = 0

    for step in range(100):
        action = env.action_space.sample()  # éš¨æ©Ÿå‹•ä½œ
        next_state, reward, terminated, truncated, info = env.step(action)
        total_reward += reward

        if action == 1:
            handover_count += 1

        if terminated:
            print(f"   Episode åœ¨ {step+1} æ­¥çµæŸ")
            break

    print(f"   ç¸½çå‹µ: {total_reward:.3f}")
    print(f"   æ›æ‰‹æ¬¡æ•¸: {handover_count}")
    avg_reward = total_reward / (step + 1) if step > 0 else 0
    print(f"   å¹³å‡çå‹µ: {avg_reward:.3f}")
    print(f"   âœ… éš¨æ©Ÿç­–ç•¥æ¸¬è©¦é€šé")

    # å®Œæ•´ episode æ¸¬è©¦
    print("\nğŸ¯ æ¸¬è©¦å®Œæ•´ Episode...")
    if len(train_episodes) > 0:
        test_episodes = train_episodes[:3]  # æ¸¬è©¦å‰ 3 å€‹ episodes
        env = HandoverEnvironment(test_episodes, config, timestamp_index=timestamp_index, mode='eval')
        state, info = env.reset()

        episode_reward = 0
        steps = 0

        while True:
            action = env.action_space.sample()
            next_state, reward, terminated, truncated, info = env.step(action)
            episode_reward += reward
            steps += 1

            if terminated or truncated:
                break

        print(f"   Episode å®Œæˆ")
        print(f"   ç¸½æ­¥æ•¸: {steps}")
        print(f"   ç¸½çå‹µ: {episode_reward:.3f}")
        print(f"   ç¸½æ›æ‰‹æ¬¡æ•¸: {info['total_handovers']}")
        print(f"   âœ… Episode æ¸¬è©¦é€šé")

    print("\n" + "=" * 70)
    print("âœ… Phase 3 å®Œæˆï¼ç’°å¢ƒé©—è­‰é€šé")
    print("=" * 70)
    print("\nä¸‹ä¸€æ­¥: é‹è¡Œ Phase 4 (RL è¨“ç·´)")
    print("  python phase4_rl_training.py")


if __name__ == "__main__":
    test_environment()
