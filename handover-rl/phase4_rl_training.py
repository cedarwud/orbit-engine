#!/usr/bin/env python3
"""
Phase 4: RL æ™ºèƒ½é«”è¨“ç·´

åŠŸèƒ½ï¼š
1. å¯¦ä½œ DQN æ™ºèƒ½é«”
2. è¨“ç·´ DQN æ¨¡å‹
3. ä¿å­˜è¨“ç·´å¥½çš„æ¨¡å‹
4. è¨˜éŒ„è¨“ç·´æ›²ç·š

åŸ·è¡Œï¼š
    python phase4_rl_training.py

è¼¸å‡ºï¼š
    results/models/dqn_final.pth          - æœ€çµ‚æ¨¡å‹
    results/models/dqn_best.pth           - æœ€ä½³æ¨¡å‹
    results/training_log.json             - è¨“ç·´æ—¥èªŒ
    results/plots/training_curve.png      - è¨“ç·´æ›²ç·š
"""

import json
import yaml
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from collections import deque
import random
from pathlib import Path
from typing import List, Dict, Tuple
import matplotlib.pyplot as plt

from phase3_rl_environment import HandoverEnvironment


class DQNNetwork(nn.Module):
    """
    DQN ç¥ç¶“ç¶²è·¯

    æ¶æ§‹:
        Input (state_dim=12) â†’ FC(128) â†’ ReLU â†’ FC(128) â†’ ReLU â†’ Output (action_dim=2)

    SOURCE: Mnih et al. (2015) Human-level control through deep reinforcement learning
    NOTE: 12-dim state = [RSRP, RSRQ, SINR, distance, elevation, doppler, velocity,
                           atmospheric_loss, path_loss, propagation_delay, offset_mo, cell_offset]
          2 actions = [maintain, handover]
    """

    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 128):
        super(DQNNetwork, self).__init__()

        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, action_dim)

        self.relu = nn.ReLU()

    def forward(self, x):
        """å‰å‘å‚³æ’­"""
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        x = self.fc3(x)
        return x


class ReplayBuffer:
    """
    ç¶“é©—å›æ”¾ç·©è¡å€

    å„²å­˜ (state, action, reward, next_state, done) è½‰ç§»
    """

    def __init__(self, capacity: int = 10000):
        self.buffer = deque(maxlen=capacity)

    def push(self, state, action, reward, next_state, done):
        """æ·»åŠ ç¶“é©—"""
        self.buffer.append((state, action, reward, next_state, done))

    def sample(self, batch_size: int):
        """éš¨æ©Ÿæ¡æ¨£æ‰¹æ¬¡"""
        batch = random.sample(self.buffer, batch_size)

        states, actions, rewards, next_states, dones = zip(*batch)

        return (
            torch.FloatTensor(np.array(states)),
            torch.LongTensor(actions),
            torch.FloatTensor(rewards),
            torch.FloatTensor(np.array(next_states)),
            torch.FloatTensor(dones)
        )

    def __len__(self):
        return len(self.buffer)


class DQNAgent:
    """DQN æ™ºèƒ½é«”"""

    def __init__(self, state_dim: int, action_dim: int, config: Dict):
        """
        Args:
            state_dim: ç‹€æ…‹ç¶­åº¦
            action_dim: å‹•ä½œç¶­åº¦
            config: DQN é…ç½®
        """
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.config = config

        # Q ç¶²è·¯
        hidden_dim = config['hidden_dim']
        self.q_network = DQNNetwork(state_dim, action_dim, hidden_dim)
        self.target_network = DQNNetwork(state_dim, action_dim, hidden_dim)
        self.target_network.load_state_dict(self.q_network.state_dict())

        # å„ªåŒ–å™¨
        self.optimizer = optim.Adam(self.q_network.parameters(), lr=config['learning_rate'])
        self.loss_fn = nn.MSELoss()

        # ç¶“é©—å›æ”¾
        self.replay_buffer = ReplayBuffer(capacity=config['memory_size'])

        # è¶…åƒæ•¸ (SOURCE: Mnih et al. 2015 DQN paper)
        self.gamma = config['gamma']                    # SOURCE: Discount factor (typical 0.99)
        self.epsilon = config['epsilon_start']          # SOURCE: Epsilon-greedy exploration (typical 1.0)
        self.epsilon_end = config['epsilon_end']        # SOURCE: Minimum epsilon (typical 0.01)
        self.epsilon_decay = config['epsilon_decay']    # SOURCE: Decay rate per episode
        self.batch_size = config['batch_size']          # SOURCE: Mini-batch size (typical 32-64)
        self.target_update = config['target_update']    # SOURCE: Target network update frequency

        # è¨“ç·´è¨ˆæ•¸
        self.update_count = 0

    def select_action(self, state: np.ndarray, eval_mode: bool = False) -> int:
        """
        é¸æ“‡å‹•ä½œï¼ˆÎµ-greedy ç­–ç•¥ï¼‰

        Args:
            state: ç•¶å‰ç‹€æ…‹
            eval_mode: è©•ä¼°æ¨¡å¼ï¼ˆä¸æ¢ç´¢ï¼‰

        Returns:
            action: é¸æ“‡çš„å‹•ä½œ
        """
        if not eval_mode and random.random() < self.epsilon:
            # æ¢ç´¢ï¼šéš¨æ©Ÿå‹•ä½œ
            return random.randint(0, self.action_dim - 1)
        else:
            # åˆ©ç”¨ï¼šæœ€ä½³å‹•ä½œ
            with torch.no_grad():
                state_tensor = torch.FloatTensor(state).unsqueeze(0)
                q_values = self.q_network(state_tensor)
                action = q_values.argmax(dim=1).item()
            return action

    def update(self):
        """æ›´æ–° Q ç¶²è·¯"""
        if len(self.replay_buffer) < self.batch_size:
            return None

        # æ¡æ¨£æ‰¹æ¬¡
        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)

        # è¨ˆç®—ç•¶å‰ Q å€¼
        current_q = self.q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)

        # è¨ˆç®—ç›®æ¨™ Q å€¼
        with torch.no_grad():
            next_q = self.target_network(next_states).max(dim=1)[0]
            target_q = rewards + self.gamma * next_q * (1 - dones)

        # è¨ˆç®—æå¤±
        loss = self.loss_fn(current_q, target_q)

        # åå‘å‚³æ’­
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

        # æ›´æ–°ç›®æ¨™ç¶²è·¯
        self.update_count += 1
        if self.update_count % self.target_update == 0:
            self.target_network.load_state_dict(self.q_network.state_dict())

        # è¡°æ¸› epsilon
        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)

        return loss.item()

    def save(self, path: str):
        """ä¿å­˜æ¨¡å‹"""
        torch.save({
            'q_network': self.q_network.state_dict(),
            'target_network': self.target_network.state_dict(),
            'optimizer': self.optimizer.state_dict(),
            'epsilon': self.epsilon
        }, path)

    def load(self, path: str):
        """è¼‰å…¥æ¨¡å‹"""
        checkpoint = torch.load(path)
        self.q_network.load_state_dict(checkpoint['q_network'])
        self.target_network.load_state_dict(checkpoint['target_network'])
        self.optimizer.load_state_dict(checkpoint['optimizer'])
        self.epsilon = checkpoint['epsilon']


class Trainer:
    """è¨“ç·´å™¨"""

    def __init__(self, config: Dict):
        import pickle

        self.config = config

        # è¼‰å…¥æ•¸æ“šï¼ˆEpisode æ ¼å¼ from phase1_data_loader_v2.pyï¼‰
        print("ğŸ“¥ è¼‰å…¥ Episode æ•¸æ“š...")
        data_path = Path("data")

        with open(data_path / "train_episodes.pkl", 'rb') as f:
            self.train_episodes = pickle.load(f)
        with open(data_path / "val_episodes.pkl", 'rb') as f:
            self.val_episodes = pickle.load(f)

        print(f"   è¨“ç·´é›†: {len(self.train_episodes)} Episodes")
        print(f"   é©—è­‰é›†: {len(self.val_episodes)} Episodes")

        # å‰µå»ºç’°å¢ƒ
        self.train_env = HandoverEnvironment(self.train_episodes, config, mode='train')
        self.val_env = HandoverEnvironment(self.val_episodes, config, mode='eval')

        # å‰µå»ºæ™ºèƒ½é«”
        state_dim = config['environment']['state_dim']
        action_dim = config['environment']['action_dim']
        self.agent = DQNAgent(state_dim, action_dim, config['dqn'])

        # è¨“ç·´é…ç½®
        self.episodes = config['training']['episodes']
        self.save_interval = config['training']['save_interval']
        self.log_interval = config['training']['log_interval']
        self.eval_interval = config['training']['eval_interval']

        # è¨˜éŒ„
        self.training_log = []
        self.best_reward = -float('inf')

    def train(self):
        """è¨“ç·´å¾ªç’°"""
        print(f"\nğŸš€ é–‹å§‹è¨“ç·´ DQN (å…± {self.episodes} episodes)...")

        for episode in range(1, self.episodes + 1):
            # è¨“ç·´ä¸€å€‹ episode
            episode_reward, episode_loss = self._train_episode()

            # è¨˜éŒ„
            log_entry = {
                'episode': episode,
                'reward': episode_reward,
                'loss': episode_loss,
                'epsilon': self.agent.epsilon
            }

            # è©•ä¼°
            if episode % self.eval_interval == 0:
                val_reward = self._evaluate()
                log_entry['val_reward'] = val_reward

                # ä¿å­˜æœ€ä½³æ¨¡å‹
                if val_reward > self.best_reward:
                    self.best_reward = val_reward
                    self.agent.save("results/models/dqn_best.pth")

            self.training_log.append(log_entry)

            # æ—¥èªŒ
            if episode % self.log_interval == 0:
                print(f"Episode {episode}/{self.episodes}: "
                      f"Reward={episode_reward:.2f}, "
                      f"Loss={episode_loss:.4f}, "
                      f"Îµ={self.agent.epsilon:.3f}")

            # ä¿å­˜æª¢æŸ¥é»
            if episode % self.save_interval == 0:
                self.agent.save(f"results/checkpoints/dqn_episode_{episode}.pth")

        # ä¿å­˜æœ€çµ‚æ¨¡å‹
        self.agent.save("results/models/dqn_final.pth")
        print(f"\nâœ… è¨“ç·´å®Œæˆï¼æœ€ä½³é©—è­‰çå‹µ: {self.best_reward:.2f}")

    def _train_episode(self) -> Tuple[float, float]:
        """è¨“ç·´ä¸€å€‹ episode"""
        state, _ = self.train_env.reset()
        episode_reward = 0
        episode_losses = []

        while True:
            # é¸æ“‡å‹•ä½œ
            action = self.agent.select_action(state)

            # åŸ·è¡Œå‹•ä½œ
            next_state, reward, terminated, truncated, info = self.train_env.step(action)

            # å„²å­˜ç¶“é©—
            done = float(terminated or truncated)
            self.agent.replay_buffer.push(state, action, reward, next_state, done)

            # æ›´æ–°ç¶²è·¯
            loss = self.agent.update()
            if loss is not None:
                episode_losses.append(loss)

            # ç´¯ç©çå‹µ
            episode_reward += reward

            # æ›´æ–°ç‹€æ…‹
            state = next_state

            if terminated or truncated:
                break

        avg_loss = np.mean(episode_losses) if episode_losses else 0
        return episode_reward, avg_loss

    def _evaluate(self) -> float:
        """è©•ä¼°ç•¶å‰ç­–ç•¥"""
        state, _ = self.val_env.reset()
        total_reward = 0

        while True:
            action = self.agent.select_action(state, eval_mode=True)
            next_state, reward, terminated, truncated, info = self.val_env.step(action)

            total_reward += reward
            state = next_state

            if terminated or truncated:
                break

        return total_reward

    def save_training_log(self):
        """ä¿å­˜è¨“ç·´æ—¥èªŒ"""
        log_file = Path("results/training_log.json")
        log_file.parent.mkdir(parents=True, exist_ok=True)

        with open(log_file, 'w') as f:
            json.dump(self.training_log, f, indent=2)

        print(f"ğŸ’¾ è¨“ç·´æ—¥èªŒå·²ä¿å­˜: {log_file}")

    def plot_training_curve(self):
        """ç¹ªè£½è¨“ç·´æ›²ç·š"""
        episodes = [entry['episode'] for entry in self.training_log]
        rewards = [entry['reward'] for entry in self.training_log]
        losses = [entry['loss'] for entry in self.training_log]

        # æå–é©—è­‰çå‹µ
        val_episodes = [entry['episode'] for entry in self.training_log if 'val_reward' in entry]
        val_rewards = [entry['val_reward'] for entry in self.training_log if 'val_reward' in entry]

        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8))

        # çå‹µæ›²ç·š
        ax1.plot(episodes, rewards, label='Training Reward', alpha=0.6)
        if val_episodes:
            ax1.plot(val_episodes, val_rewards, 'o-', label='Validation Reward', linewidth=2)
        ax1.set_xlabel('Episode')
        ax1.set_ylabel('Reward')
        ax1.set_title('DQN Training Curve - Reward')
        ax1.legend()
        ax1.grid(True, alpha=0.3)

        # æå¤±æ›²ç·š
        ax2.plot(episodes, losses, label='Loss', color='orange', alpha=0.6)
        ax2.set_xlabel('Episode')
        ax2.set_ylabel('Loss')
        ax2.set_title('DQN Training Curve - Loss')
        ax2.legend()
        ax2.grid(True, alpha=0.3)

        plt.tight_layout()

        plot_file = Path("results/plots/training_curve.png")
        plot_file.parent.mkdir(parents=True, exist_ok=True)
        plt.savefig(plot_file, dpi=150)
        print(f"ğŸ“Š è¨“ç·´æ›²ç·šå·²ä¿å­˜: {plot_file}")

        plt.close()


def main():
    """ä¸»å‡½æ•¸"""
    print("=" * 70)
    print("Phase 4: DQN è¨“ç·´")
    print("=" * 70)

    # è¼‰å…¥é…ç½®
    with open("config/rl_config.yaml", 'r') as f:
        config = yaml.safe_load(f)

    # å‰µå»ºå¿…è¦ç›®éŒ„
    Path("results/models").mkdir(parents=True, exist_ok=True)
    Path("results/checkpoints").mkdir(parents=True, exist_ok=True)
    Path("results/plots").mkdir(parents=True, exist_ok=True)

    # å‰µå»ºè¨“ç·´å™¨
    trainer = Trainer(config)

    # é–‹å§‹è¨“ç·´
    trainer.train()

    # ä¿å­˜æ—¥èªŒ
    trainer.save_training_log()

    # ç¹ªè£½æ›²ç·š
    trainer.plot_training_curve()

    print("\n" + "=" * 70)
    print("âœ… Phase 4 å®Œæˆï¼")
    print("=" * 70)
    print("\nä¸‹ä¸€æ­¥: é‹è¡Œ Phase 5 (è©•ä¼°èˆ‡æ¯”è¼ƒ)")
    print("  python phase5_evaluation.py")


if __name__ == "__main__":
    main()
