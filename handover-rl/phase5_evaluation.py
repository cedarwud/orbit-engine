#!/usr/bin/env python3
"""
Phase 5: è©•ä¼°èˆ‡æ¯”è¼ƒ

åŠŸèƒ½ï¼š
1. è©•ä¼°è¨“ç·´å¥½çš„ DQN æ¨¡å‹
2. èˆ‡ Baseline æ–¹æ³•æ¯”è¼ƒ
3. ç”Ÿæˆå®Œæ•´è©•ä¼°å ±å‘Š
4. ç¹ªè£½æ€§èƒ½æ¯”è¼ƒåœ–è¡¨

åŸ·è¡Œï¼š
    python phase5_evaluation.py

è¼¸å‡ºï¼š
    results/final_evaluation.json        - å®Œæ•´è©•ä¼°çµæœ
    results/comparison_report.txt        - æ¯”è¼ƒå ±å‘Š
    results/plots/performance_comparison.png - æ€§èƒ½æ¯”è¼ƒåœ–
"""

import json
import pickle
import yaml
import numpy as np
import torch
from pathlib import Path
from typing import Dict, List
import matplotlib.pyplot as plt
from datetime import datetime

from phase3_rl_environment import HandoverEnvironment
from phase4_rl_training import DQNAgent
from phase2_baseline_methods import RSRPBasedHandover, A3TriggeredHandover, AlwaysHandoverBaseline


class Evaluator:
    """è©•ä¼°å™¨"""

    def __init__(self, config: Dict):
        self.config = config

        # è¼‰å…¥æ¸¬è©¦æ•¸æ“šï¼ˆæ–°æ ¼å¼ï¼štest_episodes.pklï¼‰
        print("ğŸ“¥ è¼‰å…¥æ¸¬è©¦æ•¸æ“š...")
        data_path = Path("data")

        try:
            # å˜—è©¦è¼‰å…¥æ–°æ ¼å¼ï¼ˆPhase 1 v2ï¼‰
            with open(data_path / "test_episodes.pkl", 'rb') as f:
                test_episodes = pickle.load(f)
            print(f"   æ¸¬è©¦ Episodes: {len(test_episodes)}")

            # âœ… è¼‰å…¥æ™‚é–“æˆ³ç´¢å¼•ï¼ˆç”¨æ–¼çœŸå¯¦é„°å±…æŸ¥æ‰¾ - é—œéµä¿®å¾©ï¼ï¼‰
            try:
                with open(data_path / "timestamp_index.pkl", 'rb') as f:
                    self.timestamp_index = pickle.load(f)
                print(f"   âœ… æ™‚é–“æˆ³ç´¢å¼•: {len(self.timestamp_index)} å€‹æ™‚é–“æˆ³")
            except FileNotFoundError:
                print("   âŒ æ‰¾ä¸åˆ° timestamp_index.pkl - é€™å°‡å°è‡´çå‹µå‡½æ•¸å¤±æ•ˆï¼")
                print("   è«‹é‡æ–°é‹è¡Œ: python phase1_data_loader_v2.py")
                raise

            # è½‰æ›ç‚º samples æ ¼å¼ï¼ˆç”¨æ–¼ Baseline è©•ä¼°ï¼‰
            print(f"   è½‰æ› Episodes ç‚º Baseline è©•ä¼°æ ¼å¼...")
            self.test_samples = self._convert_episodes_to_samples(test_episodes)
            print(f"   æ¸¬è©¦æ¨£æœ¬æ•¸: {len(self.test_samples)}")

            # âœ… ä¿å­˜ episodes ç”¨æ–¼ DQN è©•ä¼°
            self.test_episodes = test_episodes

        except FileNotFoundError:
            # é™ç´šï¼šå˜—è©¦è¼‰å…¥èˆŠæ ¼å¼ï¼ˆå‘å¾Œç›¸å®¹ï¼‰
            print(f"   âš ï¸  æ‰¾ä¸åˆ° test_episodes.pklï¼Œå˜—è©¦è¼‰å…¥èˆŠæ ¼å¼...")
            try:
                with open(data_path / "test_data.json", 'r') as f:
                    self.test_samples = json.load(f)
                print(f"   âœ… ä½¿ç”¨èˆŠæ ¼å¼ test_data.json")
                print(f"   æ¸¬è©¦æ¨£æœ¬æ•¸: {len(self.test_samples)}")
                self.test_episodes = []
                self.timestamp_index = {}
            except FileNotFoundError:
                raise FileNotFoundError(
                    "æ‰¾ä¸åˆ°æ¸¬è©¦æ•¸æ“šï¼è«‹å…ˆé‹è¡Œ: python phase1_data_loader_v2.py"
                )

        # âœ… å‰µå»ºæ¸¬è©¦ç’°å¢ƒï¼ˆå‚³å…¥ episodes å’Œ timestamp_indexï¼‰
        self.test_env = HandoverEnvironment(self.test_episodes, config, timestamp_index=self.timestamp_index, mode='eval')

        # è©•ä¼°çµæœ
        self.results = {}

    def _convert_episodes_to_samples(self, episodes: List) -> List[Dict]:
        """
        å°‡ Episode æ ¼å¼è½‰æ›ç‚ºè©•ä¼°æ‰€éœ€çš„ samples æ ¼å¼

        âœ… ä½¿ç”¨çœŸå¯¦é„°å±…æ•¸æ“šï¼ˆç„¡ç°¡åŒ–å‡è¨­ï¼‰

        Args:
            episodes: Episode å°è±¡åˆ—è¡¨ï¼ˆä¾†è‡ª phase1_data_loader_v2.pyï¼‰

        Returns:
            samples: æ›æ‰‹æ±ºç­–æ¨£æœ¬åˆ—è¡¨ï¼ˆèˆŠæ ¼å¼ï¼Œä¾›è©•ä¼°ä½¿ç”¨ï¼‰

        SOURCE: ç›¸å®¹æ€§è½‰æ›å‡½æ•¸ï¼Œæ”¯æ´ Phase 1 v2 çš„æ–°æ•¸æ“šæ ¼å¼
        """
        samples = []

        # âœ… è¼‰å…¥æ™‚é–“æˆ³ç´¢å¼•ï¼ˆç”¨æ–¼çœŸå¯¦é„°å±…æŸ¥æ‰¾ï¼‰
        timestamp_index = None
        try:
            data_path = Path("data")
            with open(data_path / "timestamp_index.pkl", 'rb') as f:
                timestamp_index = pickle.load(f)
            print(f"   âœ… æ™‚é–“æˆ³ç´¢å¼•å·²è¼‰å…¥ï¼ˆç”¨æ–¼çœŸå¯¦é„°å±… RSRPï¼‰")
        except FileNotFoundError:
            print(f"   âš ï¸  æ‰¾ä¸åˆ° timestamp_index.pklï¼Œå°‡ä½¿ç”¨ç°¡åŒ–ç‰ˆæœ¬")

        for episode in episodes:
            # è™•ç†å­—å…¸æ ¼å¼çš„ Episode
            if isinstance(episode, dict):
                satellite_id = episode['satellite_id']
                time_series = episode.get('time_series', episode.get('time_points', []))
                gpp_events = episode.get('gpp_events', [])
            else:
                # è™•ç†å°è±¡æ ¼å¼çš„ Episode
                satellite_id = episode.satellite_id
                time_series = getattr(episode, 'time_points', getattr(episode, 'time_series', []))
                gpp_events = episode.gpp_events

            # å¾æ™‚é–“åºåˆ—å‰µå»ºæ¨£æœ¬ï¼ˆæ¯å€‹æ™‚é–“é»ä¸€å€‹æ¨£æœ¬ï¼‰
            for time_point in time_series:
                timestamp = time_point.get('timestamp', '')
                serving_rsrp = time_point.get('rsrp_dbm', -999)

                # âœ… å¾æ™‚é–“æˆ³ç´¢å¼•æ‰¾åˆ°çœŸå¯¦é„°å±…
                best_neighbor_id = 'unknown'
                best_neighbor_rsrp = -999

                if timestamp_index and timestamp in timestamp_index:
                    neighbors = timestamp_index[timestamp]

                    # æ‰¾åˆ°æœ€ä½³é„°å±…ï¼ˆRSRP æœ€é«˜ä¸”å„ªæ–¼ç•¶å‰è¡›æ˜Ÿï¼‰
                    for neighbor_id, neighbor_data in neighbors.items():
                        # æ’é™¤ç•¶å‰æœå‹™è¡›æ˜Ÿè‡ªå·±
                        if neighbor_id == satellite_id:
                            continue

                        neighbor_rsrp = neighbor_data.get('rsrp_dbm', -999)
                        if neighbor_rsrp > best_neighbor_rsrp:
                            best_neighbor_rsrp = neighbor_rsrp
                            best_neighbor_id = neighbor_id

                # æå–åŸºæœ¬ä¿¡è™Ÿå“è³ª
                sample = {
                    'satellite_id': satellite_id,
                    'serving_satellite': satellite_id,
                    'neighbor_satellite': best_neighbor_id,  # âœ… çœŸå¯¦é„°å±…è¡›æ˜Ÿ
                    'serving_rsrp': serving_rsrp,
                    'neighbor_rsrp': best_neighbor_rsrp,  # âœ… çœŸå¯¦é„°å±… RSRPï¼ˆå¾æ™‚é–“æˆ³ç´¢å¼•ç²å–ï¼‰
                    'event_type': 'NONE',
                    'timestamp': timestamp,
                    # ç‰©ç†åƒæ•¸
                    'serving_elevation': time_point.get('elevation_deg', None),
                    'serving_distance': time_point.get('distance_km', None),
                }
                samples.append(sample)

            # å¾ 3GPP äº‹ä»¶å‰µå»ºæ¨£æœ¬
            for event in gpp_events:
                if event.get('role') == 'serving':
                    measurements = event.get('measurements', {})
                    sample = {
                        'satellite_id': satellite_id,
                        'serving_satellite': event.get('serving_satellite', satellite_id),
                        'neighbor_satellite': event.get('neighbor_satellite', 'unknown'),
                        'serving_rsrp': measurements.get('serving_rsrp_dbm', -999),
                        'neighbor_rsrp': measurements.get('neighbor_rsrp_dbm', -999),
                        'event_type': event.get('type', 'NONE'),
                        'timestamp': event.get('timestamp', ''),
                        'serving_elevation': None,
                        'serving_distance': None,
                    }
                    samples.append(sample)

        return samples

    def evaluate_dqn(self, model_path: str) -> Dict:
        """è©•ä¼° DQN æ¨¡å‹"""
        print(f"\nğŸ¤– è©•ä¼° DQN æ¨¡å‹: {model_path}")

        # è¼‰å…¥æ¨¡å‹
        state_dim = self.config['environment']['state_dim']
        action_dim = self.config['environment']['action_dim']
        agent = DQNAgent(state_dim, action_dim, self.config['dqn'])

        if Path(model_path).exists():
            agent.load(model_path)
            print(f"   âœ… æ¨¡å‹è¼‰å…¥æˆåŠŸ")
        else:
            print(f"   âŒ æ¨¡å‹ä¸å­˜åœ¨: {model_path}")
            return {}

        # åŸ·è¡Œè©•ä¼°
        state, _ = self.test_env.reset()
        total_reward = 0
        handover_count = 0
        ping_pong_count = 0
        qos_values = []
        decisions = []

        step = 0
        while True:
            # é¸æ“‡å‹•ä½œï¼ˆä¸æ¢ç´¢ï¼‰
            action = agent.select_action(state, eval_mode=True)
            decisions.append(action)

            # åŸ·è¡Œå‹•ä½œ
            next_state, reward, terminated, truncated, info = self.test_env.step(action)

            total_reward += reward

            if action == 1:
                handover_count += 1

            # æ”¶é›† QoS æ•¸æ“š
            if step < len(self.test_samples):
                sample = self.test_samples[step]
                if sample['serving_rsrp'] != -999:
                    qos_values.append(sample['serving_rsrp'])

            state = next_state
            step += 1

            if terminated or truncated:
                break

        # è¨ˆç®— Ping-Pong
        for i in range(len(decisions) - 1):
            if decisions[i] == 1 and decisions[i + 1] == 1:
                if self._is_ping_pong(i):
                    ping_pong_count += 1

        # è¨ˆç®—æŒ‡æ¨™
        metrics = {
            'total_reward': total_reward,
            'average_reward': total_reward / len(self.test_samples),
            'handover_count': handover_count,
            'handover_frequency': handover_count / len(self.test_samples),
            'ping_pong_count': ping_pong_count,
            'ping_pong_rate': ping_pong_count / max(handover_count, 1),
            'average_qos': np.mean(qos_values) if qos_values else -999,
            'total_samples': len(self.test_samples)
        }

        print(f"   ç¸½çå‹µ: {metrics['total_reward']:.2f}")
        print(f"   æ›æ‰‹é »ç‡: {metrics['handover_frequency']:.3f}")
        print(f"   Ping-Pong ç‡: {metrics['ping_pong_rate']:.2%}")
        print(f"   å¹³å‡ QoS: {metrics['average_qos']:.2f} dBm")

        return metrics

    def evaluate_baseline(self, method_name: str, method) -> Dict:
        """è©•ä¼° Baseline æ–¹æ³•"""
        print(f"\nğŸ“Š è©•ä¼° Baseline: {method_name}")

        handover_count = 0
        ping_pong_count = 0
        qos_values = []
        decisions = []

        for sample in self.test_samples:
            decision = method.decide(sample)
            decisions.append(decision)

            if decision == 1:
                handover_count += 1

            if sample['serving_rsrp'] != -999:
                qos_values.append(sample['serving_rsrp'])

        # è¨ˆç®— Ping-Pong
        for i in range(len(decisions) - 1):
            if decisions[i] == 1 and decisions[i + 1] == 1:
                if self._is_ping_pong(i):
                    ping_pong_count += 1

        metrics = {
            'handover_count': handover_count,
            'handover_frequency': handover_count / len(self.test_samples),
            'ping_pong_count': ping_pong_count,
            'ping_pong_rate': ping_pong_count / max(handover_count, 1),
            'average_qos': np.mean(qos_values) if qos_values else -999,
            'total_samples': len(self.test_samples)
        }

        print(f"   æ›æ‰‹é »ç‡: {metrics['handover_frequency']:.3f}")
        print(f"   Ping-Pong ç‡: {metrics['ping_pong_rate']:.2%}")
        print(f"   å¹³å‡ QoS: {metrics['average_qos']:.2f} dBm")

        return metrics

    def _is_ping_pong(self, index: int) -> bool:
        """æª¢æ¸¬ Ping-Pong"""
        if index + 1 >= len(self.test_samples):
            return False

        sample1 = self.test_samples[index]
        sample2 = self.test_samples[index + 1]

        return (sample1['serving_satellite'] == sample2['neighbor_satellite'] and
                sample1['neighbor_satellite'] == sample2['serving_satellite'])

    def compare_all_methods(self):
        """æ¯”è¼ƒæ‰€æœ‰æ–¹æ³•"""
        print("\n" + "=" * 70)
        print("è©•ä¼°æ‰€æœ‰æ–¹æ³•")
        print("=" * 70)

        # è©•ä¼° DQN
        dqn_best = self.evaluate_dqn("results/models/dqn_best.pth")
        if dqn_best:
            self.results['DQN (Best)'] = dqn_best

        # è©•ä¼° Baseline
        self.results['RSRP-based'] = self.evaluate_baseline(
            "RSRP-based",
            RSRPBasedHandover(serving_threshold=-100.0, neighbor_threshold=-90.0, margin=3.0)
        )

        self.results['A3-triggered'] = self.evaluate_baseline(
            "A3-triggered",
            A3TriggeredHandover(a3_offset=3.0)
        )

        self.results['Always-handover'] = self.evaluate_baseline(
            "Always-handover",
            AlwaysHandoverBaseline()
        )

    def save_results(self):
        """ä¿å­˜è©•ä¼°çµæœ"""
        results_file = Path("results/final_evaluation.json")

        output = {
            'evaluation_time': datetime.now().isoformat(),
            'test_samples': len(self.test_samples),
            'methods': self.results
        }

        with open(results_file, 'w') as f:
            json.dump(output, f, indent=2)

        print(f"\nğŸ’¾ è©•ä¼°çµæœå·²ä¿å­˜: {results_file}")

    def generate_report(self):
        """ç”Ÿæˆæ¯”è¼ƒå ±å‘Š"""
        report_file = Path("results/comparison_report.txt")

        with open(report_file, 'w') as f:
            f.write("=" * 80 + "\n")
            f.write("Handover-RL æœ€çµ‚è©•ä¼°å ±å‘Š\n")
            f.write("=" * 80 + "\n\n")
            f.write(f"ç”Ÿæˆæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"æ¸¬è©¦æ¨£æœ¬æ•¸: {len(self.test_samples)}\n\n")

            # è¡¨é ­
            f.write(f"{'æ–¹æ³•':<20} {'æ›æ‰‹é »ç‡':<15} {'Ping-Pongç‡':<15} {'å¹³å‡QoS(dBm)':<15}\n")
            f.write("-" * 80 + "\n")

            # å„æ–¹æ³•çµæœ
            for method_name, metrics in self.results.items():
                f.write(f"{method_name:<20} "
                        f"{metrics['handover_frequency']:<15.3f} "
                        f"{metrics['ping_pong_rate']:<15.2%} "
                        f"{metrics['average_qos']:<15.2f}\n")

            # æœ€ä½³æ–¹æ³•
            f.write("\n" + "=" * 80 + "\n")
            f.write("æ€§èƒ½æ’å:\n\n")

            # æŒ‰æ›æ‰‹é »ç‡æ’åºï¼ˆè¶Šä½è¶Šå¥½ï¼‰
            sorted_by_frequency = sorted(self.results.items(),
                                        key=lambda x: x[1]['handover_frequency'])
            f.write("æ›æ‰‹é »ç‡ï¼ˆè¶Šä½è¶Šå¥½ï¼‰:\n")
            for i, (name, metrics) in enumerate(sorted_by_frequency, 1):
                f.write(f"  {i}. {name}: {metrics['handover_frequency']:.3f}\n")

            # æŒ‰ Ping-Pong ç‡æ’åºï¼ˆè¶Šä½è¶Šå¥½ï¼‰
            sorted_by_pingpong = sorted(self.results.items(),
                                       key=lambda x: x[1]['ping_pong_rate'])
            f.write("\nPing-Pong ç‡ï¼ˆè¶Šä½è¶Šå¥½ï¼‰:\n")
            for i, (name, metrics) in enumerate(sorted_by_pingpong, 1):
                f.write(f"  {i}. {name}: {metrics['ping_pong_rate']:.2%}\n")

            # æŒ‰å¹³å‡ QoS æ’åºï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰
            sorted_by_qos = sorted(self.results.items(),
                                  key=lambda x: x[1]['average_qos'],
                                  reverse=True)
            f.write("\nå¹³å‡ QoSï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰:\n")
            for i, (name, metrics) in enumerate(sorted_by_qos, 1):
                f.write(f"  {i}. {name}: {metrics['average_qos']:.2f} dBm\n")

            f.write("\n" + "=" * 80 + "\n")

        print(f"ğŸ“„ æ¯”è¼ƒå ±å‘Šå·²ä¿å­˜: {report_file}")

    def plot_comparison(self):
        """ç¹ªè£½æ€§èƒ½æ¯”è¼ƒåœ–"""
        methods = list(self.results.keys())
        handover_freq = [self.results[m]['handover_frequency'] for m in methods]
        ping_pong_rate = [self.results[m]['ping_pong_rate'] for m in methods]
        avg_qos = [self.results[m]['average_qos'] for m in methods]

        fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))

        # æ›æ‰‹é »ç‡
        colors = ['#2ecc71' if 'DQN' in m else '#3498db' for m in methods]
        ax1.bar(range(len(methods)), handover_freq, color=colors)
        ax1.set_xticks(range(len(methods)))
        ax1.set_xticklabels(methods, rotation=45, ha='right')
        ax1.set_ylabel('Handover Frequency')
        ax1.set_title('æ›æ‰‹é »ç‡ï¼ˆè¶Šä½è¶Šå¥½ï¼‰')
        ax1.grid(True, alpha=0.3)

        # Ping-Pong ç‡
        ax2.bar(range(len(methods)), ping_pong_rate, color=colors)
        ax2.set_xticks(range(len(methods)))
        ax2.set_xticklabels(methods, rotation=45, ha='right')
        ax2.set_ylabel('Ping-Pong Rate')
        ax2.set_title('Ping-Pong ç‡ï¼ˆè¶Šä½è¶Šå¥½ï¼‰')
        ax2.grid(True, alpha=0.3)

        # å¹³å‡ QoS
        ax3.bar(range(len(methods)), avg_qos, color=colors)
        ax3.set_xticks(range(len(methods)))
        ax3.set_xticklabels(methods, rotation=45, ha='right')
        ax3.set_ylabel('Average RSRP (dBm)')
        ax3.set_title('å¹³å‡ QoSï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰')
        ax3.grid(True, alpha=0.3)

        plt.tight_layout()

        plot_file = Path("results/plots/performance_comparison.png")
        plt.savefig(plot_file, dpi=150, bbox_inches='tight')
        print(f"ğŸ“Š æ€§èƒ½æ¯”è¼ƒåœ–å·²ä¿å­˜: {plot_file}")

        plt.close()


def main():
    """ä¸»å‡½æ•¸"""
    print("=" * 70)
    print("Phase 5: æœ€çµ‚è©•ä¼°èˆ‡æ¯”è¼ƒ")
    print("=" * 70)

    # è¼‰å…¥é…ç½®
    with open("config/rl_config.yaml", 'r') as f:
        config = yaml.safe_load(f)

    # å‰µå»ºè©•ä¼°å™¨
    evaluator = Evaluator(config)

    # æ¯”è¼ƒæ‰€æœ‰æ–¹æ³•
    evaluator.compare_all_methods()

    # ä¿å­˜çµæœ
    evaluator.save_results()

    # ç”Ÿæˆå ±å‘Š
    evaluator.generate_report()

    # ç¹ªè£½åœ–è¡¨
    evaluator.plot_comparison()

    print("\n" + "=" * 70)
    print("âœ… æ‰€æœ‰éšæ®µå®Œæˆï¼")
    print("=" * 70)
    print("\nğŸ“ è¼¸å‡ºæ–‡ä»¶:")
    print("   results/final_evaluation.json      - å®Œæ•´è©•ä¼°çµæœ")
    print("   results/comparison_report.txt      - æ¯”è¼ƒå ±å‘Š")
    print("   results/plots/                     - æ‰€æœ‰åœ–è¡¨")
    print("\nğŸ“ ä¸‹ä¸€æ­¥: æ’°å¯«è«–æ–‡ï¼Œä½¿ç”¨é€™äº›çµæœä½œç‚ºå¯¦é©—æ•¸æ“š")


if __name__ == "__main__":
    main()
