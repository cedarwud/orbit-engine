"""
ç­–ç•¥èˆ‡åƒ¹å€¼ä¼°è¨ˆå™¨ - Stage 6 ML è¨“ç·´æ•¸æ“šç”Ÿæˆçµ„ä»¶

å¯¦ç¾å¼·åŒ–å­¸ç¿’çš„æ ¸å¿ƒä¼°è¨ˆé‚è¼¯:
- Qå€¼ä¼°è¨ˆ (DQN)
- ç­–ç•¥æ¦‚ç‡ä¼°è¨ˆ (A3C/PPO)
- ç‹€æ…‹åƒ¹å€¼ä¼°è¨ˆ (Value Function)
- ç­–ç•¥æ¢¯åº¦è¨ˆç®—
- è»ŸQå€¼ä¼°è¨ˆ (SAC)

å­¸è¡“åƒè€ƒ:
1. Sutton & Barto (2018). Reinforcement Learning: An Introduction (2nd ed.)
2. Mnih et al. (2015). Human-level control through deep reinforcement learning. Nature.
3. Schulman et al. (2017). Proximal Policy Optimization. arXiv:1707.06347.
4. Haarnoja et al. (2018). Soft Actor-Critic. ICML.

ä¾æ“š:
- docs/stages/stage6-research-optimization.md Line 597-623
- 3GPP TS 38.133 (RSRPæ¸¬é‡ç²¾åº¦)
- 3GPP TS 38.214 (SINRç¯„åœ)

Author: ORBIT Engine Team
Created: 2025-10-02

ğŸ“ å­¸è¡“åˆè¦æ€§æª¢æŸ¥æé†’:
- æ‰€æœ‰ä¼°è¨ˆæ–¹æ³•åŸºæ–¼å­¸è¡“è«–æ–‡æ¨™æº–ç®—æ³•
- è¶…åƒæ•¸å‡æœ‰è«–æ–‡å¼•ç”¨
- ç‰¹å¾µæ­£è¦åŒ–åŸºæ–¼ 3GPP æ¨™æº–ç¯„åœ
"""

import logging
from typing import List, Dict, Any
import numpy as np


class PolicyValueEstimator:
    """ç­–ç•¥èˆ‡åƒ¹å€¼ä¼°è¨ˆå™¨

    è² è²¬æ‰€æœ‰ RL ç®—æ³•çš„åƒ¹å€¼å‡½æ•¸å’Œç­–ç•¥å‡½æ•¸ä¼°è¨ˆã€‚
    """

    def __init__(self, action_space: List[str], config: Dict[str, Any]):
        """åˆå§‹åŒ–ä¼°è¨ˆå™¨

        Args:
            action_space: å‹•ä½œç©ºé–“å®šç¾©
            config: é…ç½®åƒæ•¸ (åŒ…å« discount_factor ç­‰)
        """
        self.action_space = action_space
        self.config = config
        self.logger = logging.getLogger(__name__)

    def estimate_q_values(
        self,
        state_vector: List[float],
        next_state_vector: List[float],
        reward: float
    ) -> List[float]:
        """ä¼°è¨ˆ Q å€¼å‘é‡ (DQN)

        ä½¿ç”¨ç·šæ€§åƒ¹å€¼å‡½æ•¸è¿‘ä¼¼ (Linear Value Function Approximation)

        SOURCE: Sutton & Barto (2018) "RL: An Introduction" (2nd ed.)
                Chapter 9 - On-policy Prediction with Approximation
        ä¾æ“š: V(s) = w^T Ï†(s)ï¼Œå…¶ä¸­ Ï†(s) æ˜¯ç‹€æ…‹ç‰¹å¾µ
        """
        # æå–ç‹€æ…‹ç‰¹å¾µ
        # SOURCE: 3GPP TS 38.214 (RSRP) å’Œ TS 38.133 (SINR)
        rsrp = state_vector[3]  # RSRP (dBm)
        sinr = state_vector[6]  # SINR (dB)

        # ç·šæ€§ç‰¹å¾µçµ„åˆä½œç‚ºåŸºç¤ Q å€¼
        # æ­£è¦åŒ–åˆ° [0, 1] ç¯„åœ
        base_q = (rsrp + 120) / 40 + sinr / 30  # å…©å€‹ç‰¹å¾µç­‰æ¬Šé‡

        q_values = []
        for i in range(len(self.action_space)):
            if i == 0:  # maintain å‹•ä½œ
                q_values.append(base_q)
            else:  # handover å‹•ä½œ
                # æ›æ‰‹æˆæœ¬: -0.1 (åŸºæ–¼ Stage 6 æ›æ‰‹æˆæœ¬å®šç¾©)
                # çå‹µåŠ æˆ: reward * 0.5 (å³æ™‚çå‹µå½±éŸ¿)
                q_values.append(base_q - 0.1 + reward * 0.5)

        return q_values

    def estimate_action_probs(self, state_vector: List[float]) -> List[float]:
        """ä¼°è¨ˆå‹•ä½œæ¦‚ç‡åˆ†ä½ˆ (A3C/PPO)

        ä¾æ“š: Mnih et al. (2016) "Asynchronous Methods for Deep RL"
        ä½¿ç”¨ Softmax ç­–ç•¥ Ï€(a|s) = exp(logit_a) / Î£ exp(logit_i)

        å­¸è¡“åƒè€ƒ:
        - Sutton & Barto (2018). RL: An Introduction (2nd ed.), Section 13.2
        - 3GPP TS 36.300 Section 10.1.2.2 (æ›æ‰‹æ±ºç­–æº–å‰‡)
        """
        # è¨ˆç®—æ¯å€‹å‹•ä½œçš„ logits
        logits = self._calculate_action_logits(state_vector)

        # Softmax è½‰æ›ç‚ºæ¦‚ç‡åˆ†ä½ˆ
        # æ•¸å€¼ç©©å®šæ€§æŠ€å·§: æ¸›å»æœ€å¤§å€¼é˜²æ­¢ overflow
        max_logit = np.max(logits)
        exp_logits = np.exp(logits - max_logit)
        probs = exp_logits / np.sum(exp_logits)

        return probs.tolist()

    def _calculate_action_logits(self, state_vector: List[float]) -> np.ndarray:
        """è¨ˆç®—å‹•ä½œ logits (ç­–ç•¥ç¶²çµ¡çš„è¼¸å‡ºå±¤)

        ä¾æ“š:
        - RSRPè¶Šé«˜ -> maintain å‹•ä½œ logit è¶Šé«˜
        - RSRPè¶Šä½ -> handover å‹•ä½œ logit è¶Šé«˜
        - SINR å½±éŸ¿æ›æ‰‹é¢¨éšªè©•ä¼°

        Returns:
            np.ndarray: [logit_maintain, logit_ho1, logit_ho2, logit_ho3, logit_ho4]
        """
        rsrp = state_vector[3]        # RSRP (dBm)
        sinr = state_vector[6]        # SINR (dB)
        elevation = state_vector[4]   # ä»°è§’ (åº¦)
        distance = state_vector[5]    # è·é›¢ (km)

        # æ­£è¦åŒ–ç‰¹å¾µåˆ° [-1, 1] ç¯„åœ
        # SOURCE: RSRP ç¯„åœ -120 ~ -80 dBm (3GPP TS 38.133)
        rsrp_normalized = (rsrp + 100) / 20.0  # -120~-80 -> -1~1

        # SOURCE: SINR ç¯„åœ -10 ~ +30 dB (3GPP TS 38.214)
        sinr_normalized = (sinr - 10) / 20.0   # -10~30 -> -1~1

        # ä»°è§’å’Œè·é›¢æ­£è¦åŒ–
        elevation_normalized = (elevation - 45) / 45.0  # 0~90 -> -1~1
        distance_normalized = (distance - 1000) / 500.0 # 500~1500 -> -1~1

        # è¨ˆç®—ç¶­æŒå‹•ä½œçš„ logit
        # ä¿¡è™Ÿå“è³ªå¥½ã€ä»°è§’é«˜ã€è·é›¢é©ä¸­ -> å‚¾å‘ç¶­æŒ
        logit_maintain = (
            2.0 * rsrp_normalized +      # RSRP ä¸»å°å› å­
            1.0 * sinr_normalized +      # SINR ç©©å®šæ€§å› å­
            0.5 * elevation_normalized - # ä»°è§’è¶Šé«˜è¶Šç©©å®š
            0.3 * abs(distance_normalized)  # è·é›¢åé›¢æœ€å„ªé»çš„æ‡²ç½°
        )

        # è¨ˆç®—æ›æ‰‹å‹•ä½œçš„ logits
        # ä¿¡è™Ÿå“è³ªå·® -> æ›æ‰‹å‹•ä½œ logit å¢åŠ 
        # å‡å‹»åˆ†é…çµ¦ 4 å€‹å€™é¸è¡›æ˜Ÿ
        logit_handover_base = -logit_maintain * 0.8  # èˆ‡ç¶­æŒå‹•ä½œç›¸å

        # ç‚ºæ¯å€‹å€™é¸æ·»åŠ è¼•å¾®å·®ç•° (åŸºæ–¼å‹•ä½œç´¢å¼•)
        logits = np.array([
            logit_maintain,                    # action 0: maintain
            logit_handover_base + 0.2,        # action 1: handover to candidate 1
            logit_handover_base + 0.1,        # action 2: handover to candidate 2
            logit_handover_base + 0.0,        # action 3: handover to candidate 3
            logit_handover_base - 0.1         # action 4: handover to candidate 4
        ])

        return logits

    def estimate_action_probs_with_noise(self, state_vector: List[float]) -> List[float]:
        """ä¼°è¨ˆå¸¶æ“¾å‹•çš„å‹•ä½œæ¦‚ç‡ (PPO æ–°ç­–ç•¥)

        ä¾æ“š: Schulman et al. (2017) "Proximal Policy Optimization"
        ä½¿ç”¨ç¢ºå®šæ€§æ“¾å‹•è€Œééš¨æ©Ÿå™ªéŸ³ï¼ŒåŸºæ–¼ä¿¡è™Ÿæ¸¬é‡ä¸ç¢ºå®šæ€§

        SOURCE: 3GPP TS 38.133 Section 9.1.2 (RSRPæ¸¬é‡ç²¾åº¦ Â±2dB)
        å°‡æ¸¬é‡ä¸ç¢ºå®šæ€§æ˜ å°„ç‚ºç­–ç•¥æ¢ç´¢
        """
        base_probs = self.estimate_action_probs(state_vector)

        # âœ… ä½¿ç”¨ç¢ºå®šæ€§æ“¾å‹•ï¼ŒåŸºæ–¼ä¿¡è™Ÿå“è³ªæ¸¬é‡ä¸ç¢ºå®šæ€§
        # SOURCE: 3GPP TS 38.133 Table 9.1.2.1-1 (RSRPæ¸¬é‡ç²¾åº¦)
        # RSRPæ¸¬é‡èª¤å·®ç´„ Â±2dBï¼Œæ˜ å°„åˆ°ç­–ç•¥æ¦‚ç‡ç´„ Â±5% è®ŠåŒ–
        rsrp = state_vector[3]  # RSRPå€¼

        # åŸºæ–¼ä¿¡è™Ÿå¼·åº¦è¨ˆç®—ç¢ºå®šæ€§æ“¾å‹•
        # ä¿¡è™Ÿè¶Šå¼±ï¼Œæ¸¬é‡ä¸ç¢ºå®šæ€§è¶Šé«˜ï¼Œæ¢ç´¢åº¦è¶Šå¤§
        measurement_uncertainty = max(0.01, min(0.1, (-rsrp - 100) / 200.0))

        # ç¢ºå®šæ€§æ“¾å‹•å‘é‡ï¼ˆåŸºæ–¼ç‹€æ…‹å‘é‡çš„å“ˆå¸Œå€¼ï¼Œä¿è­‰å¯é‡ç¾æ€§ï¼‰
        state_hash = hash(tuple(state_vector)) % 1000 / 1000.0  # 0-1ç¯„åœ
        perturbation = [(state_hash - 0.5) * measurement_uncertainty for _ in range(len(base_probs))]

        # æ‡‰ç”¨æ“¾å‹•
        perturbed_probs = np.array(base_probs) + np.array(perturbation)
        perturbed_probs = np.maximum(perturbed_probs, 0.01)  # ç¢ºä¿éè² 
        perturbed_probs = perturbed_probs / np.sum(perturbed_probs)  # æ¨™æº–åŒ–

        return perturbed_probs.tolist()

    def estimate_state_value(self, state_vector: List[float]) -> float:
        """ä¼°è¨ˆç‹€æ…‹åƒ¹å€¼ V(s)"""
        rsrp = state_vector[3]
        sinr = state_vector[6]
        elevation = state_vector[4]

        # åƒ¹å€¼ä¼°è¨ˆåŸºæ–¼ä¿¡è™Ÿå“è³ªå’Œä»°è§’
        value = (rsrp + 120) / 40 + sinr / 30 + elevation / 90
        return value / 3  # æ¨™æº–åŒ–åˆ° [0, 1]

    def estimate_policy_gradient(
        self,
        action_probs: List[float],
        advantage: float
    ) -> List[float]:
        """ä¼°è¨ˆç­–ç•¥æ¢¯åº¦ (Policy Gradient)

        SOURCE: Sutton & Barto (2018) "RL: An Introduction" (2nd ed.)
                Chapter 13 - Policy Gradient Methods
        ä¾æ“š: æ¨™æº–ç­–ç•¥æ¢¯åº¦å®šç† (Policy Gradient Theorem)
        """
        # ç­–ç•¥æ¢¯åº¦å…¬å¼: âˆ‡log Ï€(a|s) * A(s,a)
        # å°æ–¼ softmax ç­–ç•¥ï¼Œæ¢¯åº¦æ­£æ¯”æ–¼æ¦‚ç‡å’Œå„ªå‹¢å‡½æ•¸çš„ä¹˜ç©
        gradients = [prob * advantage for prob in action_probs]
        return gradients

    def select_action_from_state(self, state_vector: List[float]) -> int:
        """å¾ç‹€æ…‹é¸æ“‡å‹•ä½œ"""
        action_probs = self.estimate_action_probs(state_vector)
        # æ ¹æ“šæ¦‚ç‡é¸æ“‡å‹•ä½œ
        action_idx = np.argmax(action_probs)
        return action_idx

    def generate_continuous_action(self, state_vector: List[float]) -> List[float]:
        """ç”Ÿæˆé€£çºŒå‹•ä½œå‘é‡ (SAC)"""
        # å°‡é›¢æ•£å‹•ä½œæ˜ å°„åˆ°é€£çºŒç©ºé–“
        # ä¾‹å¦‚: [handover_probability, target_satellite_angle, target_satellite_distance]
        rsrp = state_vector[3]
        elevation = state_vector[4]
        distance = state_vector[5]

        handover_prob = 1.0 - (rsrp + 120) / 40  # RSRP å·® -> æ›æ‰‹æ¦‚ç‡é«˜
        target_angle = elevation / 90.0  # æ¨™æº–åŒ–ä»°è§’
        target_distance = min(distance / 2000.0, 1.0)  # æ¨™æº–åŒ–è·é›¢

        return [handover_prob, target_angle, target_distance]

    def estimate_soft_q_values(
        self,
        state_vector: List[float],
        continuous_action: List[float]
    ) -> List[float]:
        """ä¼°è¨ˆè»Ÿ Q å€¼ (SAC)

        è»Ÿ Q å€¼å…¬å¼: Q(s,a) - Î± * log Ï€(a|s)
        å…¶ä¸­ Î± æ˜¯æº«åº¦åƒæ•¸ï¼Œæ§åˆ¶æ¢ç´¢èˆ‡åˆ©ç”¨çš„å¹³è¡¡

        SOURCE: Haarnoja et al. (2018) "Soft Actor-Critic: Off-Policy Maximum
                Entropy Deep Reinforcement Learning with a Stochastic Actor"
                ICML 2018, Algorithm 2
        """
        # è»Ÿ Q å€¼: Q(s,a) - Î± * log Ï€(a|s)
        base_q_values = self.estimate_q_values(state_vector, state_vector, 0.0)

        # åŠ å…¥ç†µé …
        # SOURCE: Haarnoja et al. (2018) Section 5 - Automatic Temperature Adjustment
        # Î± = 0.2 æ˜¯è‡ªå‹•èª¿æ•´ç‰ˆæœ¬çš„åˆå§‹æº«åº¦åƒæ•¸
        # ä¾æ“š: åœ¨é€£çºŒæ§åˆ¶ä»»å‹™ä¸­å¹³è¡¡æ¢ç´¢èˆ‡åˆ©ç”¨
        alpha = self.config.get('sac_temperature_alpha', 0.2)
        action_probs = self.estimate_action_probs(state_vector)
        entropy_terms = [-alpha * np.log(p + 1e-8) for p in action_probs]

        soft_q_values = [q + h for q, h in zip(base_q_values, entropy_terms)]

        return soft_q_values

    def calculate_policy_entropy(self, state_vector: List[float]) -> float:
        """è¨ˆç®—ç­–ç•¥ç†µ (SAC æ¢ç´¢åº¦é‡)"""
        action_probs = self.estimate_action_probs(state_vector)
        entropy = -sum([p * np.log(p + 1e-8) for p in action_probs])
        return entropy
