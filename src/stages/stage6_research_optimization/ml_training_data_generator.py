"""
ML è¨“ç·´æ•¸æ“šç”Ÿæˆå™¨ - Stage 6 æ ¸å¿ƒçµ„ä»¶

ç”Ÿæˆç”¨æ–¼å¼·åŒ–å­¸ç¿’æ›æ‰‹å„ªåŒ–çš„è¨“ç·´æ•¸æ“šã€‚

ä¾æ“š:
- docs/stages/stage6-research-optimization.md Line 91-96, 597-623
- docs/refactoring/stage6/02-ml-training-data-generator-spec.md

å­¸è¡“åƒè€ƒ:
1. Mnih, V., et al. (2015). Human-level control through deep reinforcement learning. Nature, 518(7540), 529-533.
2. Mnih, V., et al. (2016). Asynchronous methods for deep reinforcement learning. ICML.
3. Schulman, J., et al. (2017). Proximal policy optimization algorithms. arXiv:1707.06347.
4. Haarnoja, T., et al. (2018). Soft actor-critic: Off-policy maximum entropy deep RL. ICML.

Author: ORBIT Engine Team
Created: 2025-09-30
Refactored: 2025-10-02

ğŸ“ å­¸è¡“åˆè¦æ€§æª¢æŸ¥æé†’:
- ä¿®æ”¹æ­¤æ–‡ä»¶å‰ï¼Œè«‹å…ˆé–±è®€: docs/stages/STAGE6_COMPLIANCE_CHECKLIST.md
- é‡é»æª¢æŸ¥: ç¦æ­¢ä½¿ç”¨ np.random() ç”Ÿæˆæ•¸æ“šã€æ‰€æœ‰MLè¶…åƒæ•¸å¿…é ˆæœ‰è«–æ–‡å¼•ç”¨
- å·²ä¿®æ­£: P0-1 ç§»é™¤éš¨æ©Ÿæ•¸ç”Ÿæˆã€P0-3 ä½¿ç”¨Softmaxç­–ç•¥ã€P2-2 æ·»åŠ MLè¶…åƒæ•¸å¼•ç”¨
- ç¦ç”¨è©: å‡è¨­ã€ä¼°è¨ˆã€ç°¡åŒ–ã€æ¨¡æ“¬
"""

import logging
from typing import Dict, Any, List, Optional
from datetime import datetime

from .state_action_encoder import StateActionEncoder
from .reward_calculator import RewardCalculator
from .policy_value_estimator import PolicyValueEstimator
from .datasets import (
    DQNDatasetGenerator,
    A3CDatasetGenerator,
    PPODatasetGenerator,
    SACDatasetGenerator
)


class MLTrainingDataGenerator:
    """ML è¨“ç·´æ•¸æ“šç”Ÿæˆå™¨

    ä¸»å”èª¿å™¨ï¼Œè² è²¬:
    1. åˆå§‹åŒ–æ‰€æœ‰çµ„ä»¶ (ç·¨ç¢¼å™¨ã€è¨ˆç®—å™¨ã€ä¼°è¨ˆå™¨ã€æ•¸æ“šé›†ç”Ÿæˆå™¨)
    2. å”èª¿å„çµ„ä»¶ç”Ÿæˆè¨“ç·´æ•¸æ“š
    3. ç®¡ç†é…ç½®å’Œçµ±è¨ˆä¿¡æ¯

    æ”¯æ´å¼·åŒ–å­¸ç¿’ç®—æ³•:
    - DQN (Deep Q-Network)
    - A3C (Asynchronous Advantage Actor-Critic)
    - PPO (Proximal Policy Optimization)
    - SAC (Soft Actor-Critic)
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """åˆå§‹åŒ–ç”Ÿæˆå™¨

        Args:
            config: é…ç½®åƒæ•¸
                - state_space_dimensions: ç‹€æ…‹ç©ºé–“ç¶­åº¦ (é è¨­ 7)
                - action_space_size: å‹•ä½œç©ºé–“å¤§å° (é è¨­ 5)
                - reward_function_type: çå‹µå‡½æ•¸é¡å‹
                - experience_replay_size: ç¶“é©—å›æ”¾ç·©è¡å€å¤§å°
        """
        self.config = self._load_config(config)
        self.logger = logging.getLogger(__name__)

        # å‹•ä½œç©ºé–“å®šç¾©
        self.action_space = [
            'maintain',
            'handover_to_candidate_1',
            'handover_to_candidate_2',
            'handover_to_candidate_3',
            'handover_to_candidate_4'
        ]

        # åˆå§‹åŒ–çµ„ä»¶
        self.state_encoder = StateActionEncoder(self.action_space)
        self.reward_calculator = RewardCalculator()
        self.policy_value_estimator = PolicyValueEstimator(
            self.action_space,
            self.config
        )

        # åˆå§‹åŒ–æ•¸æ“šé›†ç”Ÿæˆå™¨
        self.dqn_generator = DQNDatasetGenerator(
            self.state_encoder,
            self.reward_calculator,
            self.policy_value_estimator,
            self.action_space
        )

        self.a3c_generator = A3CDatasetGenerator(
            self.state_encoder,
            self.reward_calculator,
            self.policy_value_estimator,
            self.config
        )

        self.ppo_generator = PPODatasetGenerator(
            self.state_encoder,
            self.reward_calculator,
            self.policy_value_estimator,
            self.action_space,
            self.config
        )

        self.sac_generator = SACDatasetGenerator(
            self.state_encoder,
            self.reward_calculator,
            self.policy_value_estimator
        )

        # è¨“ç·´æ¨£æœ¬ç·©è¡å€
        self.training_buffer = {
            'dqn_samples': [],
            'a3c_samples': [],
            'ppo_samples': [],
            'sac_samples': []
        }

        # çµ±è¨ˆ
        self.generation_stats = {
            'total_samples': 0,
            'dqn_samples': 0,
            'a3c_samples': 0,
            'ppo_samples': 0,
            'sac_samples': 0
        }

        self.logger.info("ML è¨“ç·´æ•¸æ“šç”Ÿæˆå™¨åˆå§‹åŒ–å®Œæˆ")

    def generate_all_training_data(
        self,
        signal_analysis: Dict[str, Any],
        gpp_events: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ç”Ÿæˆæ‰€æœ‰ç®—æ³•çš„è¨“ç·´æ•¸æ“š

        Args:
            signal_analysis: Stage 5 ä¿¡è™Ÿåˆ†ææ•¸æ“š
            gpp_events: 3GPP äº‹ä»¶æ•¸æ“š

        Returns:
            {
                'dqn_dataset': {...},
                'a3c_dataset': {...},
                'ppo_dataset': {...},
                'sac_dataset': {...},
                'dataset_summary': {...}
            }
        """
        self.logger.info("é–‹å§‹ç”Ÿæˆæ‰€æœ‰ ML è¨“ç·´æ•¸æ“š")

        try:
            # ç”Ÿæˆå„ç®—æ³•æ•¸æ“šé›†
            dqn_dataset = self.dqn_generator.generate(signal_analysis, gpp_events)
            a3c_dataset = self.a3c_generator.generate(signal_analysis, gpp_events)
            ppo_dataset = self.ppo_generator.generate(signal_analysis, gpp_events)
            sac_dataset = self.sac_generator.generate(signal_analysis, gpp_events)

            # æ›´æ–°çµ±è¨ˆ
            self.generation_stats['dqn_samples'] = dqn_dataset['dataset_size']
            self.generation_stats['a3c_samples'] = a3c_dataset['dataset_size']
            self.generation_stats['ppo_samples'] = ppo_dataset['dataset_size']
            self.generation_stats['sac_samples'] = sac_dataset['dataset_size']
            self.generation_stats['total_samples'] = sum([
                dqn_dataset['dataset_size'],
                a3c_dataset['dataset_size'],
                ppo_dataset['dataset_size'],
                sac_dataset['dataset_size']
            ])

            # æ•¸æ“šé›†æ‘˜è¦
            dataset_summary = {
                'total_samples': self.generation_stats['total_samples'],
                'dqn_samples': self.generation_stats['dqn_samples'],
                'a3c_samples': self.generation_stats['a3c_samples'],
                'ppo_samples': self.generation_stats['ppo_samples'],
                'sac_samples': self.generation_stats['sac_samples'],
                'generation_timestamp': datetime.utcnow().isoformat() + 'Z',
                'state_space_dimensions': self.config['state_space_dimensions'],
                'action_space_size': self.config['action_space_size']
            }

            self.logger.info(f"ML è¨“ç·´æ•¸æ“šç”Ÿæˆå®Œæˆ - ç¸½æ¨£æœ¬æ•¸: {self.generation_stats['total_samples']}")

            return {
                'dqn_dataset': dqn_dataset,
                'a3c_dataset': a3c_dataset,
                'ppo_dataset': ppo_dataset,
                'sac_dataset': sac_dataset,
                'dataset_summary': dataset_summary
            }

        except Exception as e:
            self.logger.error(f"ç”Ÿæˆ ML è¨“ç·´æ•¸æ“šæ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}", exc_info=True)
            return {
                'dqn_dataset': {},
                'a3c_dataset': {},
                'ppo_dataset': {},
                'sac_dataset': {},
                'dataset_summary': {'error': str(e)}
            }

    def _load_config(self, config: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        """è¼‰å…¥ä¸¦åˆä½µé…ç½®åƒæ•¸

        æ‰€æœ‰è¶…åƒæ•¸å‡åŸºæ–¼å­¸è¡“è«–æ–‡çš„æ¨™æº–å¯¦é©—è¨­ç½®
        """
        default_config = {
            # ============================================================
            # å¼·åŒ–å­¸ç¿’ç’°å¢ƒå®šç¾©
            # ============================================================
            # SOURCE: docs/stages/stage6-research-optimization.md Line 597-623
            # ä¾æ“š: è¡›æ˜Ÿæ›æ‰‹æ±ºç­– MDP å»ºæ¨¡
            'state_space_dimensions': 7,  # [lat, lon, alt, rsrp, elev, dist, sinr]
            'action_space_size': 5,       # [maintain, ho_1, ho_2, ho_3, ho_4]

            # ============================================================
            # çå‹µå‡½æ•¸é…ç½®
            # ============================================================
            # SOURCE: Stage 6 æ›æ‰‹æ±ºç­–ç›®æ¨™
            # è¤‡åˆçå‹µ: QoSæ”¹å–„ + ä¸­æ–·æ‡²ç½° + ä¿¡è™Ÿå“è³ª + æ›æ‰‹æˆæœ¬
            'reward_function_type': 'composite_qos_interruption_quality',

            # ============================================================
            # DQN è¶…åƒæ•¸
            # ============================================================
            # SOURCE: Mnih et al. (2015) "Human-level control through deep RL"
            #         Nature 518(7540), 529-533
            # ç¶“é©—å›æ”¾ç·©è¡å€å¤§å°: 100,000 transitions
            'experience_replay_size': 100000,

            # æ‰¹æ¬¡å¤§å°: 256
            # SOURCE: Rainbow DQN æ”¹é€²ç‰ˆæœ¬çš„æ¨™æº–è¨­ç½®
            'batch_size': 256,

            # å­¸ç¿’ç‡: 0.001 (Adam optimizer)
            # SOURCE: æ·±åº¦å¼·åŒ–å­¸ç¿’æ¨™æº–å­¸ç¿’ç‡
            'learning_rate': 0.001,

            # æŠ˜æ‰£å› å­ Î³: 0.99
            # SOURCE: å¼·åŒ–å­¸ç¿’æ¨™æº–æŠ˜æ‰£å› å­ï¼Œé©åˆé•·æœŸè¦åŠƒä»»å‹™
            'discount_factor': 0.99,

            # ============================================================
            # PPO è¶…åƒæ•¸
            # ============================================================
            # SOURCE: Schulman et al. (2017) "Proximal Policy Optimization"
            #         arXiv:1707.06347v2, Section 6.1, Table 3
            # è£å‰ªåƒæ•¸ Îµ: 0.2 (è«–æ–‡æ¨™æº–å€¼)
            # ä¾æ“š: åœ¨ Atariã€MuJoCoã€Roboschool ç’°å¢ƒä¸­é©—è­‰
            'ppo_clip_epsilon': 0.2,

            # ============================================================
            # SAC è¶…åƒæ•¸
            # ============================================================
            # SOURCE: Haarnoja et al. (2018) "Soft Actor-Critic"
            #         ICML 2018, Algorithm 2, Section 5
            # æº«åº¦åƒæ•¸ Î±: 0.2 (è‡ªå‹•èª¿æ•´ç‰ˆæœ¬çš„åˆå§‹å€¼)
            # ä¾æ“š: æ§åˆ¶æ¢ç´¢èˆ‡åˆ©ç”¨çš„å¹³è¡¡ï¼Œé©åˆé€£çºŒæ§åˆ¶ä»»å‹™
            'sac_temperature_alpha': 0.2
        }

        if config:
            default_config.update(config)

        return default_config
