"""
Stage 4 Optimization Performance Monitor

This module provides comprehensive performance monitoring, metrics collection,
and benchmarking for the Stage 4 optimization decision layer.

Features:
- Real-time performance tracking
- Memory usage monitoring
- Decision quality metrics
- Algorithm convergence analysis
- Bottleneck identification
- Performance alerts and thresholds
"""

import logging
import time
import psutil
import threading
from typing import Dict, Any, List, Optional, Callable
from datetime import datetime, timezone, timedelta
from dataclasses import dataclass, asdict
from collections import deque
import statistics
import json

logger = logging.getLogger(__name__)


@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ¨™æ•¸æ“šçµæ§‹"""
    timestamp: str
    processing_time_seconds: float
    memory_usage_mb: float
    cpu_usage_percent: float
    satellites_processed: int
    decision_quality_score: float
    constraint_satisfaction_rate: float
    optimization_effectiveness: float
    algorithm_convergence: str
    error_count: int


@dataclass
class AlgorithmBenchmark:
    """ç®—æ³•åŸºæº–æ¸¬è©¦çµæœ"""
    algorithm_name: str
    input_size: int
    processing_time: float
    memory_peak: float
    quality_score: float
    convergence_iterations: int
    success_rate: float


class PerformanceMonitor:
    """æ€§èƒ½ç›£æ§å™¨"""

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        self.logger = logging.getLogger(f"{__name__}.PerformanceMonitor")

        # ç›£æ§é…ç½®
        self.enable_detailed_metrics = self.config.get('enable_detailed_metrics', True)
        self.enable_real_time_monitoring = self.config.get('enable_real_time_monitoring', False)
        self.metric_collection_interval = self.config.get('metric_collection_interval', 1.0)
        self.max_history_size = self.config.get('max_history_size', 1000)

        # æ€§èƒ½ç›®æ¨™
        self.benchmark_targets = self.config.get('benchmark_targets', {
            'processing_time_max_seconds': 10.0,
            'memory_usage_max_mb': 300,
            'decision_quality_min_score': 0.8,
            'constraint_satisfaction_min_rate': 0.95
        })

        # ç›£æ§æ•¸æ“š
        self.metrics_history: deque = deque(maxlen=self.max_history_size)
        self.algorithm_benchmarks: List[AlgorithmBenchmark] = []
        self.performance_alerts: List[Dict[str, Any]] = []

        # å¯¦æ™‚ç›£æ§
        self.monitoring_active = False
        self.monitoring_thread: Optional[threading.Thread] = None

        # çµ±è¨ˆä¿¡æ¯
        self.total_processes = 0
        self.total_errors = 0
        self.start_time = datetime.now(timezone.utc)

        # ç•¶å‰è™•ç†è¿½è¹¤
        self.current_process_start: Optional[float] = None
        self.current_process_metrics: Dict[str, Any] = {}

        self.logger.info("âœ… æ€§èƒ½ç›£æ§å™¨åˆå§‹åŒ–å®Œæˆ")

    def start_process_monitoring(self, process_info: Dict[str, Any] = None):
        """é–‹å§‹è™•ç†éç¨‹ç›£æ§"""
        self.current_process_start = time.time()
        self.current_process_metrics = {
            'process_info': process_info or {},
            'start_time': self.current_process_start,
            'start_memory': self._get_memory_usage(),
            'start_cpu': self._get_cpu_usage(),
            'error_count': 0
        }

        if self.enable_real_time_monitoring and not self.monitoring_active:
            self._start_real_time_monitoring()

        self.logger.debug("ğŸ“Š é–‹å§‹è™•ç†éç¨‹ç›£æ§")

    def end_process_monitoring(self, results: Dict[str, Any] = None) -> PerformanceMetrics:
        """çµæŸè™•ç†éç¨‹ç›£æ§ä¸¦æ”¶é›†æŒ‡æ¨™"""
        if self.current_process_start is None:
            self.logger.warning("âš ï¸ æ²’æœ‰æ­£åœ¨é€²è¡Œçš„è™•ç†éç¨‹ç›£æ§")
            return self._create_empty_metrics()

        end_time = time.time()
        processing_time = end_time - self.current_process_start

        # æ”¶é›†æ€§èƒ½æŒ‡æ¨™
        metrics = PerformanceMetrics(
            timestamp=datetime.now(timezone.utc).isoformat(),
            processing_time_seconds=processing_time,
            memory_usage_mb=self._get_memory_usage(),
            cpu_usage_percent=self._get_cpu_usage(),
            satellites_processed=self._extract_satellites_count(results),
            decision_quality_score=self._calculate_decision_quality(results),
            constraint_satisfaction_rate=self._calculate_constraint_satisfaction(results),
            optimization_effectiveness=self._calculate_optimization_effectiveness(results),
            algorithm_convergence=self._assess_algorithm_convergence(results),
            error_count=self.current_process_metrics.get('error_count', 0)
        )

        # å­˜å„²æŒ‡æ¨™
        self.metrics_history.append(metrics)
        self.total_processes += 1

        # æª¢æŸ¥æ€§èƒ½è­¦å ±
        self._check_performance_alerts(metrics)

        # é‡ç½®ç•¶å‰è™•ç†è¿½è¹¤
        self.current_process_start = None
        self.current_process_metrics = {}

        if self.enable_real_time_monitoring:
            self._stop_real_time_monitoring()

        self.logger.info(f"ğŸ“Š è™•ç†ç›£æ§å®Œæˆ: {processing_time:.3f}s")
        return metrics

    def record_error(self, error: Exception, context: Dict[str, Any] = None):
        """è¨˜éŒ„éŒ¯èª¤"""
        self.total_errors += 1
        if self.current_process_metrics:
            self.current_process_metrics['error_count'] += 1

        error_record = {
            'timestamp': datetime.now(timezone.utc).isoformat(),
            'error_type': type(error).__name__,
            'error_message': str(error),
            'context': context or {},
            'process_info': self.current_process_metrics.get('process_info', {})
        }

        self.logger.error(f"âŒ éŒ¯èª¤è¨˜éŒ„: {error_record}")

    def benchmark_algorithm(self, algorithm_name: str, algorithm_func: Callable,
                          test_inputs: List[Any], description: str = "") -> AlgorithmBenchmark:
        """ç®—æ³•åŸºæº–æ¸¬è©¦"""
        self.logger.info(f"ğŸ é–‹å§‹ç®—æ³•åŸºæº–æ¸¬è©¦: {algorithm_name}")

        results = []
        total_time = 0
        peak_memory = 0
        success_count = 0

        for i, test_input in enumerate(test_inputs):
            try:
                # è¨˜éŒ„é–‹å§‹ç‹€æ…‹
                start_time = time.time()
                start_memory = self._get_memory_usage()

                # åŸ·è¡Œç®—æ³•
                result = algorithm_func(test_input)

                # è¨˜éŒ„çµæŸç‹€æ…‹
                end_time = time.time()
                end_memory = self._get_memory_usage()

                # è¨ˆç®—æŒ‡æ¨™
                processing_time = end_time - start_time
                memory_used = end_memory - start_memory

                total_time += processing_time
                peak_memory = max(peak_memory, memory_used)
                success_count += 1

                results.append({
                    'input_size': len(test_input) if hasattr(test_input, '__len__') else 1,
                    'processing_time': processing_time,
                    'memory_used': memory_used,
                    'result': result,
                    'success': True
                })

            except Exception as e:
                results.append({
                    'error': str(e),
                    'success': False
                })
                self.logger.error(f"âŒ åŸºæº–æ¸¬è©¦å¤±æ•— (è¼¸å…¥ {i}): {e}")

        # è¨ˆç®—åŸºæº–æŒ‡æ¨™
        avg_input_size = statistics.mean([r.get('input_size', 0) for r in results if r.get('success')])
        avg_processing_time = total_time / len(test_inputs) if test_inputs else 0
        success_rate = success_count / len(test_inputs) if test_inputs else 0

        # ä¼°ç®—è³ªé‡åˆ†æ•¸ (åŸºæ–¼æˆåŠŸç‡å’Œè™•ç†æ™‚é–“)
        quality_score = success_rate * (1.0 / (1.0 + avg_processing_time))

        # ä¼°ç®—æ”¶æ–‚è¿­ä»£æ¬¡æ•¸ (ç°¡åŒ–)
        convergence_iterations = max(1, int(avg_processing_time * 10))

        benchmark = AlgorithmBenchmark(
            algorithm_name=algorithm_name,
            input_size=int(avg_input_size),
            processing_time=avg_processing_time,
            memory_peak=peak_memory,
            quality_score=quality_score,
            convergence_iterations=convergence_iterations,
            success_rate=success_rate
        )

        self.algorithm_benchmarks.append(benchmark)

        self.logger.info(f"âœ… ç®—æ³•åŸºæº–æ¸¬è©¦å®Œæˆ: {algorithm_name} (æˆåŠŸç‡: {success_rate:.2%})")
        return benchmark

    def get_performance_summary(self) -> Dict[str, Any]:
        """ç²å–æ€§èƒ½æ‘˜è¦"""
        if not self.metrics_history:
            return {'status': 'no_data', 'total_processes': self.total_processes}

        recent_metrics = list(self.metrics_history)[-10:]  # æœ€è¿‘10æ¬¡

        summary = {
            'total_processes': self.total_processes,
            'total_errors': self.total_errors,
            'error_rate': self.total_errors / max(1, self.total_processes),
            'uptime_hours': (datetime.now(timezone.utc) - self.start_time).total_seconds() / 3600,

            'processing_time': {
                'average': statistics.mean([m.processing_time_seconds for m in recent_metrics]),
                'median': statistics.median([m.processing_time_seconds for m in recent_metrics]),
                'max': max([m.processing_time_seconds for m in recent_metrics]),
                'min': min([m.processing_time_seconds for m in recent_metrics])
            },

            'memory_usage': {
                'average': statistics.mean([m.memory_usage_mb for m in recent_metrics]),
                'peak': max([m.memory_usage_mb for m in recent_metrics])
            },

            'decision_quality': {
                'average': statistics.mean([m.decision_quality_score for m in recent_metrics]),
                'trend': self._calculate_trend([m.decision_quality_score for m in recent_metrics])
            },

            'constraint_satisfaction': {
                'average': statistics.mean([m.constraint_satisfaction_rate for m in recent_metrics]),
                'compliance_rate': sum(1 for m in recent_metrics
                                     if m.constraint_satisfaction_rate >=
                                     self.benchmark_targets.get('constraint_satisfaction_min_rate', 0.95)) / len(recent_metrics)
            },

            'performance_alerts': len(self.performance_alerts),
            'benchmark_results': len(self.algorithm_benchmarks)
        }

        return summary

    def get_detailed_metrics(self, limit: int = 100) -> List[Dict[str, Any]]:
        """ç²å–è©³ç´°æŒ‡æ¨™"""
        metrics_list = list(self.metrics_history)[-limit:]
        return [asdict(m) for m in metrics_list]

    def get_algorithm_benchmarks(self) -> List[Dict[str, Any]]:
        """ç²å–ç®—æ³•åŸºæº–æ¸¬è©¦çµæœ"""
        return [asdict(b) for b in self.algorithm_benchmarks]

    def get_performance_alerts(self, severity: str = None) -> List[Dict[str, Any]]:
        """ç²å–æ€§èƒ½è­¦å ±"""
        if severity:
            return [alert for alert in self.performance_alerts
                   if alert.get('severity') == severity]
        return self.performance_alerts.copy()

    def clear_performance_data(self, data_type: str = 'all'):
        """æ¸…é™¤æ€§èƒ½æ•¸æ“š"""
        if data_type in ['all', 'metrics']:
            self.metrics_history.clear()
        if data_type in ['all', 'benchmarks']:
            self.algorithm_benchmarks.clear()
        if data_type in ['all', 'alerts']:
            self.performance_alerts.clear()

        self.logger.info(f"ğŸ§¹ æ€§èƒ½æ•¸æ“šå·²æ¸…é™¤: {data_type}")

    def export_performance_data(self, export_path: str, format: str = 'json'):
        """å°å‡ºæ€§èƒ½æ•¸æ“š"""
        try:
            data = {
                'summary': self.get_performance_summary(),
                'metrics': self.get_detailed_metrics(),
                'benchmarks': self.get_algorithm_benchmarks(),
                'alerts': self.get_performance_alerts(),
                'export_timestamp': datetime.now(timezone.utc).isoformat()
            }

            if format.lower() == 'json':
                with open(export_path, 'w', encoding='utf-8') as f:
                    json.dump(data, f, indent=2, ensure_ascii=False)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„å°å‡ºæ ¼å¼: {format}")

            self.logger.info(f"âœ… æ€§èƒ½æ•¸æ“šå°å‡ºæˆåŠŸ: {export_path}")

        except Exception as e:
            self.logger.error(f"âŒ æ€§èƒ½æ•¸æ“šå°å‡ºå¤±æ•—: {e}")
            raise

    def _get_memory_usage(self) -> float:
        """ç²å–å…§å­˜ä½¿ç”¨é‡ (MB)"""
        try:
            process = psutil.Process()
            return process.memory_info().rss / 1024 / 1024
        except:
            return 0.0

    def _get_cpu_usage(self) -> float:
        """ç²å–CPUä½¿ç”¨ç‡"""
        try:
            return psutil.cpu_percent(interval=0.1)
        except:
            return 0.0

    def _extract_satellites_count(self, results: Dict[str, Any]) -> int:
        """æå–è™•ç†çš„è¡›æ˜Ÿæ•¸é‡"""
        if not results:
            return 0

        # å˜—è©¦å¾å¤šå€‹å¯èƒ½çš„ä½ç½®æå–è¡›æ˜Ÿæ•¸é‡
        paths = [
            ['metadata', 'optimized_satellites'],
            ['optimal_pool', 'selected_satellites'],
            ['statistics', 'satellites_processed']
        ]

        for path in paths:
            value = results
            for key in path:
                if isinstance(value, dict) and key in value:
                    value = value[key]
                else:
                    value = None
                    break

            if isinstance(value, int):
                return value
            elif isinstance(value, list):
                return len(value)

        return 0

    def _calculate_decision_quality(self, results: Dict[str, Any]) -> float:
        """è¨ˆç®—æ±ºç­–è³ªé‡åˆ†æ•¸"""
        if not results:
            return 0.0

        # åŸºæ–¼å¤šå€‹å› ç´ è¨ˆç®—è³ªé‡åˆ†æ•¸
        quality_factors = []

        # æª¢æŸ¥æ˜¯å¦æœ‰æœ€å„ªè§£
        if results.get('optimization_results', {}).get('pareto_solutions'):
            pareto_count = len(results['optimization_results']['pareto_solutions'])
            quality_factors.append(min(1.0, pareto_count / 10.0))

        # æª¢æŸ¥ç´„æŸæ»¿è¶³
        constraint_rate = self._calculate_constraint_satisfaction(results)
        quality_factors.append(constraint_rate)

        # æª¢æŸ¥æ± è¦åŠƒè³ªé‡
        pools = results.get('optimal_pool', {}).get('selected_satellites', [])
        if pools:
            pool_quality = min(1.0, len(pools) / 5.0)
            quality_factors.append(pool_quality)

        return statistics.mean(quality_factors) if quality_factors else 0.5

    def _calculate_constraint_satisfaction(self, results: Dict[str, Any]) -> float:
        """è¨ˆç®—ç´„æŸæ»¿è¶³ç‡"""
        # ç°¡åŒ–å¯¦ç¾ï¼šåŸºæ–¼çµæœçµæ§‹è©•ä¼°
        if not results:
            return 0.0

        satisfaction_score = 0.8  # åŸºæº–åˆ†æ•¸

        # æª¢æŸ¥æ˜¯å¦æœ‰éŒ¯èª¤
        if 'error' in results:
            satisfaction_score -= 0.3

        # æª¢æŸ¥é—œéµçµæœæ˜¯å¦å­˜åœ¨
        required_sections = ['optimal_pool', 'handover_strategy', 'optimization_results']
        present_sections = sum(1 for section in required_sections if section in results)
        satisfaction_score *= (present_sections / len(required_sections))

        return max(0.0, min(1.0, satisfaction_score))

    def _calculate_optimization_effectiveness(self, results: Dict[str, Any]) -> float:
        """è¨ˆç®—å„ªåŒ–æ•ˆæœ"""
        if not results:
            return 0.0

        effectiveness_factors = []

        # å¤šç›®æ¨™å„ªåŒ–æ•ˆæœ
        pareto_solutions = results.get('optimization_results', {}).get('pareto_solutions', [])
        if pareto_solutions:
            effectiveness_factors.append(min(1.0, len(pareto_solutions) / 5.0))

        # æ›æ‰‹ç­–ç•¥æ•ˆæœ
        strategies = results.get('handover_strategy', {}).get('triggers', [])
        if strategies:
            effectiveness_factors.append(min(1.0, len(strategies) / 3.0))

        return statistics.mean(effectiveness_factors) if effectiveness_factors else 0.5

    def _assess_algorithm_convergence(self, results: Dict[str, Any]) -> str:
        """è©•ä¼°ç®—æ³•æ”¶æ–‚æ€§"""
        if not results:
            return 'no_data'

        # åŸºæ–¼çµæœå®Œæ•´æ€§è©•ä¼°æ”¶æ–‚
        completeness_score = 0

        if results.get('optimal_pool', {}).get('selected_satellites'):
            completeness_score += 1

        if results.get('optimization_results', {}).get('pareto_solutions'):
            completeness_score += 1

        if results.get('handover_strategy', {}).get('triggers'):
            completeness_score += 1

        if completeness_score >= 3:
            return 'excellent_convergence'
        elif completeness_score >= 2:
            return 'good_convergence'
        elif completeness_score >= 1:
            return 'partial_convergence'
        else:
            return 'poor_convergence'

    def _check_performance_alerts(self, metrics: PerformanceMetrics):
        """æª¢æŸ¥æ€§èƒ½è­¦å ±"""
        alerts = []

        # è™•ç†æ™‚é–“è­¦å ±
        max_time = self.benchmark_targets.get('processing_time_max_seconds', 10.0)
        if metrics.processing_time_seconds > max_time:
            alerts.append({
                'type': 'processing_time_exceeded',
                'severity': 'warning' if metrics.processing_time_seconds < max_time * 1.5 else 'critical',
                'value': metrics.processing_time_seconds,
                'threshold': max_time,
                'timestamp': metrics.timestamp
            })

        # å…§å­˜ä½¿ç”¨è­¦å ±
        max_memory = self.benchmark_targets.get('memory_usage_max_mb', 300)
        if metrics.memory_usage_mb > max_memory:
            alerts.append({
                'type': 'memory_usage_exceeded',
                'severity': 'warning' if metrics.memory_usage_mb < max_memory * 1.5 else 'critical',
                'value': metrics.memory_usage_mb,
                'threshold': max_memory,
                'timestamp': metrics.timestamp
            })

        # æ±ºç­–è³ªé‡è­¦å ±
        min_quality = self.benchmark_targets.get('decision_quality_min_score', 0.8)
        if metrics.decision_quality_score < min_quality:
            alerts.append({
                'type': 'decision_quality_low',
                'severity': 'warning' if metrics.decision_quality_score > min_quality * 0.8 else 'critical',
                'value': metrics.decision_quality_score,
                'threshold': min_quality,
                'timestamp': metrics.timestamp
            })

        # å­˜å„²è­¦å ±
        self.performance_alerts.extend(alerts)

        # é™åˆ¶è­¦å ±æ­·å²å¤§å°
        if len(self.performance_alerts) > 100:
            self.performance_alerts = self.performance_alerts[-100:]

    def _calculate_trend(self, values: List[float]) -> str:
        """è¨ˆç®—è¶¨å‹¢"""
        if len(values) < 2:
            return 'insufficient_data'

        recent_avg = statistics.mean(values[-3:]) if len(values) >= 3 else values[-1]
        earlier_avg = statistics.mean(values[:-3]) if len(values) >= 6 else values[0]

        diff = recent_avg - earlier_avg
        if abs(diff) < 0.01:
            return 'stable'
        elif diff > 0:
            return 'improving'
        else:
            return 'declining'

    def _create_empty_metrics(self) -> PerformanceMetrics:
        """å‰µå»ºç©ºæŒ‡æ¨™"""
        return PerformanceMetrics(
            timestamp=datetime.now(timezone.utc).isoformat(),
            processing_time_seconds=0.0,
            memory_usage_mb=0.0,
            cpu_usage_percent=0.0,
            satellites_processed=0,
            decision_quality_score=0.0,
            constraint_satisfaction_rate=0.0,
            optimization_effectiveness=0.0,
            algorithm_convergence='no_data',
            error_count=0
        )

    def _start_real_time_monitoring(self):
        """é–‹å§‹å¯¦æ™‚ç›£æ§"""
        if self.monitoring_active:
            return

        self.monitoring_active = True
        self.monitoring_thread = threading.Thread(target=self._real_time_monitor_loop)
        self.monitoring_thread.daemon = True
        self.monitoring_thread.start()

        self.logger.debug("ğŸ”„ å¯¦æ™‚ç›£æ§å·²å•Ÿå‹•")

    def _stop_real_time_monitoring(self):
        """åœæ­¢å¯¦æ™‚ç›£æ§"""
        self.monitoring_active = False
        if self.monitoring_thread:
            self.monitoring_thread.join(timeout=2.0)

        self.logger.debug("â¸ï¸ å¯¦æ™‚ç›£æ§å·²åœæ­¢")

    def _real_time_monitor_loop(self):
        """å¯¦æ™‚ç›£æ§å¾ªç’°"""
        while self.monitoring_active:
            try:
                # æ”¶é›†å¯¦æ™‚æŒ‡æ¨™
                current_metrics = {
                    'timestamp': datetime.now(timezone.utc).isoformat(),
                    'memory_usage_mb': self._get_memory_usage(),
                    'cpu_usage_percent': self._get_cpu_usage()
                }

                # å¯ä»¥åœ¨é€™è£¡æ·»åŠ å¯¦æ™‚è­¦å ±é‚è¼¯

                time.sleep(self.metric_collection_interval)

            except Exception as e:
                self.logger.error(f"âŒ å¯¦æ™‚ç›£æ§éŒ¯èª¤: {e}")
                time.sleep(1.0)