"""
Reinforcement Learning Extension Interface for Stage 4 Optimization

This module provides extension points for integrating reinforcement learning
algorithms with the existing optimization decision layer, as specified in
@docs/stages/stage4-optimization.md

Key Features:
- RL Agent Interface for custom algorithms
- State representation for satellite environments
- Action space definition for optimization decisions
- Reward function framework
- Experience replay and training coordination
"""

import logging
from abc import ABC, abstractmethod
from typing import Dict, List, Any, Optional, Union, Tuple
from datetime import datetime, timezone
import numpy as np
from dataclasses import dataclass

logger = logging.getLogger(__name__)


@dataclass
class RLState:
    """RLç’°å¢ƒç‹€æ…‹è¡¨ç¤º"""
    satellite_positions: List[Dict[str, float]]
    signal_strengths: List[float]
    coverage_metrics: Dict[str, float]
    handover_history: List[Dict[str, Any]]
    timestamp: str
    metadata: Dict[str, Any]


@dataclass
class RLAction:
    """RLå‹•ä½œè¡¨ç¤º"""
    action_type: str  # 'satellite_selection', 'handover_trigger', 'pool_adjustment'
    target_satellites: List[str]
    parameters: Dict[str, Any]
    confidence: float


@dataclass
class RLReward:
    """RLçå‹µä¿¡è™Ÿ"""
    total_reward: float
    component_rewards: Dict[str, float]  # signal_quality, coverage, cost, efficiency
    penalty_factors: Dict[str, float]
    normalized_score: float


class RLAgentInterface(ABC):
    """å¼·åŒ–å­¸ç¿’ä»£ç†æ¥å£"""

    @abstractmethod
    def select_action(self, state: RLState) -> RLAction:
        """æ ¹æ“šç•¶å‰ç‹€æ…‹é¸æ“‡å‹•ä½œ"""
        pass

    @abstractmethod
    def update_policy(self, state: RLState, action: RLAction, reward: RLReward, next_state: RLState):
        """æ›´æ–°ç­–ç•¥ç¶²çµ¡"""
        pass

    @abstractmethod
    def save_model(self, path: str) -> bool:
        """ä¿å­˜æ¨¡å‹"""
        pass

    @abstractmethod
    def load_model(self, path: str) -> bool:
        """è¼‰å…¥æ¨¡å‹"""
        pass


class RLEnvironmentAdapter:
    """RLç’°å¢ƒé©é…å™¨ - å°‡è¡›æ˜Ÿå„ªåŒ–å•é¡Œè½‰æ›ç‚ºRLç’°å¢ƒ"""

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        self.logger = logging.getLogger(f"{__name__}.RLEnvironmentAdapter")

        # RLç’°å¢ƒé…ç½®
        self.state_dimensions = self.config.get('state_dimensions', 128)
        self.action_space_size = self.config.get('action_space_size', 64)
        self.reward_weights = self.config.get('reward_weights', {
            'signal_quality': 0.4,
            'coverage': 0.3,
            'handover_cost': -0.2,
            'energy_efficiency': 0.1
        })

        # ç’°å¢ƒç‹€æ…‹
        self.current_state = None
        self.step_count = 0
        self.episode_reward = 0.0

        self.logger.info("âœ… RLç’°å¢ƒé©é…å™¨åˆå§‹åŒ–å®Œæˆ")

    def convert_to_rl_state(self, optimization_data: Dict[str, Any]) -> RLState:
        """å°‡å„ªåŒ–æ•¸æ“šè½‰æ›ç‚ºRLç‹€æ…‹"""
        try:
            # æå–è¡›æ˜Ÿä½ç½®æ•¸æ“š
            satellite_positions = []
            signal_strengths = []

            candidates = optimization_data.get('candidates', [])
            for candidate in candidates:
                if 'signal_analysis' in candidate:
                    signal_data = candidate['signal_analysis']

                    # æå–ä½ç½®ä¿¡æ¯
                    position_data = {
                        'latitude': signal_data.get('latitude', 0.0),
                        'longitude': signal_data.get('longitude', 0.0),
                        'altitude': signal_data.get('altitude_km', 0.0)
                    }
                    satellite_positions.append(position_data)

                    # æå–ä¿¡è™Ÿå¼·åº¦
                    signal_strength = signal_data.get('average_signal_strength', -120.0)
                    signal_strengths.append(signal_strength)

            # è¨ˆç®—è¦†è“‹æŒ‡æ¨™
            coverage_metrics = {
                'total_satellites': len(candidates),
                'average_signal_strength': np.mean(signal_strengths) if signal_strengths else -120.0,
                'coverage_area': self._estimate_coverage_area(satellite_positions),
                'signal_diversity': np.std(signal_strengths) if len(signal_strengths) > 1 else 0.0
            }

            # æ§‹å»ºRLç‹€æ…‹
            rl_state = RLState(
                satellite_positions=satellite_positions,
                signal_strengths=signal_strengths,
                coverage_metrics=coverage_metrics,
                handover_history=optimization_data.get('handover_history', []),
                timestamp=datetime.now(timezone.utc).isoformat(),
                metadata={
                    'state_id': f"rl_state_{self.step_count}",
                    'environment_version': '1.0.0',
                    'adaptation_timestamp': datetime.now(timezone.utc).isoformat()
                }
            )

            self.current_state = rl_state
            self.step_count += 1

            return rl_state

        except Exception as e:
            self.logger.error(f"RLç‹€æ…‹è½‰æ›å¤±æ•—: {e}")
            raise

    def convert_from_rl_action(self, rl_action: RLAction) -> Dict[str, Any]:
        """å°‡RLå‹•ä½œè½‰æ›ç‚ºå„ªåŒ–æ±ºç­–"""
        try:
            optimization_decision = {
                'decision_type': rl_action.action_type,
                'selected_satellites': rl_action.target_satellites,
                'optimization_parameters': rl_action.parameters,
                'confidence_score': rl_action.confidence,
                'rl_generated': True,
                'timestamp': datetime.now(timezone.utc).isoformat()
            }

            return optimization_decision

        except Exception as e:
            self.logger.error(f"RLå‹•ä½œè½‰æ›å¤±æ•—: {e}")
            raise

    def calculate_reward(self, previous_state: RLState, action: RLAction,
                        current_state: RLState, optimization_results: Dict[str, Any]) -> RLReward:
        """è¨ˆç®—çå‹µä¿¡è™Ÿ"""
        try:
            component_rewards = {}
            penalty_factors = {}

            # ä¿¡è™Ÿå“è³ªçå‹µ
            prev_signal_avg = np.mean(previous_state.signal_strengths) if previous_state.signal_strengths else -120.0
            curr_signal_avg = np.mean(current_state.signal_strengths) if current_state.signal_strengths else -120.0
            signal_improvement = curr_signal_avg - prev_signal_avg
            component_rewards['signal_quality'] = signal_improvement * self.reward_weights['signal_quality']

            # è¦†è“‹ç¯„åœçå‹µ
            coverage_improvement = (current_state.coverage_metrics.get('coverage_area', 0) -
                                  previous_state.coverage_metrics.get('coverage_area', 0))
            component_rewards['coverage'] = coverage_improvement * self.reward_weights['coverage']

            # æ›æ‰‹æˆæœ¬æ‡²ç½°
            handover_cost = len(current_state.handover_history) - len(previous_state.handover_history)
            component_rewards['handover_cost'] = handover_cost * self.reward_weights['handover_cost']

            # èƒ½æ•ˆçå‹µ
            energy_score = optimization_results.get('energy_efficiency_score', 0.5)
            component_rewards['energy_efficiency'] = energy_score * self.reward_weights['energy_efficiency']

            # ç´„æŸé•åæ‡²ç½°
            constraint_violations = optimization_results.get('constraint_violations', 0)
            if constraint_violations > 0:
                penalty_factors['constraint_violation'] = -10.0 * constraint_violations

            # è¨ˆç®—ç¸½çå‹µ
            total_reward = sum(component_rewards.values()) + sum(penalty_factors.values())

            # æ¨™æº–åŒ–åˆ†æ•¸ (0-1ç¯„åœ)
            normalized_score = max(0.0, min(1.0, (total_reward + 10.0) / 20.0))

            rl_reward = RLReward(
                total_reward=total_reward,
                component_rewards=component_rewards,
                penalty_factors=penalty_factors,
                normalized_score=normalized_score
            )

            self.episode_reward += total_reward

            return rl_reward

        except Exception as e:
            self.logger.error(f"çå‹µè¨ˆç®—å¤±æ•—: {e}")
            raise

    def _estimate_coverage_area(self, satellite_positions: List[Dict[str, float]]) -> float:
        """ä¼°ç®—è¦†è“‹é¢ç©"""
        if not satellite_positions:
            return 0.0

        # ç°¡åŒ–çš„è¦†è“‹é¢ç©è¨ˆç®— (åŸºæ–¼è¡›æ˜Ÿæ•¸é‡å’Œåˆ†å¸ƒ)
        num_satellites = len(satellite_positions)

        # è¨ˆç®—ä½ç½®åˆ†æ•£åº¦
        if num_satellites > 1:
            latitudes = [pos['latitude'] for pos in satellite_positions]
            longitudes = [pos['longitude'] for pos in satellite_positions]

            lat_spread = max(latitudes) - min(latitudes)
            lon_spread = max(longitudes) - min(longitudes)

            # è¦†è“‹é¢ç© = åŸºç¤è¦†è“‹ Ã— è¡›æ˜Ÿæ•¸é‡ Ã— åˆ†æ•£åº¦å› å­
            base_coverage = 1000.0  # kmÂ²
            spread_factor = 1.0 + (lat_spread + lon_spread) / 180.0
            coverage_area = base_coverage * num_satellites * spread_factor
        else:
            coverage_area = 1000.0  # å–®è¡›æ˜ŸåŸºç¤è¦†è“‹

        return coverage_area

    def reset_environment(self):
        """é‡ç½®ç’°å¢ƒç‹€æ…‹"""
        self.current_state = None
        self.step_count = 0
        self.episode_reward = 0.0
        self.logger.info("ğŸ”„ RLç’°å¢ƒå·²é‡ç½®")

    def get_environment_info(self) -> Dict[str, Any]:
        """ç²å–ç’°å¢ƒä¿¡æ¯"""
        return {
            'state_dimensions': self.state_dimensions,
            'action_space_size': self.action_space_size,
            'reward_weights': self.reward_weights,
            'current_step': self.step_count,
            'episode_reward': self.episode_reward,
            'environment_version': '1.0.0'
        }


class RLExtensionCoordinator:
    """RLæ“´å±•å”èª¿å™¨ - ç®¡ç†RLèˆ‡å‚³çµ±å„ªåŒ–çš„æ•´åˆ"""

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        self.logger = logging.getLogger(f"{__name__}.RLExtensionCoordinator")

        # RLé…ç½®
        self.rl_enabled = self.config.get('rl_enabled', False)
        self.hybrid_mode = self.config.get('hybrid_mode', True)  # RL + å‚³çµ±å„ªåŒ–æ··åˆ
        self.rl_confidence_threshold = self.config.get('rl_confidence_threshold', 0.7)

        # çµ„ä»¶åˆå§‹åŒ–
        self.environment_adapter = RLEnvironmentAdapter(config.get('rl_environment', {}))
        self.rl_agent: Optional[RLAgentInterface] = None

        # å”èª¿çµ±è¨ˆ
        self.coordination_stats = {
            'rl_decisions_used': 0,
            'traditional_decisions_used': 0,
            'hybrid_decisions': 0,
            'total_decisions': 0
        }

        self.logger.info(f"âœ… RLæ“´å±•å”èª¿å™¨åˆå§‹åŒ–å®Œæˆ (RLå•Ÿç”¨: {self.rl_enabled})")

    def register_rl_agent(self, agent: RLAgentInterface):
        """è¨»å†ŠRLä»£ç†"""
        self.rl_agent = agent
        self.logger.info("ğŸ¤– RLä»£ç†å·²è¨»å†Š")

    def coordinate_decision_making(self, optimization_data: Dict[str, Any],
                                 traditional_results: Dict[str, Any]) -> Dict[str, Any]:
        """å”èª¿RLèˆ‡å‚³çµ±å„ªåŒ–çš„æ±ºç­–åˆ¶å®š"""
        try:
            if not self.rl_enabled or self.rl_agent is None:
                # åƒ…ä½¿ç”¨å‚³çµ±å„ªåŒ–
                self.coordination_stats['traditional_decisions_used'] += 1
                return self._wrap_traditional_decision(traditional_results)

            # è½‰æ›ç‚ºRLç‹€æ…‹
            rl_state = self.environment_adapter.convert_to_rl_state(optimization_data)

            # ç²å–RLæ±ºç­–
            rl_action = self.rl_agent.select_action(rl_state)
            rl_decision = self.environment_adapter.convert_from_rl_action(rl_action)

            if self.hybrid_mode:
                # æ··åˆæ¨¡å¼ï¼šçµåˆRLå’Œå‚³çµ±å„ªåŒ–
                coordinated_decision = self._coordinate_hybrid_decision(
                    rl_decision, traditional_results, rl_action.confidence
                )
                self.coordination_stats['hybrid_decisions'] += 1
            else:
                # ç´”RLæ¨¡å¼
                if rl_action.confidence >= self.rl_confidence_threshold:
                    coordinated_decision = self._wrap_rl_decision(rl_decision, rl_action)
                    self.coordination_stats['rl_decisions_used'] += 1
                else:
                    # RLä¿¡å¿ƒä¸è¶³ï¼Œå›é€€åˆ°å‚³çµ±å„ªåŒ–
                    coordinated_decision = self._wrap_traditional_decision(traditional_results)
                    self.coordination_stats['traditional_decisions_used'] += 1

            self.coordination_stats['total_decisions'] += 1

            return coordinated_decision

        except Exception as e:
            self.logger.error(f"æ±ºç­–å”èª¿å¤±æ•—: {e}")
            # éŒ¯èª¤æ™‚å›é€€åˆ°å‚³çµ±å„ªåŒ–
            self.coordination_stats['traditional_decisions_used'] += 1
            return self._wrap_traditional_decision(traditional_results)

    def _coordinate_hybrid_decision(self, rl_decision: Dict[str, Any],
                                  traditional_results: Dict[str, Any],
                                  rl_confidence: float) -> Dict[str, Any]:
        """å”èª¿æ··åˆæ±ºç­–"""
        # åŸºæ–¼ä¿¡å¿ƒåº¦åŠ æ¬Šçµåˆæ±ºç­–
        weight_rl = min(rl_confidence, 0.8)  # RLæ¬Šé‡ä¸Šé™80%
        weight_traditional = 1.0 - weight_rl

        hybrid_decision = {
            'decision_source': 'hybrid_rl_traditional',
            'rl_weight': weight_rl,
            'traditional_weight': weight_traditional,
            'rl_decision': rl_decision,
            'traditional_decision': traditional_results,
            'final_selection': self._merge_decisions(rl_decision, traditional_results, weight_rl),
            'coordination_metadata': {
                'rl_confidence': rl_confidence,
                'hybrid_strategy': 'confidence_weighted',
                'timestamp': datetime.now(timezone.utc).isoformat()
            }
        }

        return hybrid_decision

    def _merge_decisions(self, rl_decision: Dict[str, Any],
                        traditional_decision: Dict[str, Any],
                        rl_weight: float) -> Dict[str, Any]:
        """åˆä½µRLå’Œå‚³çµ±æ±ºç­–"""
        # ç°¡åŒ–çš„æ±ºç­–åˆä½µé‚è¼¯
        merged = {
            'selected_satellites': self._merge_satellite_selections(
                rl_decision.get('selected_satellites', []),
                traditional_decision.get('optimal_pool', {}).get('selected_satellites', []),
                rl_weight
            ),
            'optimization_confidence': (
                rl_weight * rl_decision.get('confidence_score', 0.5) +
                (1 - rl_weight) * traditional_decision.get('metadata', {}).get('confidence', 0.5)
            ),
            'merge_strategy': 'weighted_combination'
        }

        return merged

    def _merge_satellite_selections(self, rl_satellites: List[str],
                                   traditional_satellites: List[Dict[str, Any]],
                                   rl_weight: float) -> List[str]:
        """åˆä½µè¡›æ˜Ÿé¸æ“‡"""
        # æå–å‚³çµ±é¸æ“‡çš„è¡›æ˜ŸID
        trad_satellite_ids = [sat.get('satellite_id', '') for sat in traditional_satellites if 'satellite_id' in sat]

        # ç°¡å–®ç­–ç•¥ï¼šå„ªå…ˆé¸æ“‡RLæ¨è–¦ï¼Œè£œå……å‚³çµ±æ¨è–¦
        merged_selection = []

        # æ·»åŠ RLæ¨è–¦ (æŒ‰æ¬Šé‡æ¯”ä¾‹)
        rl_count = int(len(rl_satellites) * rl_weight)
        merged_selection.extend(rl_satellites[:rl_count])

        # æ·»åŠ å‚³çµ±æ¨è–¦ (é¿å…é‡è¤‡)
        trad_count = max(1, len(trad_satellite_ids) - rl_count)
        for sat_id in trad_satellite_ids:
            if sat_id not in merged_selection and len(merged_selection) < rl_count + trad_count:
                merged_selection.append(sat_id)

        return merged_selection

    def _wrap_rl_decision(self, rl_decision: Dict[str, Any], rl_action: RLAction) -> Dict[str, Any]:
        """åŒ…è£RLæ±ºç­–"""
        return {
            'decision_source': 'rl_agent',
            'rl_action': rl_action,
            'decision': rl_decision,
            'metadata': {
                'rl_confidence': rl_action.confidence,
                'action_type': rl_action.action_type,
                'timestamp': datetime.now(timezone.utc).isoformat()
            }
        }

    def _wrap_traditional_decision(self, traditional_results: Dict[str, Any]) -> Dict[str, Any]:
        """åŒ…è£å‚³çµ±å„ªåŒ–æ±ºç­–"""
        return {
            'decision_source': 'traditional_optimization',
            'decision': traditional_results,
            'metadata': {
                'fallback_reason': 'rl_disabled_or_low_confidence',
                'timestamp': datetime.now(timezone.utc).isoformat()
            }
        }

    def update_rl_agent(self, state: RLState, action: RLAction, reward: RLReward, next_state: RLState):
        """æ›´æ–°RLä»£ç†"""
        if self.rl_agent and self.rl_enabled:
            try:
                self.rl_agent.update_policy(state, action, reward, next_state)
                self.logger.debug("ğŸ§  RLä»£ç†ç­–ç•¥å·²æ›´æ–°")
            except Exception as e:
                self.logger.error(f"RLä»£ç†æ›´æ–°å¤±æ•—: {e}")

    def get_coordination_statistics(self) -> Dict[str, Any]:
        """ç²å–å”èª¿çµ±è¨ˆä¿¡æ¯"""
        stats = self.coordination_stats.copy()
        if stats['total_decisions'] > 0:
            stats['rl_usage_rate'] = stats['rl_decisions_used'] / stats['total_decisions']
            stats['hybrid_usage_rate'] = stats['hybrid_decisions'] / stats['total_decisions']
            stats['traditional_usage_rate'] = stats['traditional_decisions_used'] / stats['total_decisions']

        return stats

    def save_rl_model(self, path: str) -> bool:
        """ä¿å­˜RLæ¨¡å‹"""
        if self.rl_agent:
            return self.rl_agent.save_model(path)
        return False

    def load_rl_model(self, path: str) -> bool:
        """è¼‰å…¥RLæ¨¡å‹"""
        if self.rl_agent:
            return self.rl_agent.load_model(path)
        return False


# ç¤ºä¾‹RLä»£ç†å¯¦ç¾ (å¯ä»¥è¢«æ›¿æ›ç‚ºæ›´å¾©é›œçš„å¯¦ç¾)
class DummyRLAgent(RLAgentInterface):
    """ç¤ºä¾‹RLä»£ç† - ç”¨æ–¼æ¸¬è©¦å’Œé–‹ç™¼"""

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        self.logger = logging.getLogger(f"{__name__}.DummyRLAgent")
        self.policy_version = "dummy_v1.0"

        self.logger.info("ğŸ¤– ç¤ºä¾‹RLä»£ç†åˆå§‹åŒ–å®Œæˆ")

    def select_action(self, state: RLState) -> RLAction:
        """ç°¡å–®çš„å‹•ä½œé¸æ“‡ç­–ç•¥ (å¯¦éš›æ‡‰ç”¨ä¸­æ‡‰æ›¿æ›ç‚ºç¥ç¶“ç¶²çµ¡)"""
        # åŸºæ–¼ä¿¡è™Ÿå¼·åº¦é¸æ“‡æœ€ä½³è¡›æ˜Ÿ
        if state.signal_strengths:
            max_signal_idx = np.argmax(state.signal_strengths)
            selected_satellites = [f"satellite_{max_signal_idx}"]
            confidence = min(0.9, max(0.3, (max(state.signal_strengths) + 120) / 40))
        else:
            selected_satellites = []
            confidence = 0.1

        action = RLAction(
            action_type="satellite_selection",
            target_satellites=selected_satellites,
            parameters={
                "selection_strategy": "max_signal_strength",
                "policy_version": self.policy_version
            },
            confidence=confidence
        )

        return action

    def update_policy(self, state: RLState, action: RLAction, reward: RLReward, next_state: RLState):
        """ç¤ºä¾‹ç­–ç•¥æ›´æ–° (å¯¦éš›æ‡‰ç”¨ä¸­æ‡‰é€²è¡Œç¥ç¶“ç¶²çµ¡è¨“ç·´)"""
        # åœ¨çœŸå¯¦å¯¦ç¾ä¸­ï¼Œé€™è£¡æœƒæ›´æ–°ç¥ç¶“ç¶²çµ¡æ¬Šé‡
        self.logger.debug(f"ç­–ç•¥æ›´æ–°: çå‹µ={reward.total_reward:.3f}")

    def save_model(self, path: str) -> bool:
        """ä¿å­˜æ¨¡å‹ (ç¤ºä¾‹å¯¦ç¾)"""
        self.logger.info(f"ç¤ºä¾‹æ¨¡å‹ä¿å­˜åˆ°: {path}")
        return True

    def load_model(self, path: str) -> bool:
        """è¼‰å…¥æ¨¡å‹ (ç¤ºä¾‹å¯¦ç¾)"""
        self.logger.info(f"ç¤ºä¾‹æ¨¡å‹å¾ä»¥ä¸‹è·¯å¾‘è¼‰å…¥: {path}")
        return True