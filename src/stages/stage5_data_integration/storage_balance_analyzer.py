"""
å­˜å„²å¹³è¡¡åˆ†æå™¨ - Stage 5æ¨¡çµ„åŒ–çµ„ä»¶

è·è²¬ï¼š
1. åˆ†æå’Œå„ªåŒ–æ··åˆå­˜å„²ç­–ç•¥
2. è¨ˆç®—PostgreSQLå’ŒVolumeå­˜å„²éœ€æ±‚
3. è©•ä¼°å­˜å„²å¹³è¡¡æœ€å„ªæ€§
4. æä¾›å­˜å„²å„ªåŒ–å»ºè­°
"""

import os
import json
import logging
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime, timezone

logger = logging.getLogger(__name__)

class StorageBalanceAnalyzer:
    """å­˜å„²å¹³è¡¡åˆ†æå™¨ - åˆ†æå’Œå„ªåŒ–æ··åˆå­˜å„²æ¶æ§‹å¹³è¡¡"""
    
    def __init__(self):
        """åˆå§‹åŒ–å­˜å„²å¹³è¡¡åˆ†æå™¨"""
        self.logger = logging.getLogger(f"{__name__}.StorageBalanceAnalyzer")
        
        # åˆ†æçµ±è¨ˆ
        self.analysis_statistics = {
            "balance_analyses_performed": 0,
            "storage_calculations_completed": 0,
            "optimization_recommendations_generated": 0,
            "performance_assessments": 0
        }
        
        # å­˜å„²é¡å‹ç‰¹æ€§é…ç½®
        self.storage_characteristics = {
            "postgresql": {
                "optimal_data_types": ["structured_metadata", "indexed_queries", "relational_data"],
                "performance_profile": {
                    "query_speed": "high",
                    "storage_efficiency": "medium",
                    "scalability": "high",
                    "maintenance_overhead": "medium"
                },
                "cost_factors": {
                    "storage_cost_per_gb": 0.10,  # USD
                    "query_cost_per_1000": 0.005,
                    "maintenance_cost_factor": 1.2
                }
            },
            "volume_storage": {
                "optimal_data_types": ["large_json_files", "timeseries_data", "bulk_data"],
                "performance_profile": {
                    "query_speed": "medium",
                    "storage_efficiency": "high", 
                    "scalability": "very_high",
                    "maintenance_overhead": "low"
                },
                "cost_factors": {
                    "storage_cost_per_gb": 0.023,  # USD
                    "access_cost_per_1000": 0.0004,
                    "maintenance_cost_factor": 0.3
                }
            }
        }
        
        self.logger.info("âœ… å­˜å„²å¹³è¡¡åˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")
        self.logger.info("   æ”¯æŒPostgreSQLå’ŒVolumeæ··åˆå­˜å„²åˆ†æ")
    
    def analyze_storage_balance(self, 
                              integrated_satellites: List[Dict[str, Any]],
                              postgresql_data: Dict[str, Any],
                              volume_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        åˆ†æå­˜å„²å¹³è¡¡
        
        Args:
            integrated_satellites: æ•´åˆçš„è¡›æ˜Ÿæ•¸æ“š
            postgresql_data: PostgreSQLå­˜å„²æ•¸æ“š
            volume_data: Volumeå­˜å„²æ•¸æ“š
            
        Returns:
            å­˜å„²å¹³è¡¡åˆ†æçµæœ
        """
        self.logger.info(f"âš–ï¸ é–‹å§‹å­˜å„²å¹³è¡¡åˆ†æ ({len(integrated_satellites)} è¡›æ˜Ÿ)...")
        
        balance_analysis = {
            "analysis_timestamp": datetime.now(timezone.utc).isoformat(),
            "total_satellites": len(integrated_satellites),
            "storage_requirements": {},
            "performance_analysis": {},
            "balance_assessment": {},
            "optimization_recommendations": [],
            "cost_analysis": {}
        }
        
        # 1. è¨ˆç®—å­˜å„²éœ€æ±‚
        storage_requirements = self._calculate_storage_requirements(
            integrated_satellites, postgresql_data, volume_data
        )
        balance_analysis["storage_requirements"] = storage_requirements
        
        # 2. åˆ†ææŸ¥è©¢æ€§èƒ½
        performance_analysis = self._analyze_query_performance(
            integrated_satellites, postgresql_data, volume_data
        )
        balance_analysis["performance_analysis"] = performance_analysis
        
        # 3. è©•ä¼°å­˜å„²å¹³è¡¡æœ€å„ªæ€§
        balance_assessment = self._assess_storage_optimality(
            storage_requirements, performance_analysis
        )
        balance_analysis["balance_assessment"] = balance_assessment
        
        # 4. ç”Ÿæˆå„ªåŒ–å»ºè­°
        optimization_recommendations = self._generate_optimization_recommendations(
            balance_assessment, storage_requirements
        )
        balance_analysis["optimization_recommendations"] = optimization_recommendations
        
        # 5. è¨ˆç®—æˆæœ¬åˆ†æ
        cost_analysis = self._calculate_cost_analysis(
            storage_requirements, performance_analysis
        )
        balance_analysis["cost_analysis"] = cost_analysis
        
        # æ›´æ–°çµ±è¨ˆ
        self.analysis_statistics["balance_analyses_performed"] += 1
        self.analysis_statistics["storage_calculations_completed"] += 1
        self.analysis_statistics["optimization_recommendations_generated"] += len(optimization_recommendations)
        self.analysis_statistics["performance_assessments"] += 1
        
        self.logger.info(f"âœ… å­˜å„²å¹³è¡¡åˆ†æå®Œæˆ (å¹³è¡¡åˆ†æ•¸: {balance_assessment.get('overall_balance_score', 'N/A')})")
        
        return balance_analysis
    
    def _calculate_storage_requirements(self, 
                                      integrated_satellites: List[Dict[str, Any]],
                                      postgresql_data: Dict[str, Any],
                                      volume_data: Dict[str, Any]) -> Dict[str, Any]:
        """è¨ˆç®—å­˜å„²éœ€æ±‚"""
        self.logger.info("ğŸ“Š è¨ˆç®—å­˜å„²éœ€æ±‚...")
        
        requirements = {
            "postgresql_requirements": self._calculate_postgresql_requirements(integrated_satellites, postgresql_data),
            "volume_requirements": self._calculate_volume_requirements(integrated_satellites, volume_data),
            "total_requirements": {},
            "storage_distribution": {}
        }
        
        # è¨ˆç®—ç¸½éœ€æ±‚
        pg_size = requirements["postgresql_requirements"]["estimated_size_gb"]
        volume_size = requirements["volume_requirements"]["estimated_size_gb"]
        total_size = pg_size + volume_size
        
        requirements["total_requirements"] = {
            "total_storage_gb": total_size,
            "postgresql_percentage": (pg_size / total_size * 100) if total_size > 0 else 0,
            "volume_percentage": (volume_size / total_size * 100) if total_size > 0 else 0
        }
        
        # å­˜å„²åˆ†å¸ƒåˆ†æ
        requirements["storage_distribution"] = {
            "structured_data_gb": pg_size,
            "unstructured_data_gb": volume_size,
            "distribution_balance": "optimal" if 20 <= (pg_size / total_size * 100) <= 40 else "suboptimal" if total_size > 0 else "unknown"
        }
        
        return requirements
    
    def _calculate_postgresql_requirements(self, 
                                         integrated_satellites: List[Dict[str, Any]], 
                                         postgresql_data: Dict[str, Any]) -> Dict[str, Any]:
        """è¨ˆç®—PostgreSQLå­˜å„²éœ€æ±‚"""
        satellites_count = len(integrated_satellites)
        
        # ä¼°ç®—è¡¨å¤§å° (bytes per row)
        table_estimates = {
            "satellite_metadata": 500,      # æ¯è¡Œ500å­—ç¯€
            "signal_statistics": 200,       # æ¯è¡Œ200å­—ç¯€  
            "handover_events": 300,         # æ¯è¡Œ300å­—ç¯€
            "processing_summary": 1000      # æ¯è¡Œ1000å­—ç¯€
        }
        
        # è¨ˆç®—æ•¸æ“šé‡
        estimated_rows = {
            "satellite_metadata": satellites_count,
            "signal_statistics": satellites_count,
            "handover_events": satellites_count * 5,  # æ¯è¡›æ˜Ÿ5å€‹äº‹ä»¶
            "processing_summary": max(1, satellites_count // 1000)  # æ¯1000è¡›æ˜Ÿ1å€‹æ‘˜è¦
        }
        
        total_bytes = 0
        table_sizes = {}
        
        for table, bytes_per_row in table_estimates.items():
            rows = estimated_rows.get(table, 0)
            size_bytes = rows * bytes_per_row
            size_mb = size_bytes / (1024 * 1024)
            
            table_sizes[table] = {
                "estimated_rows": rows,
                "size_bytes": size_bytes,
                "size_mb": size_mb
            }
            
            total_bytes += size_bytes
        
        # æ·»åŠ ç´¢å¼•é–‹éŠ· (ç´„30%)
        index_overhead = total_bytes * 0.3
        total_with_indexes = total_bytes + index_overhead
        
        return {
            "estimated_size_bytes": int(total_with_indexes),
            "estimated_size_gb": total_with_indexes / (1024 ** 3),
            "table_breakdown": table_sizes,
            "index_overhead_bytes": int(index_overhead),
            "optimization_potential": {
                "compression_savings": "15-25%",
                "partition_benefits": "high" if satellites_count > 10000 else "medium"
            }
        }
    
    def _calculate_volume_requirements(self, 
                                     integrated_satellites: List[Dict[str, Any]], 
                                     volume_data: Dict[str, Any]) -> Dict[str, Any]:
        """è¨ˆç®—Volumeå­˜å„²éœ€æ±‚"""
        satellites_count = len(integrated_satellites)
        
        # ä¼°ç®—æ¯è¡›æ˜Ÿçš„JSONæ•¸æ“šå¤§å°
        data_size_estimates = {
            "stage1_orbital": 2048,         # 2KB per satellite
            "stage2_visibility": 5120,      # 5KB per satellite  
            "stage3_timeseries": 20480,     # 20KB per satellite
            "stage4_signal_analysis": 8192, # 8KB per satellite
            "layered_data": 4096,           # 4KB per satellite
            "handover_scenarios": 6144      # 6KB per satellite
        }
        
        total_bytes = 0
        file_breakdown = {}
        
        for data_type, bytes_per_satellite in data_size_estimates.items():
            total_size = satellites_count * bytes_per_satellite
            size_mb = total_size / (1024 * 1024)
            
            file_breakdown[data_type] = {
                "satellites": satellites_count,
                "size_bytes": total_size,
                "size_mb": size_mb
            }
            
            total_bytes += total_size
        
        return {
            "estimated_size_bytes": total_bytes,
            "estimated_size_gb": total_bytes / (1024 ** 3),
            "file_breakdown": file_breakdown,
            "compression_potential": {
                "json_compression_ratio": "3:1",
                "gzip_savings": "60-70%",
                "estimated_compressed_gb": (total_bytes / (1024 ** 3)) * 0.35
            }
        }
    
    def _analyze_query_performance(self, 
                                 integrated_satellites: List[Dict[str, Any]],
                                 postgresql_data: Dict[str, Any],
                                 volume_data: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†ææŸ¥è©¢æ€§èƒ½"""
        self.logger.info("âš¡ åˆ†ææŸ¥è©¢æ€§èƒ½...")
        
        satellites_count = len(integrated_satellites)
        
        performance_analysis = {
            "postgresql_performance": self._assess_postgresql_performance(satellites_count),
            "volume_performance": self._assess_volume_performance(satellites_count),
            "comparative_analysis": {},
            "bottleneck_identification": []
        }
        
        # æ¯”è¼ƒåˆ†æ
        pg_perf = performance_analysis["postgresql_performance"]
        vol_perf = performance_analysis["volume_performance"]
        
        performance_analysis["comparative_analysis"] = {
            "structured_queries": {
                "postgresql_advantage": pg_perf["structured_query_ms"] < vol_perf["structured_query_ms"],
                "performance_ratio": vol_perf["structured_query_ms"] / pg_perf["structured_query_ms"]
            },
            "bulk_data_access": {
                "volume_advantage": vol_perf["bulk_access_ms"] < pg_perf["bulk_access_ms"],
                "performance_ratio": pg_perf["bulk_access_ms"] / vol_perf["bulk_access_ms"]
            },
            "scalability": {
                "postgresql_scalability_score": pg_perf["scalability_score"],
                "volume_scalability_score": vol_perf["scalability_score"],
                "better_scalability": "volume" if vol_perf["scalability_score"] > pg_perf["scalability_score"] else "postgresql"
            }
        }
        
        # ç“¶é ¸è­˜åˆ¥
        bottlenecks = []
        if pg_perf["structured_query_ms"] > 1000:
            bottlenecks.append("PostgreSQLç´¢å¼•å„ªåŒ–éœ€æ±‚")
        
        if vol_perf["bulk_access_ms"] > 5000:
            bottlenecks.append("Volumeå­˜å„²I/Oå„ªåŒ–éœ€æ±‚")
        
        if satellites_count > 50000:
            bottlenecks.append("å¤§è¦æ¨¡æ•¸æ“šåˆ†å€ç­–ç•¥")
        
        performance_analysis["bottleneck_identification"] = bottlenecks
        
        return performance_analysis
    
    def _assess_postgresql_performance(self, satellites_count: int) -> Dict[str, Any]:
        """è©•ä¼°PostgreSQLæ€§èƒ½"""
        # åŸºæ–¼æ•¸æ“šé‡çš„æ€§èƒ½æ¨¡å‹
        base_query_time = 50  # ms
        scaling_factor = max(1, satellites_count / 10000)
        
        return {
            "structured_query_ms": base_query_time * scaling_factor,
            "index_query_ms": 25 * scaling_factor,
            "bulk_access_ms": 200 * scaling_factor,
            "concurrent_connections": min(100, satellites_count // 100),
            "scalability_score": max(50, 100 - (scaling_factor - 1) * 20),
            "memory_efficiency": "high" if satellites_count < 100000 else "medium",
            "query_optimization_potential": "high"
        }
    
    def _assess_volume_performance(self, satellites_count: int) -> Dict[str, Any]:
        """è©•ä¼°Volumeå­˜å„²æ€§èƒ½"""
        base_access_time = 100  # ms
        scaling_factor = max(1, satellites_count / 100000)  # Volumeæ“´å±•æ€§æ›´å¥½
        
        return {
            "structured_query_ms": 500 * scaling_factor,  # çµæ§‹åŒ–æŸ¥è©¢è¼ƒæ…¢
            "bulk_access_ms": base_access_time * scaling_factor,
            "file_streaming_ms": 50 * scaling_factor,
            "concurrent_access": min(1000, satellites_count // 10),
            "scalability_score": min(100, 80 + (100000 / max(satellites_count, 1000)) * 0.2),
            "storage_efficiency": "very_high",
            "compression_benefit": "high"
        }
    
    def _assess_storage_optimality(self, 
                                 storage_requirements: Dict[str, Any], 
                                 performance_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """è©•ä¼°å­˜å„²å¹³è¡¡æœ€å„ªæ€§"""
        self.logger.info("ğŸ¯ è©•ä¼°å­˜å„²æœ€å„ªæ€§...")
        
        total_req = storage_requirements["total_requirements"]
        pg_percentage = total_req["postgresql_percentage"]
        volume_percentage = total_req["volume_percentage"]
        
        # å¹³è¡¡åˆ†æ•¸è¨ˆç®— (0-100)
        balance_factors = []
        
        # 1. å­˜å„²åˆ†å¸ƒå¹³è¡¡ (30% æ¬Šé‡)
        if 20 <= pg_percentage <= 40:
            distribution_score = 100
        elif 15 <= pg_percentage <= 45:
            distribution_score = 80
        elif 10 <= pg_percentage <= 50:
            distribution_score = 60
        else:
            distribution_score = 40
        
        balance_factors.append(("storage_distribution", distribution_score, 0.3))
        
        # 2. æ€§èƒ½åŒ¹é… (25% æ¬Šé‡)
        comp_analysis = performance_analysis["comparative_analysis"]
        performance_score = 0
        
        if comp_analysis["structured_queries"]["postgresql_advantage"]:
            performance_score += 50
        
        if comp_analysis["bulk_data_access"]["volume_advantage"]:
            performance_score += 50
        
        balance_factors.append(("performance_matching", performance_score, 0.25))
        
        # 3. å¯æ“´å±•æ€§ (20% æ¬Šé‡)
        pg_scalability = performance_analysis["postgresql_performance"]["scalability_score"]
        vol_scalability = performance_analysis["volume_performance"]["scalability_score"]
        avg_scalability = (pg_scalability + vol_scalability) / 2
        
        balance_factors.append(("scalability", avg_scalability, 0.20))
        
        # 4. æˆæœ¬æ•ˆç›Š (15% æ¬Šé‡)
        cost_efficiency_score = self._estimate_cost_efficiency(storage_requirements)
        balance_factors.append(("cost_efficiency", cost_efficiency_score, 0.15))
        
        # 5. ç¶­è­·è¤‡é›œåº¦ (10% æ¬Šé‡)
        maintenance_score = self._assess_maintenance_complexity(storage_requirements)
        balance_factors.append(("maintenance_complexity", maintenance_score, 0.10))
        
        # è¨ˆç®—æ•´é«”å¹³è¡¡åˆ†æ•¸
        overall_score = sum(score * weight for _, score, weight in balance_factors)
        
        # åˆ†ç´šè©•ä¼°
        if overall_score >= 85:
            balance_grade = "Excellent"
            recommendation = "ç•¶å‰å­˜å„²é…ç½®å·²é”åˆ°æœ€å„ªå¹³è¡¡"
        elif overall_score >= 75:
            balance_grade = "Good"
            recommendation = "å­˜å„²é…ç½®è‰¯å¥½ï¼Œæœ‰å°‘é‡å„ªåŒ–ç©ºé–“"
        elif overall_score >= 65:
            balance_grade = "Fair"
            recommendation = "å­˜å„²é…ç½®å¯æ¥å—ï¼Œå»ºè­°é€²è¡Œå„ªåŒ–"
        else:
            balance_grade = "Poor"
            recommendation = "å­˜å„²é…ç½®éœ€è¦é‡å¤§èª¿æ•´"
        
        return {
            "overall_balance_score": round(overall_score, 2),
            "balance_grade": balance_grade,
            "recommendation": recommendation,
            "balance_factors": {name: score for name, score, _ in balance_factors},
            "optimization_priority": self._identify_optimization_priorities(balance_factors)
        }
    
    def _estimate_cost_efficiency(self, storage_requirements: Dict[str, Any]) -> float:
        """ä¼°ç®—æˆæœ¬æ•ˆç›Š"""
        pg_req = storage_requirements["postgresql_requirements"]
        vol_req = storage_requirements["volume_requirements"]
        
        # è¨ˆç®—æœˆåº¦æˆæœ¬
        pg_cost = (pg_req["estimated_size_gb"] * 
                  self.storage_characteristics["postgresql"]["cost_factors"]["storage_cost_per_gb"])
        
        vol_cost = (vol_req["estimated_size_gb"] * 
                   self.storage_characteristics["volume_storage"]["cost_factors"]["storage_cost_per_gb"])
        
        total_cost = pg_cost + vol_cost
        total_storage = pg_req["estimated_size_gb"] + vol_req["estimated_size_gb"]
        
        # æˆæœ¬æ•ˆç›Šåˆ†æ•¸ (åŸºæ–¼æ¯GBæˆæœ¬)
        if total_storage > 0:
            cost_per_gb = total_cost / total_storage
            
            # è©•åˆ†æ¨™æº– (USD per GB per month)
            if cost_per_gb <= 0.05:
                return 100
            elif cost_per_gb <= 0.08:
                return 80
            elif cost_per_gb <= 0.12:
                return 60
            else:
                return 40
        
        return 70  # é è¨­åˆ†æ•¸
    
    def _assess_maintenance_complexity(self, storage_requirements: Dict[str, Any]) -> float:
        """è©•ä¼°ç¶­è­·è¤‡é›œåº¦"""
        # ç°¡åŒ–çš„ç¶­è­·è¤‡é›œåº¦è©•ä¼°
        pg_tables = len(storage_requirements["postgresql_requirements"]["table_breakdown"])
        total_storage = storage_requirements["total_requirements"]["total_storage_gb"]
        
        # åŸºæ–¼è¡¨æ•¸é‡å’Œå­˜å„²å¤§å°çš„è¤‡é›œåº¦
        complexity_factors = []
        
        # è¡¨æ•¸é‡å› å­
        if pg_tables <= 5:
            table_score = 100
        elif pg_tables <= 10:
            table_score = 80
        else:
            table_score = 60
        
        complexity_factors.append(table_score * 0.4)
        
        # å­˜å„²å¤§å°å› å­
        if total_storage <= 10:
            size_score = 100
        elif total_storage <= 100:
            size_score = 85
        elif total_storage <= 1000:
            size_score = 70
        else:
            size_score = 55
        
        complexity_factors.append(size_score * 0.6)
        
        return sum(complexity_factors)
    
    def _identify_optimization_priorities(self, balance_factors: List[Tuple[str, float, float]]) -> List[str]:
        """è­˜åˆ¥å„ªåŒ–å„ªå…ˆç´š"""
        priorities = []
        
        for name, score, weight in balance_factors:
            if score < 70:  # éœ€è¦æ”¹å–„çš„å› å­
                importance = weight * (70 - score)  # æ¬Šé‡ Ã— æ”¹å–„ç©ºé–“
                priorities.append((name, importance))
        
        # æŒ‰é‡è¦æ€§æ’åº
        priorities.sort(key=lambda x: x[1], reverse=True)
        
        return [name for name, _ in priorities[:3]]  # è¿”å›å‰3å€‹å„ªå…ˆç´š
    
    def _generate_optimization_recommendations(self, 
                                             balance_assessment: Dict[str, Any], 
                                             storage_requirements: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ç”Ÿæˆå„ªåŒ–å»ºè­°"""
        self.logger.info("ğŸ’¡ ç”Ÿæˆå­˜å„²å„ªåŒ–å»ºè­°...")
        
        recommendations = []
        balance_score = balance_assessment["overall_balance_score"]
        priorities = balance_assessment["optimization_priority"]
        
        # åŸºæ–¼å„ªå…ˆç´šç”Ÿæˆå…·é«”å»ºè­°
        for priority in priorities:
            if priority == "storage_distribution":
                recommendations.append({
                    "category": "å­˜å„²åˆ†å¸ƒå„ªåŒ–",
                    "priority": "high",
                    "recommendation": "èª¿æ•´PostgreSQLå’ŒVolumeå­˜å„²çš„æ•¸æ“šåˆ†å¸ƒæ¯”ä¾‹",
                    "specific_actions": [
                        "å°‡æ™‚é–“åºåˆ—æ•¸æ“šé·ç§»è‡³Volumeå­˜å„²",
                        "ä¿ç•™ç´¢å¼•æŸ¥è©¢æ•¸æ“šåœ¨PostgreSQL",
                        "å¯¦æ–½æ™ºèƒ½æ•¸æ“šåˆ†å±¤ç­–ç•¥"
                    ],
                    "expected_improvement": "15-25%"
                })
            
            elif priority == "performance_matching":
                recommendations.append({
                    "category": "æ€§èƒ½åŒ¹é…å„ªåŒ–", 
                    "priority": "medium",
                    "recommendation": "å„ªåŒ–æŸ¥è©¢è·¯ç”±ä»¥åŒ¹é…å­˜å„²ç‰¹æ€§",
                    "specific_actions": [
                        "çµæ§‹åŒ–æŸ¥è©¢å°å‘PostgreSQL",
                        "æ‰¹é‡æ•¸æ“šè¨ªå•å°å‘Volumeå­˜å„²",
                        "å¯¦æ–½æ™ºèƒ½æŸ¥è©¢è·¯ç”±å™¨"
                    ],
                    "expected_improvement": "20-30%"
                })
            
            elif priority == "scalability":
                recommendations.append({
                    "category": "å¯æ“´å±•æ€§å„ªåŒ–",
                    "priority": "medium",
                    "recommendation": "å¯¦æ–½åˆ†å€å’Œåˆ†ç‰‡ç­–ç•¥",
                    "specific_actions": [
                        "PostgreSQLè¡¨åˆ†å€(æŒ‰æ˜Ÿåº§æˆ–æ™‚é–“)",
                        "Volumeå­˜å„²éšå±¤åŒ–çµ„ç¹”",
                        "å¯¦æ–½è‡ªå‹•æ“´å±•æ©Ÿåˆ¶"
                    ],
                    "expected_improvement": "40-60%"
                })
        
        # é€šç”¨å„ªåŒ–å»ºè­°
        if balance_score < 75:
            recommendations.append({
                "category": "é€šç”¨å„ªåŒ–",
                "priority": "low",
                "recommendation": "å¯¦æ–½å­˜å„²ç›£æ§å’Œè‡ªå‹•åŒ–ç®¡ç†",
                "specific_actions": [
                    "éƒ¨ç½²å­˜å„²ä½¿ç”¨ç›£æ§",
                    "è¨­ç½®è‡ªå‹•å‚™ä»½ç­–ç•¥",
                    "å¯¦æ–½æˆæœ¬å„ªåŒ–è­¦å ±"
                ],
                "expected_improvement": "10-20%"
            })
        
        return recommendations
    
    def _calculate_cost_analysis(self, 
                               storage_requirements: Dict[str, Any], 
                               performance_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """è¨ˆç®—æˆæœ¬åˆ†æ"""
        self.logger.info("ğŸ’° è¨ˆç®—æˆæœ¬åˆ†æ...")
        
        pg_req = storage_requirements["postgresql_requirements"]
        vol_req = storage_requirements["volume_requirements"]
        
        # PostgreSQLæˆæœ¬
        pg_storage_cost = (pg_req["estimated_size_gb"] * 
                          self.storage_characteristics["postgresql"]["cost_factors"]["storage_cost_per_gb"])
        
        pg_query_cost = (1000 * 
                        self.storage_characteristics["postgresql"]["cost_factors"]["query_cost_per_1000"])
        
        pg_maintenance_cost = (pg_storage_cost * 
                              self.storage_characteristics["postgresql"]["cost_factors"]["maintenance_cost_factor"])
        
        pg_total_cost = pg_storage_cost + pg_query_cost + pg_maintenance_cost
        
        # Volumeå­˜å„²æˆæœ¬
        vol_storage_cost = (vol_req["estimated_size_gb"] * 
                           self.storage_characteristics["volume_storage"]["cost_factors"]["storage_cost_per_gb"])
        
        vol_access_cost = (1000 * 
                          self.storage_characteristics["volume_storage"]["cost_factors"]["access_cost_per_1000"])
        
        vol_maintenance_cost = (vol_storage_cost * 
                               self.storage_characteristics["volume_storage"]["cost_factors"]["maintenance_cost_factor"])
        
        vol_total_cost = vol_storage_cost + vol_access_cost + vol_maintenance_cost
        
        total_monthly_cost = pg_total_cost + vol_total_cost
        
        return {
            "monthly_costs": {
                "postgresql": {
                    "storage_cost": round(pg_storage_cost, 2),
                    "query_cost": round(pg_query_cost, 2), 
                    "maintenance_cost": round(pg_maintenance_cost, 2),
                    "total_cost": round(pg_total_cost, 2)
                },
                "volume_storage": {
                    "storage_cost": round(vol_storage_cost, 2),
                    "access_cost": round(vol_access_cost, 2),
                    "maintenance_cost": round(vol_maintenance_cost, 2),
                    "total_cost": round(vol_total_cost, 2)
                },
                "total_monthly_cost": round(total_monthly_cost, 2)
            },
            "cost_efficiency": {
                "cost_per_satellite": round(total_monthly_cost / max(storage_requirements.get("total_satellites", 1), 1), 4),
                "cost_per_gb": round(total_monthly_cost / max(storage_requirements["total_requirements"]["total_storage_gb"], 1), 2),
                "postgresql_percentage": round((pg_total_cost / total_monthly_cost * 100), 1) if total_monthly_cost > 0 else 0,
                "volume_percentage": round((vol_total_cost / total_monthly_cost * 100), 1) if total_monthly_cost > 0 else 0
            },
            "cost_optimization_potential": {
                "compression_savings": round(vol_storage_cost * 0.65, 2),
                "query_optimization_savings": round(pg_query_cost * 0.3, 2),
                "total_potential_savings": round((vol_storage_cost * 0.65) + (pg_query_cost * 0.3), 2)
            }
        }
    
    def get_analysis_statistics(self) -> Dict[str, Any]:
        """ç²å–åˆ†æçµ±è¨ˆä¿¡æ¯"""
        return self.analysis_statistics.copy()