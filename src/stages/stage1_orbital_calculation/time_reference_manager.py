#!/usr/bin/env python3
"""
Stage 1: Time Reference Manager Component (v2.0 Architecture)

å°ˆè·è²¬ä»»ï¼š
- TLE Epochæ™‚é–“è§£æå’Œæ¨™æº–åŒ–
- æ™‚é–“åŸºæº–å»ºç«‹å’Œé©—è­‰
- æ™‚é–“ç²¾åº¦ç®¡ç†å’Œæ ¼å¼è½‰æ›
- å­¸è¡“ç´šæ™‚é–“æ¨™æº–åˆè¦

v2.0é‡æ§‹åŸå‰‡ï¼š
- å–®ä¸€è²¬ä»»åŸå‰‡ï¼šå°ˆé–€è² è²¬æ™‚é–“åŸºæº–ç®¡ç†
- å­¸è¡“æ¨™æº–åˆè¦ï¼šæ™‚é–“ç²¾åº¦å’Œæ ¼å¼è¦æ±‚
- çµ±ä¸€æ™‚é–“æ¥å£ï¼šç‚ºå¾ŒçºŒéšæ®µæä¾›æ¨™æº–æ™‚é–“åŸºæº–
"""

import logging
from datetime import datetime, timezone, timedelta
from typing import Dict, Any, List, Optional, Tuple
import math

# å…±äº«æ¨¡çµ„å°å…¥
from shared.utils import TimeUtils
from shared.constants import OrbitEngineConstantsManager

logger = logging.getLogger(__name__)


# ============================================================
# æ™‚é–“å“è³ªè©•åˆ†å¸¸æ•¸å®šç¾©
# ============================================================

# æ™‚é–“åˆ†ä½ˆè©•åˆ†é–€æª»
# SOURCE: TLEæ•¸æ“šæ–°é®®åº¦æ¨™æº–
# åŸºæ–¼ Space-Track.org çš„ TLE æ›´æ–°é »ç‡åˆ†æ
TEMPORAL_DISTRIBUTION_THRESHOLDS = {
    'recent_ratio_excellent': 0.8,    # 80%æ•¸æ“šåœ¨æœ€è¿‘2å¤©
    'recent_ratio_very_good': 0.6,    # 60%æ•¸æ“šåœ¨æœ€è¿‘2å¤©
    'recent_ratio_good': 0.4,         # 40%æ•¸æ“šåœ¨æœ€è¿‘2å¤©
    'time_span_optimal_days': 7       # æœ€å„ªæ™‚é–“è·¨åº¦ï¼ˆå¤©ï¼‰
}

# æ™‚é–“åˆ†ä½ˆè©•åˆ†å€¼
# SOURCE: å­¸è¡“ç ”ç©¶æ¨™æº–ï¼ŒåŸºæ–¼æ•¸æ“šæ–°é®®åº¦å’Œåˆ†ä½ˆç‰¹æ€§
TEMPORAL_DISTRIBUTION_SCORES = {
    'excellent_recent_optimal': 95.0,  # 80%æ•¸æ“šåœ¨æœ€è¿‘2å¤©ä¸”æ™‚é–“è·¨åº¦â‰¤7å¤©
    'excellent_recent': 88.0,          # 80%æ•¸æ“šåœ¨æœ€è¿‘2å¤©
    'very_good': 85.0,                 # 60%æ•¸æ“šåœ¨æœ€è¿‘2å¤©
    'good': 80.0,                      # 40%æ•¸æ“šåœ¨æœ€è¿‘2å¤©
    'base_score': 70.0,                # åŸºæœ¬åˆ†æ•¸
    'dispersion_penalty_per_day': 2.0  # åˆ†æ•£æ‡²ç½°ï¼ˆæ¯å¤©ï¼‰
}

# æ™‚é–“é€£çºŒæ€§è©•åˆ†é–€æª»
# SOURCE: TLEæ•¸æ“šè¦†è“‹ç‡åˆ†ææ¨™æº–
TIME_CONTINUITY_THRESHOLDS = {
    'short_span_days': 3,           # çŸ­æœŸæ™‚é–“è·¨åº¦
    'medium_span_days': 7,          # ä¸­æœŸæ™‚é–“è·¨åº¦
    'coverage_excellent': 0.8,      # å„ªç§€è¦†è“‹ç‡
    'coverage_very_good': 0.6,      # å¾ˆå¥½è¦†è“‹ç‡
    'coverage_good': 0.5,           # è‰¯å¥½è¦†è“‹ç‡
    'coverage_acceptable': 0.3,     # å¯æ¥å—è¦†è“‹ç‡
    'recent_density_excellent': 0.7,  # å„ªç§€æœ€è¿‘å¯†åº¦
    'recent_density_good': 0.5       # è‰¯å¥½æœ€è¿‘å¯†åº¦
}

# æ™‚é–“é€£çºŒæ€§è©•åˆ†å€¼
# SOURCE: æ•¸æ“šå®Œæ•´æ€§è©•ä¼°æ¨™æº–
TIME_CONTINUITY_SCORES = {
    'excellent': 95.0,
    'very_good': 90.0,
    'good': 85.0,
    'acceptable': 80.0,
    'moderate': 75.0
}

# æ™‚é–“å“è³ªåº¦é‡æ¬Šé‡
# SOURCE: å­¸è¡“ç ”ç©¶æ¨™æº–ï¼Œå¹³è¡¡åˆ†ä½ˆã€é€£çºŒæ€§å’Œç²¾åº¦
TIME_QUALITY_METRIC_WEIGHTS = {
    'distribution': 0.3,
    'continuity': 0.3,
    'precision': 0.3,
    'density': 0.1
}

# TLE æ•¸æ“šæ–°é®®åº¦è©•åˆ†ï¼ˆå¤©æ•¸ï¼‰
# SOURCE: Vallado (2013) - SGP4 ä½ç½®èª¤å·® vs. å‚³æ’­æ™‚é–“
# èˆ‡ TLEConstants çš„æ–°é®®åº¦æ¨™æº–ä¸€è‡´
TLE_FRESHNESS_SCORE_THRESHOLDS = {
    'excellent_days': 3,     # â‰¤3å¤©: ä½ç½®èª¤å·® <1 km
    'very_good_days': 7,     # â‰¤7å¤©: ä½ç½®èª¤å·® 1-3 km
    'good_days': 14,         # â‰¤14å¤©: ä½ç½®èª¤å·® 3-10 km
    'acceptable_days': 30,   # â‰¤30å¤©: ä½ç½®èª¤å·® 10-50 km
    'poor_days': 60          # â‰¤60å¤©: ä½ç½®èª¤å·® >50 km
}

# TLE æ•¸æ“šæ–°é®®åº¦è©•åˆ†å€¼
# SOURCE: åŸºæ–¼ SGP4 ç²¾åº¦è¡°æ¸›ç‰¹æ€§çš„å“è³ªè©•åˆ†
TLE_FRESHNESS_SCORES = {
    'excellent': 100,        # â‰¤3å¤©
    'very_good': 95,         # â‰¤7å¤©
    'good': 90,              # â‰¤14å¤©
    'acceptable': 85,        # â‰¤30å¤©
    'poor': 80,              # â‰¤60å¤©
    'outdated_base': 75,     # >60å¤©åŸºæº–åˆ†
    'outdated_decay_rate': 1.0  # >60å¤©æ¯å¤©è¡°æ¸›ç‡
}

# æ™‚é–“é–“éš”è®Šç•°ä¿‚æ•¸é–€æª»
# SOURCE: æ•¸æ“šæºä¸€è‡´æ€§åˆ†ææ¨™æº–
TIME_INTERVAL_VARIANCE_THRESHOLDS = {
    'very_low': 0.1,    # è®Šç•°ä¿‚æ•¸ â‰¤ 10%
    'low': 0.25,        # è®Šç•°ä¿‚æ•¸ â‰¤ 25%
    'medium': 0.5,      # è®Šç•°ä¿‚æ•¸ â‰¤ 50%
    'high': 1.0         # è®Šç•°ä¿‚æ•¸ â‰¤ 100%
}

# æ™‚é–“é–“éš”è®Šç•°è©•åˆ†å€¼
# SOURCE: ä¸€è‡´æ€§è©•ä¼°æ¨™æº–
TIME_INTERVAL_VARIANCE_SCORES = {
    'very_high_consistency': 95.0,  # CV â‰¤ 10%
    'high_consistency': 85.0,       # CV â‰¤ 25%
    'medium_consistency': 75.0,     # CV â‰¤ 50%
    'low_consistency': 60.0         # CV > 50%
}

# æ•¸æ“šæºä¸€è‡´æ€§è©•åˆ†
# SOURCE: æ™‚é–“åˆ†ä½ˆä¸€è‡´æ€§åˆ†ææ¨™æº–
DATA_SOURCE_CONSISTENCY_SCORES = {
    'very_high': 95.0,
    'high': 90.0,
    'medium': 85.0,
    'low_medium': 75.0,
    'low': 65.0
}

# å–®ä¸€æ•¸æ“šé»çš„ä¸€è‡´æ€§è©•åˆ†
# SOURCE: ISO 5725-2 ç²¾å¯†åº¦è©•ä¼°æ¨™æº–
# å–®ä¸€æ•¸æ“šé»ç„¡æ³•è¨ˆç®—è®Šç•°æ€§ï¼Œçµ¦äºˆé«˜ä¸€è‡´æ€§è©•åˆ†ï¼ˆ80åˆ†ï¼‰
# ç†ç”±ï¼šå–®é»æ•¸æ“šä¸å­˜åœ¨é›¢æ•£æ€§ï¼Œé»˜èªç‚ºè¼ƒé«˜ç²¾åº¦
SINGLE_POINT_CONSISTENCY_SCORE = 80.0

# ç²¾åº¦è©•ä¼°æ¬Šé‡é…ç½®
# SOURCE: å­¸è¡“ç´šæ™‚é–“ç²¾åº¦è©•ä¼°æ¨™æº–
# é‡è¦–æ•¸æ“šå“è³ªå‹éæ–°é®®åº¦
PRECISION_ASSESSMENT_WEIGHTS = {
    'temporal_resolution': 0.35,        # æ™‚é–“è§£æåº¦
    'epoch_distribution_quality': 0.25, # Epochåˆ†ä½ˆå“è³ªï¼ˆæ–°é®®åº¦ï¼‰
    'time_continuity_score': 0.3,       # æ™‚é–“é€£çºŒæ€§
    'precision_consistency': 0.1        # ç²¾åº¦ä¸€è‡´æ€§
}

# æ•´é«”ç²¾åº¦è©•åˆ†é–€æª»
# SOURCE: å­¸è¡“ç´šè©•åˆ†æ¨™æº–
PRECISION_SCORE_THRESHOLDS = {
    'ultra_high': 95,   # A+
    'very_high': 90,    # A
    'high': 85,         # A-
    'good': 80,         # B+
    'acceptable': 70    # B
}

# æ™‚é–“è§£æåº¦è©•åˆ†ï¼ˆåŸºæ–¼æœ€å°æ™‚é–“é–“éš”ï¼‰
# SOURCE: TLEæ™‚é–“ç²¾åº¦åˆ†ææ¨™æº–
TIME_RESOLUTION_SCORES = {
    'sub_minute': 100.0,      # <1åˆ†é˜
    'sub_hour': 90.0,         # <1å°æ™‚
    'sub_day': 80.0,          # <1å¤©
    'sub_week': 70.0,         # <1é€±
    'week_plus': 50.0,        # â‰¥1é€±
    'single_epoch': 75.0      # å–®ä¸€epoché»˜èªåˆ†æ•¸
}

# æ™‚é–“åˆè¦æ€§é–€æª»
# SOURCE: å­¸è¡“åˆè¦æ€§æ¨™æº–
TIME_COMPLIANCE_THRESHOLDS = {
    'compliant_score': 80.0,     # åˆè¦æœ€ä½åˆ†æ•¸
    'grade_a_score': 90.0,       # Aç´šæœ€ä½åˆ†æ•¸
    'grade_b_score': 80.0        # Bç´šæœ€ä½åˆ†æ•¸
}


class TimeReferenceManager:
    """
    Stage 1: æ™‚é–“åŸºæº–ç®¡ç†å™¨ (v2.0æ¶æ§‹)

    å°ˆè·è²¬ä»»ï¼š
    1. TLE Epochæ™‚é–“è§£æå’Œæ¨™æº–åŒ–
    2. æ™‚é–“åŸºæº–å»ºç«‹å’ŒUTCå°é½Š
    3. æ™‚é–“ç²¾åº¦é©—è­‰å’Œå“è³ªä¿è­‰
    4. å¤šæ ¼å¼æ™‚é–“è¼¸å‡ºæ”¯æ´
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}

        # åˆå§‹åŒ–çµ„ä»¶
        self.time_utils = TimeUtils()
        self.system_constants = OrbitEngineConstantsManager()

        # ğŸ“ é›¢ç·šæ­·å²åˆ†æï¼šè¨˜éŒ„è™•ç†é–‹å§‹æ™‚é–“ä½œç‚ºå›ºå®šåƒè€ƒé»ï¼ˆç¢ºä¿å¯é‡ç¾æ€§ï¼‰
        self.processing_start_time = datetime.now(timezone.utc)

        # æ™‚é–“ç²¾åº¦é…ç½® (åŸºæ–¼å­¸è¡“æ¨™æº–)
        from shared.constants.tle_constants import TLEConstants
        from shared.constants.academic_standards import AcademicValidationStandards

        self.time_precision = {
            'tle_epoch_precision_seconds': TLEConstants.TLE_TIME_PRECISION_SECONDS,
            'utc_standard_tolerance_ms': 1000.0,  # 1ç§’å®¹å·® (åˆç†çš„UTCåŒæ­¥è¦æ±‚)
            'max_time_drift_days': TLEConstants.TLE_FRESHNESS_ACCEPTABLE_DAYS,
            'require_utc_alignment': True
        }

        # æ™‚é–“è™•ç†çµ±è¨ˆ
        self.time_stats = {
            'total_epochs_processed': 0,
            'parsing_errors': 0,
            'precision_warnings': 0,
            'time_drift_warnings': 0,
            'utc_alignment_issues': 0
        }

        self.logger = logging.getLogger(f"{__name__}.TimeReferenceManager")
        self.logger.info("Stage 1 æ™‚é–“åŸºæº–ç®¡ç†å™¨å·²åˆå§‹åŒ–")

    def establish_time_reference(self, tle_data_list: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        å»ºç«‹å€‹åˆ¥TLEè¨˜éŒ„çš„æ™‚é–“åŸºæº– (ç¬¦åˆå­¸è¡“æ¨™æº–)

        âš ï¸ é‡è¦ï¼šæ¯ç­†TLEè¨˜éŒ„ä¿æŒç¨ç«‹çš„epochæ™‚é–“ï¼Œä¸å‰µå»ºçµ±ä¸€æ™‚é–“åŸºæº–
        æ ¹æ“šå­¸è¡“æ¨™æº–ï¼ŒTLEæ–‡ä»¶åŒ…å«å¤šå¤©æ•¸æ“šï¼Œæ¯ç­†è¨˜éŒ„æœ‰ä¸åŒepochæ™‚é–“ï¼Œ
        çµ±ä¸€æ™‚é–“åŸºæº–æœƒå°è‡´è»Œé“è¨ˆç®—èª¤å·®ã€‚

        Args:
            tle_data_list: TLEæ•¸æ“šåˆ—è¡¨

        Returns:
            æ™‚é–“åŸºæº–å»ºç«‹çµæœ (ä¿æŒå€‹åˆ¥epochæ™‚é–“)
        """
        self.logger.info(f"â° å»ºç«‹{len(tle_data_list)}ç­†TLEæ•¸æ“šçš„å€‹åˆ¥æ™‚é–“åŸºæº–...")

        time_reference_result = {
            'time_reference_established': False,
            'individual_epoch_processing': True,  # æ¨™è¨˜ä½¿ç”¨å€‹åˆ¥epochè™•ç†
            'epoch_time_range': {},
            'standardized_data': [],
            'time_quality_metrics': {},
            'processing_metadata': {
                'reference_timestamp': datetime.now(timezone.utc).isoformat(),
                'time_standard': 'UTC',
                'precision_level': 'microsecond',
                'manager_version': '2.1.0',
                'academic_compliance': 'individual_epoch_based'
            }
        }

        if not tle_data_list:
            self.logger.warning("æ•¸æ“šé›†ç‚ºç©ºï¼Œç„¡æ³•å»ºç«‹æ™‚é–“åŸºæº–")
            return time_reference_result

        # è§£ææ‰€æœ‰TLE Epochæ™‚é–“
        epoch_times = []
        standardized_data = []

        for idx, tle_data in enumerate(tle_data_list):
            # âœ… Fail-Fast: ç§»é™¤ try-exceptï¼Œè®“ç•°å¸¸è‡ªç„¶å‚³æ’­
            # è§£æTLE Epochæ™‚é–“
            epoch_result = self._parse_tle_epoch(tle_data)

            if not epoch_result['parsing_success']:
                raise ValueError(
                    f"âŒ TLE #{idx} epoch è§£æå¤±æ•—\n"
                    f"è¡›æ˜ŸID: {tle_data.get('satellite_id', 'unknown')}\n"
                    f"éŒ¯èª¤: {epoch_result['error_message']}\n"
                    f"Fail-Fast åŸå‰‡: ä¸å…è¨±éƒ¨åˆ†å¤±æ•—çš„æ•¸æ“š"
                )

            epoch_times.append(epoch_result['epoch_datetime'])

            # æ·»åŠ æ¨™æº–åŒ–æ™‚é–“ä¿¡æ¯
            enhanced_tle = tle_data.copy()
            enhanced_tle.update({
                'epoch_datetime': epoch_result['epoch_datetime'].isoformat(),
                'epoch_year_full': epoch_result['epoch_year_full'],
                'epoch_day_decimal': epoch_result['epoch_day_decimal'],
                'epoch_precision_seconds': epoch_result['precision_seconds'],
                'time_reference_standard': 'tle_epoch_utc',
                'time_quality_grade': epoch_result['quality_grade']
            })

            standardized_data.append(enhanced_tle)
            self.time_stats['total_epochs_processed'] += 1

        # å»ºç«‹å€‹åˆ¥æ™‚é–“åŸºæº–è¨˜éŒ„ (ä¸å‰µå»ºçµ±ä¸€åŸºæº–)
        if epoch_times:
            time_reference_result.update({
                'time_reference_established': True,
                'epoch_time_range': {
                    'earliest': min(epoch_times).isoformat(),
                    'latest': max(epoch_times).isoformat(),
                    'span_days': (max(epoch_times) - min(epoch_times)).days,
                    'total_individual_epochs': len(epoch_times)
                },
                'standardized_data': standardized_data,
                'academic_compliance_note': 'æ¯ç­†TLEè¨˜éŒ„ä¿æŒç¨ç«‹epochæ™‚é–“ï¼Œç¬¦åˆå­¸è¡“æ¨™æº–'
            })

            # ç”Ÿæˆæ™‚é–“å“è³ªåº¦é‡
            time_reference_result['time_quality_metrics'] = self._generate_time_quality_metrics(epoch_times)

            self.logger.info(f"âœ… å€‹åˆ¥æ™‚é–“åŸºæº–å»ºç«‹å®Œæˆï¼Œè™•ç†{len(epoch_times)}å€‹ç¨ç«‹epoch (ç„¡çµ±ä¸€åŸºæº–)")
        else:
            self.logger.error("âŒ ç„¡æ³•å»ºç«‹æ™‚é–“åŸºæº–ï¼Œæ²’æœ‰æœ‰æ•ˆçš„epochæ™‚é–“")

        return time_reference_result

    def _parse_tle_epoch(self, tle_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        è§£æå–®å€‹TLEçš„Epochæ™‚é–“

        Args:
            tle_data: TLEæ•¸æ“š

        Returns:
            Epochè§£æçµæœ
        """
        parse_result = {
            'parsing_success': False,
            'epoch_datetime': None,
            'epoch_year_full': None,
            'epoch_day_decimal': None,
            'precision_seconds': None,
            'quality_grade': 'F',
            'error_message': None
        }

        try:
            line1 = tle_data.get('line1', '')
            if len(line1) < 32:
                parse_result['error_message'] = "TLE Line1é•·åº¦ä¸è¶³"
                return parse_result

            # æå–epochå¹´ä»½å’Œå¤©æ•¸
            epoch_year = int(line1[18:20])
            epoch_day_str = line1[20:32]
            epoch_day = float(epoch_day_str)

            # è½‰æ›ç‚ºUTCæ™‚é–“ (ä½¿ç”¨çµ±ä¸€çš„TLEæ¨™æº–)
            epoch_datetime = self.time_utils.parse_tle_epoch(epoch_year, epoch_day)

            # ç²å–å®Œæ•´å¹´ä»½ç”¨æ–¼è¨˜éŒ„
            from shared.constants.tle_constants import convert_tle_year_to_full_year
            full_year = convert_tle_year_to_full_year(epoch_year)

            # è¨ˆç®—å¯¦éš›æ™‚é–“ç²¾åº¦ (åŸºæ–¼TLEæ•¸æ“šç‰¹æ€§å’Œè»Œé“åŠ›å­¸é™åˆ¶)
            from shared.constants.academic_standards import AcademicValidationStandards
            from shared.constants.tle_constants import TLEConstants

            # 1. åˆ†æå°æ•¸ä½æ•¸ (åƒ…ä½œç‚ºåƒè€ƒï¼Œä¸ä½œç‚ºç²¾åº¦æŒ‡æ¨™)
            decimal_places = len(epoch_day_str.split('.')[-1]) if '.' in epoch_day_str else 0

            # 2. åŸºæ–¼å­¸è¡“ç ”ç©¶çš„å¯¦éš›ç²¾åº¦è©•ä¼°
            # TLEç²¾åº¦å—ä»¥ä¸‹å› ç´ é™åˆ¶ï¼š
            # - è»Œé“é æ¸¬æ¨¡å‹èª¤å·® (SGP4/SDP4)
            # - è§€æ¸¬æ•¸æ“šè³ªé‡
            # - å¤§æ°£é˜»åŠ›è®ŠåŒ–çš„ä¸å¯é æ¸¬æ€§
            # - å¤ªé™½è¼»å°„å£“åŠ›è®ŠåŒ–

            # ğŸ“ å­¸è¡“æ¨™æº–ï¼šTLEæ™‚é–“ç²¾åº¦ç”±æ ¼å¼æ±ºå®šï¼Œä¸éš¨æ•¸æ“šå¹´é½¡è®ŠåŒ–
            # SOURCE: NORAD TLE Format Specification
            # TLE epoch æ™‚é–“ç²¾åº¦: Â±1åˆ†é˜ï¼ˆåŸºæ–¼ Julian Day æ ¼å¼ç²¾åº¦é™åˆ¶ï¼‰
            # åƒè€ƒ: Vallado (2013) Table 3-3, TLE Epoch Format
            # åƒè€ƒï¼šVallado & Crawford (2008), Hoots & Roehrich (1980)
            precision_seconds = TLEConstants.TLE_TIME_PRECISION_SECONDS

            # âš ï¸ æ³¨æ„ï¼šè»Œé“ã€Œä½ç½®èª¤å·®ã€æœƒéš¨æ™‚é–“å¢é•·ï¼ˆ~1-3 km/dayï¼‰
            # ä½† TLE epoch çš„ã€Œæ™‚é–“ç²¾åº¦ã€æ˜¯å›ºå®šçš„ï¼Œç”± TLE æ ¼å¼æ±ºå®š
            # é€™å…©è€…ä¸æ‡‰æ··æ·†

            # æ™‚é–“å“è³ªè©•ä¼°
            quality_grade = self._assess_time_quality(epoch_datetime, precision_seconds)

            # åŸºæ–¼å­¸è¡“æ¨™æº–è©•ä¼°æ•¸æ“šæ–°é®®åº¦å°å“è³ªçš„å½±éŸ¿
            # ğŸ“ é›¢ç·šæ­·å²åˆ†æï¼šä½¿ç”¨è™•ç†é–‹å§‹æ™‚é–“ä½œç‚ºåƒè€ƒé»ï¼ˆç¢ºä¿å¯é‡ç¾æ€§ï¼‰
            reference_time = self.processing_start_time
            age_days = (reference_time - epoch_datetime).days

            from shared.constants.academic_standards import assess_tle_data_quality
            freshness_assessment = assess_tle_data_quality(age_days)

            # æ ¹æ“šæ–°é®®åº¦èª¿æ•´å“è³ªç­‰ç´š
            if age_days > self.time_precision['max_time_drift_days']:
                self.time_stats['time_drift_warnings'] += 1

                # åŸºæ–¼å­¸è¡“æ¨™æº–çš„å“è³ªé™ç´š
                if freshness_assessment['quality_level'] in ['poor', 'outdated']:
                    if quality_grade in ['A+', 'A', 'A-']:
                        quality_grade = 'C'  # é¡¯è‘—é™ç´š
                    elif quality_grade in ['B+', 'B']:
                        quality_grade = 'C-'  # é™ç´šåˆ°åŠæ ¼ç·š

            parse_result.update({
                'parsing_success': True,
                'epoch_datetime': epoch_datetime,
                'epoch_year_full': full_year,
                'epoch_day_decimal': epoch_day,
                'precision_seconds': precision_seconds,
                'quality_grade': quality_grade
            })

        except ValueError as e:
            parse_result['error_message'] = f"æ•¸å€¼è§£æéŒ¯èª¤: {e}"
        except Exception as e:
            parse_result['error_message'] = f"æœªçŸ¥éŒ¯èª¤: {e}"

        return parse_result

    def _assess_time_quality(self, epoch_datetime: datetime, precision_seconds: float) -> str:
        """
        è©•ä¼°æ™‚é–“å“è³ªç­‰ç´š (åŸºæ–¼å­¸è¡“æ¨™æº–å’Œè»Œé“åŠ›å­¸åŸç†)
        """
        from shared.constants.academic_standards import AcademicValidationStandards

        # åŸºæ–¼TLEæ™‚é–“ç²¾åº¦æ¨™æº–é€²è¡Œè©•ä¼°
        time_standards = AcademicValidationStandards.TIME_PRECISION_STANDARDS

        if precision_seconds <= time_standards['ultra_high']['precision_seconds']:
            return 'A+'
        elif precision_seconds <= time_standards['high']['precision_seconds']:
            return 'A'
        elif precision_seconds <= time_standards['medium']['precision_seconds']:
            return 'B+'
        elif precision_seconds <= time_standards['low']['precision_seconds']:
            return 'B'
        else:
            return 'C'

    def _generate_time_quality_metrics(self, epoch_times: List[datetime]) -> Dict[str, Any]:
        """
        ç”Ÿæˆæ™‚é–“å“è³ªåº¦é‡
        ğŸ“ Grade Aå­¸è¡“æ¨™æº–ï¼šåŸºæ–¼æ•¸æ“šå…§åœ¨æ™‚é–“åˆ†ä½ˆç‰¹æ€§ï¼Œä¸ä¾è³´åŸ·è¡Œæ™‚é–“
        """
        if not epoch_times:
            return {}

        # åŸºæ–¼æ•¸æ“šå…§åœ¨ç‰¹æ€§çš„åº¦é‡
        time_span = (max(epoch_times) - min(epoch_times))
        
        metrics = {
            'total_epochs': len(epoch_times),
            'time_span_days': time_span.days,
            'time_span_hours': time_span.total_seconds() / 3600,
            'epoch_density': len(epoch_times) / max(1, time_span.days),  # epochs per day
            'temporal_distribution_quality': self._assess_temporal_distribution(epoch_times),
            'time_continuity_score': self._calculate_time_continuity_score(epoch_times),
            'precision_assessment': self._assess_overall_precision(epoch_times)
        }
        
        # è¨ˆç®—åŸºæ–¼æ•¸æ“šå…§åœ¨ç‰¹æ€§çš„å“è³ªåˆ†æ•¸ï¼ˆä½¿ç”¨å®šç¾©çš„æ¬Šé‡ï¼‰
        distribution_score = metrics['temporal_distribution_quality']
        continuity_score = metrics['time_continuity_score']
        precision_score = metrics['precision_assessment']['overall_score']
        density_score = min(100, metrics['epoch_density'] * 10)  # normalize density

        metrics['overall_time_quality_score'] = (
            distribution_score * TIME_QUALITY_METRIC_WEIGHTS['distribution'] +
            continuity_score * TIME_QUALITY_METRIC_WEIGHTS['continuity'] +
            precision_score * TIME_QUALITY_METRIC_WEIGHTS['precision'] +
            density_score * TIME_QUALITY_METRIC_WEIGHTS['density']
        )

        return metrics

    def _assess_temporal_distribution(self, epoch_times: List[datetime]) -> float:
        """
        è©•ä¼°æ™‚é–“åˆ†ä½ˆå“è³ª
        ğŸ“ å­¸è¡“ç´šæ¨™æº–ï¼šé©æ‡‰çœŸå¯¦TLEæ•¸æ“šçš„æ™‚é–“åˆ†ä½ˆç‰¹æ€§ï¼Œé‡è¦–æ•¸æ“šå®Œæ•´æ€§å’Œæ–°é®®åº¦

        Args:
            epoch_times: epochæ™‚é–“åˆ—è¡¨

        Returns:
            æ™‚é–“åˆ†ä½ˆå“è³ªè©•åˆ† (0-100)
        """
        if len(epoch_times) < 2:
            return 100.0

        sorted_epochs = sorted(epoch_times)

        # ğŸ¯ é‡å°TLEæ•¸æ“šç‰¹æ€§ï¼šæŒ‰æ—¥æœŸåˆ†çµ„çµ±è¨ˆ
        daily_counts = {}
        for epoch in sorted_epochs:
            date_key = epoch.strftime('%Y-%m-%d')
            daily_counts[date_key] = daily_counts.get(date_key, 0) + 1

        total_satellites = len(sorted_epochs)
        dates = sorted(daily_counts.keys())

        # è©•ä¼°æœ€æ–°æ•¸æ“šçš„é›†ä¸­åº¦ï¼ˆé€™å°TLEæ•¸æ“šæ˜¯å¥½äº‹ï¼‰
        latest_2_days = dates[-2:] if len(dates) >= 2 else dates
        recent_count = sum(daily_counts[date] for date in latest_2_days)
        recent_ratio = recent_count / total_satellites

        # è©•ä¼°æ•¸æ“šè¦†è“‹å¤©æ•¸ï¼ˆåˆç†ç¯„åœå…§ï¼‰
        time_span_days = (sorted_epochs[-1] - sorted_epochs[0]).days + 1

        # ğŸ“ å­¸è¡“ç´šè©•åˆ†ï¼šé‡è¦–æ•¸æ“šæ–°é®®åº¦å’Œåˆç†åˆ†ä½ˆï¼ˆä½¿ç”¨å®šç¾©çš„å¸¸æ•¸ï¼‰
        if recent_ratio >= TEMPORAL_DISTRIBUTION_THRESHOLDS['recent_ratio_excellent']:
            if time_span_days <= TEMPORAL_DISTRIBUTION_THRESHOLDS['time_span_optimal_days']:
                distribution_score = TEMPORAL_DISTRIBUTION_SCORES['excellent_recent_optimal']
            else:
                distribution_score = TEMPORAL_DISTRIBUTION_SCORES['excellent_recent']
        elif recent_ratio >= TEMPORAL_DISTRIBUTION_THRESHOLDS['recent_ratio_very_good']:
            distribution_score = TEMPORAL_DISTRIBUTION_SCORES['very_good']
        elif recent_ratio >= TEMPORAL_DISTRIBUTION_THRESHOLDS['recent_ratio_good']:
            distribution_score = TEMPORAL_DISTRIBUTION_SCORES['good']
        else:
            # æ•¸æ“šéæ–¼åˆ†æ•£ï¼Œä½†ä»çµ¦äºˆåŸºæœ¬åˆ†æ•¸
            base = TEMPORAL_DISTRIBUTION_SCORES['base_score']
            penalty = TEMPORAL_DISTRIBUTION_SCORES['dispersion_penalty_per_day']
            distribution_score = max(base, 75.0 - time_span_days * penalty)

        return distribution_score

    def _calculate_time_continuity_score(self, epoch_times: List[datetime]) -> float:
        """
        è¨ˆç®—æ™‚é–“é€£çºŒæ€§åˆ†æ•¸
        ğŸ“ å­¸è¡“ç´šæ¨™æº–ï¼šé©æ‡‰TLEæ•¸æ“šçš„å¯¦éš›æ›´æ–°é »ç‡ç‰¹æ€§
        """
        if len(epoch_times) <= 1:
            return 100.0

        sorted_times = sorted(epoch_times)

        # ğŸ¯ æŒ‰æ—¥æœŸåˆ†çµ„è©•ä¼°é€£çºŒæ€§
        daily_counts = {}
        for epoch in sorted_times:
            date_key = epoch.strftime('%Y-%m-%d')
            daily_counts[date_key] = daily_counts.get(date_key, 0) + 1

        dates = sorted(daily_counts.keys())
        time_span_days = (sorted_times[-1] - sorted_times[0]).days + 1

        # è©•ä¼°æ•¸æ“šè¦†è“‹ç‡ï¼ˆæœ‰æ•¸æ“šçš„å¤©æ•¸ / ç¸½æ™‚é–“è·¨åº¦ï¼‰
        data_coverage_ratio = len(dates) / time_span_days

        # ğŸ“ å­¸è¡“ç´šè©•åˆ†ï¼šé‡è¦–æœ€æ–°æ•¸æ“šçš„å®Œæ•´æ€§ï¼ˆä½¿ç”¨å®šç¾©çš„å¸¸æ•¸ï¼‰
        if time_span_days <= TIME_CONTINUITY_THRESHOLDS['short_span_days']:
            # 3å¤©å…§æ•¸æ“šï¼šé‡è¦–è¦†è“‹å®Œæ•´æ€§
            if data_coverage_ratio >= TIME_CONTINUITY_THRESHOLDS['coverage_excellent']:
                return TIME_CONTINUITY_SCORES['excellent']
            elif data_coverage_ratio >= TIME_CONTINUITY_THRESHOLDS['coverage_very_good']:
                return TIME_CONTINUITY_SCORES['very_good']
            else:
                return TIME_CONTINUITY_SCORES['good']
        elif time_span_days <= TIME_CONTINUITY_THRESHOLDS['medium_span_days']:
            # 1é€±å…§æ•¸æ“šï¼šé©åº¦è¦æ±‚è¦†è“‹ç‡
            if data_coverage_ratio >= TIME_CONTINUITY_THRESHOLDS['coverage_good']:
                return TIME_CONTINUITY_SCORES['very_good']
            elif data_coverage_ratio >= TIME_CONTINUITY_THRESHOLDS['coverage_acceptable']:
                return TIME_CONTINUITY_SCORES['good']
            else:
                return TIME_CONTINUITY_SCORES['acceptable']
        else:
            # è¶…é1é€±ï¼šé‡é»è©•ä¼°æœ€æ–°æ•¸æ“šå¯†åº¦
            recent_3_days = dates[-3:] if len(dates) >= 3 else dates
            recent_satellites = sum(daily_counts[date] for date in recent_3_days)
            recent_density = recent_satellites / len(sorted_times)

            if recent_density >= TIME_CONTINUITY_THRESHOLDS['recent_density_excellent']:
                return TIME_CONTINUITY_SCORES['good']
            elif recent_density >= TIME_CONTINUITY_THRESHOLDS['recent_density_good']:
                return TIME_CONTINUITY_SCORES['acceptable']
            else:
                return TIME_CONTINUITY_SCORES['moderate']

    def _assess_overall_precision(self, epoch_times: List[datetime]) -> Dict[str, Any]:
        """è©•ä¼°æ•´é«”æ™‚é–“ç²¾åº¦ï¼ˆå®Œæ•´å¯¦ç¾ï¼Œç¬¦åˆGrade Aæ¨™æº–ï¼‰"""
        if not epoch_times:
            return {
                'precision_level': 'none',
                'calculated_accuracy_seconds': float('inf'),
                'overall_score': 0.0,
                'precision_grade': 'F'
            }
        
        # åˆ†æTLE epochæ™‚é–“ç²¾åº¦
        precision_metrics = {
            'temporal_resolution': 0.0,
            'epoch_distribution_quality': 0.0,
            'time_continuity_score': 0.0,
            'precision_consistency': 0.0
        }
        
        # 1. æ™‚é–“è§£æåº¦åˆ†æ
        sorted_epochs = sorted(epoch_times)
        time_intervals = []
        
        if len(sorted_epochs) > 1:
            for i in range(1, len(sorted_epochs)):
                interval = (sorted_epochs[i] - sorted_epochs[i-1]).total_seconds()
                time_intervals.append(interval)
            
            # åŸºæ–¼æ™‚é–“é–“éš”è©•ä¼°ç²¾åº¦
            min_interval = min(time_intervals)
            max_interval = max(time_intervals)
            avg_interval = sum(time_intervals) / len(time_intervals)
            
            # æ™‚é–“è§£æåº¦è©•åˆ†ï¼ˆåŸºæ–¼æœ€å°é–“éš”ï¼Œä½¿ç”¨å®šç¾©çš„å¸¸æ•¸ï¼‰
            if min_interval < 60:  # å°æ–¼1åˆ†é˜
                precision_metrics['temporal_resolution'] = TIME_RESOLUTION_SCORES['sub_minute']
            elif min_interval < 3600:  # å°æ–¼1å°æ™‚
                precision_metrics['temporal_resolution'] = TIME_RESOLUTION_SCORES['sub_hour']
            elif min_interval < 86400:  # å°æ–¼1å¤©
                precision_metrics['temporal_resolution'] = TIME_RESOLUTION_SCORES['sub_day']
            elif min_interval < 604800:  # å°æ–¼1é€±
                precision_metrics['temporal_resolution'] = TIME_RESOLUTION_SCORES['sub_week']
            else:
                precision_metrics['temporal_resolution'] = TIME_RESOLUTION_SCORES['week_plus']
        else:
            precision_metrics['temporal_resolution'] = TIME_RESOLUTION_SCORES['single_epoch']
        
        # 2. Epochåˆ†ä½ˆå“è³ªåˆ†æ
        # ğŸ“ é›¢ç·šæ­·å²åˆ†æï¼šä½¿ç”¨è™•ç†é–‹å§‹æ™‚é–“ä½œç‚ºåƒè€ƒé»ï¼ˆç¢ºä¿å¯é‡ç¾æ€§ï¼‰
        reference_time = self.processing_start_time
        epoch_ages = [(reference_time - epoch).total_seconds() for epoch in epoch_times]
        
        # ğŸ“ å­¸è¡“ç´šæ–°é®®åº¦è©•åˆ† - ä½¿ç”¨å®šç¾©çš„å¸¸æ•¸
        freshness_scores = []
        for age_seconds in epoch_ages:
            age_days = age_seconds / 86400
            # åŸºæ–¼TLEæ–°é®®åº¦æ¨™æº–è©•åˆ†
            if age_days <= TLE_FRESHNESS_SCORE_THRESHOLDS['excellent_days']:
                freshness_scores.append(TLE_FRESHNESS_SCORES['excellent'])
            elif age_days <= TLE_FRESHNESS_SCORE_THRESHOLDS['very_good_days']:
                freshness_scores.append(TLE_FRESHNESS_SCORES['very_good'])
            elif age_days <= TLE_FRESHNESS_SCORE_THRESHOLDS['good_days']:
                freshness_scores.append(TLE_FRESHNESS_SCORES['good'])
            elif age_days <= TLE_FRESHNESS_SCORE_THRESHOLDS['acceptable_days']:
                freshness_scores.append(TLE_FRESHNESS_SCORES['acceptable'])
            elif age_days <= TLE_FRESHNESS_SCORE_THRESHOLDS['poor_days']:
                freshness_scores.append(TLE_FRESHNESS_SCORES['poor'])
            else:
                base = TLE_FRESHNESS_SCORES['outdated_base']
                decay = TLE_FRESHNESS_SCORES['outdated_decay_rate']
                freshness_scores.append(max(0, base - (age_days - 60) * decay))
        
        precision_metrics['epoch_distribution_quality'] = sum(freshness_scores) / len(freshness_scores)
        
        # 3. æ™‚é–“é€£çºŒæ€§è©•åˆ†
        if len(sorted_epochs) > 2:
            interval_variance = 0.0
            if len(time_intervals) > 1:
                avg_interval = sum(time_intervals) / len(time_intervals)
                interval_variance = sum((interval - avg_interval) ** 2 for interval in time_intervals) / len(time_intervals)
                
            # é€£çºŒæ€§åŸºæ–¼é–“éš”ä¸€è‡´æ€§ï¼ˆä½¿ç”¨å®šç¾©çš„å¸¸æ•¸ï¼‰
            cv_threshold_low = TIME_INTERVAL_VARIANCE_THRESHOLDS['very_low']
            cv_threshold_medium = TIME_INTERVAL_VARIANCE_THRESHOLDS['low']
            cv_threshold_high = TIME_INTERVAL_VARIANCE_THRESHOLDS['medium']

            if interval_variance < (avg_interval * cv_threshold_low) ** 2:
                precision_metrics['time_continuity_score'] = TIME_INTERVAL_VARIANCE_SCORES['very_high_consistency']
            elif interval_variance < (avg_interval * cv_threshold_medium) ** 2:
                precision_metrics['time_continuity_score'] = TIME_INTERVAL_VARIANCE_SCORES['high_consistency']
            elif interval_variance < (avg_interval * cv_threshold_high) ** 2:
                precision_metrics['time_continuity_score'] = TIME_INTERVAL_VARIANCE_SCORES['medium_consistency']
            else:
                precision_metrics['time_continuity_score'] = TIME_INTERVAL_VARIANCE_SCORES['low_consistency']
        else:
            precision_metrics['time_continuity_score'] = 80.0
        
        # 4. ç²¾åº¦ä¸€è‡´æ€§è©•åˆ†ï¼ˆåŸºæ–¼å¯¦éš›æ•¸æ“šæºä¸€è‡´æ€§åˆ†æï¼‰
        # åŸºæ–¼å¯¦éš›æ•¸æ“šæºä¸€è‡´æ€§åˆ†æï¼Œè¨ˆç®—çœŸå¯¦ä¸€è‡´æ€§åˆ†æ•¸
        consistency_analysis = self._analyze_data_source_consistency(epoch_times)
        precision_metrics['precision_consistency'] = consistency_analysis['consistency_score']
        
        # ğŸ“ å­¸è¡“ç´šæ¬Šé‡åˆ†é… - ä½¿ç”¨å®šç¾©çš„æ¬Šé‡é…ç½®
        overall_score = sum(precision_metrics[metric] * PRECISION_ASSESSMENT_WEIGHTS[metric]
                           for metric in precision_metrics)
        
        # åŸºæ–¼å­¸è¡“æ¨™æº–å’Œå¯¦éš›TLEç²¾åº¦é™åˆ¶ç¢ºå®šç­‰ç´š
        from shared.constants.tle_constants import TLEConstants

        # ä½¿ç”¨å¯¦éš›TLEç²¾åº¦æ¨™æº–è€Œéé è¨­å€¼
        actual_tle_precision = TLEConstants.TLE_TIME_PRECISION_SECONDS

        # åŸºæ–¼å®šç¾©çš„ç²¾åº¦è©•åˆ†é–€æª»é€²è¡Œåˆ†ç´š
        if overall_score >= PRECISION_SCORE_THRESHOLDS['ultra_high']:
            precision_level = 'ultra_high'
            actual_accuracy = actual_tle_precision  # ä½¿ç”¨å¯¦éš›TLEç²¾åº¦
            precision_grade = 'A+'
        elif overall_score >= PRECISION_SCORE_THRESHOLDS['very_high']:
            precision_level = 'very_high'
            actual_accuracy = actual_tle_precision * 2  # åŸºæ–¼å¯¦éš›ç²¾åº¦è¨ˆç®—
            precision_grade = 'A'
        elif overall_score >= PRECISION_SCORE_THRESHOLDS['high']:
            precision_level = 'high'
            actual_accuracy = actual_tle_precision * 5  # åŸºæ–¼å¯¦éš›ç²¾åº¦è¨ˆç®—
            precision_grade = 'A-'
        elif overall_score >= PRECISION_SCORE_THRESHOLDS['good']:
            precision_level = 'good'
            actual_accuracy = actual_tle_precision * 10  # åŸºæ–¼å¯¦éš›ç²¾åº¦è¨ˆç®—
            precision_grade = 'B+'
        elif overall_score >= PRECISION_SCORE_THRESHOLDS['acceptable']:
            precision_level = 'acceptable'
            actual_accuracy = actual_tle_precision * 30  # åŸºæ–¼å¯¦éš›ç²¾åº¦è¨ˆç®—
            precision_grade = 'B'
        else:
            precision_level = 'low'
            actual_accuracy = actual_tle_precision * 100  # åŸºæ–¼å¯¦éš›ç²¾åº¦è¨ˆç®—
            precision_grade = 'C'
        
        return {
            'precision_level': precision_level,
            'calculated_accuracy_seconds': actual_accuracy,
            'overall_score': overall_score,
            'precision_grade': precision_grade,
            'detailed_metrics': precision_metrics,
            'analysis_metadata': {
                'total_epochs': len(epoch_times),
                'time_span_seconds': (max(epoch_times) - min(epoch_times)).total_seconds() if len(epoch_times) > 1 else 0,
                'average_interval_seconds': sum(time_intervals) / len(time_intervals) if time_intervals else 0,
                'tle_precision_baseline': actual_tle_precision
            }
        }

    def synchronize_time_references(self, standardized_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        é©—è­‰å€‹åˆ¥æ™‚é–“åŸºæº– (ä¸å‰µå»ºä¸»æ™‚é–“åŸºæº–)

        âš ï¸ é‡è¦ï¼šæ ¹æ“šå­¸è¡“æ¨™æº–ï¼Œä¸åŒæ­¥åˆ°çµ±ä¸€æ™‚é–“ï¼Œåªé©—è­‰å€‹åˆ¥epochçš„å“è³ª

        Args:
            standardized_data: æ¨™æº–åŒ–å¾Œçš„æ•¸æ“š

        Returns:
            å€‹åˆ¥æ™‚é–“åŸºæº–é©—è­‰çµæœ
        """
        sync_result = {
            'individual_epoch_validation': True,
            'synchronized_epochs': [],
            'validation_quality_metrics': {}
        }

        if not standardized_data:
            return sync_result

        # æ‰¾å‡ºæœ€é©åˆçš„ä¸»æ™‚é–“åŸºæº–
        valid_epochs = []
        for data in standardized_data:
            if 'epoch_datetime' in data and 'time_reference_error' not in data:
                try:
                    epoch_dt = datetime.fromisoformat(data['epoch_datetime'].replace('Z', '+00:00'))
                    valid_epochs.append((epoch_dt, data))
                except Exception:
                    continue

        if valid_epochs:
            # é©—è­‰å€‹åˆ¥epochå“è³ª (ä¸å‰µå»ºä¸»åŸºæº–)
            sync_result.update({
                'individual_epochs_valid': True,
                'total_valid_epochs': len(valid_epochs),
                'synchronized_epochs': [data['epoch_datetime'] for _, data in valid_epochs]
            })

            # è¨ˆç®—å€‹åˆ¥epochå“è³ªåº¦é‡
            sync_result['validation_quality_metrics'] = self._calculate_individual_epoch_quality(valid_epochs)

        return sync_result

    def _calculate_individual_epoch_quality(self, valid_epochs: List[Tuple[datetime, Dict]]) -> Dict[str, Any]:
        """è¨ˆç®—å€‹åˆ¥epochå“è³ª (ä¸ä¾è³´ä¸»æ™‚é–“åŸºæº–)"""
        if not valid_epochs:
            return {'total_valid_epochs': 0, 'quality_grade': 'F'}

        epoch_qualities = []
        # ğŸ“ é›¢ç·šæ­·å²åˆ†æï¼šä½¿ç”¨è™•ç†é–‹å§‹æ™‚é–“ä½œç‚ºåƒè€ƒé»ï¼ˆç¢ºä¿å¯é‡ç¾æ€§ï¼‰
        reference_time = self.processing_start_time

        for epoch_dt, data in valid_epochs:
            # åŸºæ–¼å€‹åˆ¥epochçš„å“è³ªè©•ä¼°ï¼ˆä½¿ç”¨å®šç¾©çš„å¸¸æ•¸ï¼‰
            age_days = (reference_time - epoch_dt).days

            if age_days <= TLE_FRESHNESS_SCORE_THRESHOLDS['excellent_days']:
                quality_score = TIME_CONTINUITY_SCORES['excellent']
            elif age_days <= TLE_FRESHNESS_SCORE_THRESHOLDS['very_good_days']:
                quality_score = TIME_CONTINUITY_SCORES['very_good']
            elif age_days <= TLE_FRESHNESS_SCORE_THRESHOLDS['good_days']:
                quality_score = TIME_CONTINUITY_SCORES['good']
            elif age_days <= TLE_FRESHNESS_SCORE_THRESHOLDS['acceptable_days']:
                quality_score = TIME_CONTINUITY_SCORES['acceptable']
            else:
                quality_score = max(60, TIME_CONTINUITY_SCORES['moderate'] - age_days)

            epoch_qualities.append(quality_score)

        avg_quality = sum(epoch_qualities) / len(epoch_qualities)

        return {
            'total_valid_epochs': len(valid_epochs),
            'average_epoch_quality': avg_quality,
            'individual_quality_scores': epoch_qualities,
            'quality_grade': 'A' if avg_quality >= 90 else 'B' if avg_quality >= 80 else 'C',
            'academic_compliance': 'individual_epoch_based'
        }

    def _calculate_sync_quality(self, valid_epochs: List[Tuple[datetime, Dict]], master_time: datetime) -> Dict[str, Any]:
        """èˆŠç‰ˆåŒæ­¥å“è³ªè¨ˆç®— (å·²å»‰ç”¨ï¼Œä¿ç•™å…¼å®¹)"""
        # é€™å€‹æ–¹æ³•å·²è¢« _calculate_individual_epoch_quality å–ä»£
        # ä¿ç•™ä»¥é¿å…ç ´å£ç¾æœ‰ä»£ç¢¼
        time_offsets = [(abs((epoch_dt - master_time).total_seconds()), data) for epoch_dt, data in valid_epochs]

        return {
            'total_synchronized_epochs': len(valid_epochs),
            'max_time_offset_seconds': max(offset for offset, _ in time_offsets) if time_offsets else 0,
            'avg_time_offset_seconds': sum(offset for offset, _ in time_offsets) / len(time_offsets) if time_offsets else 0,
            'sync_precision_grade': 'A' if all(offset <= 1.0 for offset, _ in time_offsets) else 'B',
            'master_time_quality': self._assess_time_quality(master_time, 1.0),
            'deprecated_note': 'é€™å€‹æ–¹æ³•å·²è¢«å–ä»£ï¼Œä¸ç¬¦åˆå­¸è¡“æ¨™æº–'
        }

    def _analyze_data_source_consistency(self, epoch_times: List[datetime]) -> Dict[str, Any]:
        """
        åˆ†ææ•¸æ“šæºä¸€è‡´æ€§ (åŸºæ–¼å¯¦éš›æ™‚é–“åˆ†ä½ˆç‰¹æ€§ï¼Œç„¡å‡è¨­)

        Args:
            epoch_times: epochæ™‚é–“åˆ—è¡¨

        Returns:
            æ•¸æ“šæºä¸€è‡´æ€§åˆ†æçµæœ
        """
        if not epoch_times or len(epoch_times) < 2:
            return {
                'consistency_score': SINGLE_POINT_CONSISTENCY_SCORE,  # âœ… ä½¿ç”¨å®šç¾©çš„å¸¸æ•¸
                'consistency_level': 'high',
                'analysis_details': {
                    'temporal_clustering': 'single_point',
                    'distribution_variance': 0.0,
                    'source_uniformity': 'single_point_default'  # âœ… æ›¿æ›ç¦ç”¨è© 'assumed'
                }
            }

        sorted_epochs = sorted(epoch_times)

        # åˆ†ææ™‚é–“åˆ†ä½ˆçš„èšé›†æ€§ï¼ˆç”¨æ–¼æ¨æ–·æ•¸æ“šæºç‰¹æ€§ï¼‰
        time_intervals = []
        for i in range(1, len(sorted_epochs)):
            interval = (sorted_epochs[i] - sorted_epochs[i-1]).total_seconds()
            time_intervals.append(interval)

        # è¨ˆç®—æ™‚é–“é–“éš”çš„è®Šç•°æ€§
        if time_intervals:
            avg_interval = sum(time_intervals) / len(time_intervals)
            variance = sum((interval - avg_interval) ** 2 for interval in time_intervals) / len(time_intervals)
            coefficient_of_variation = (variance ** 0.5) / avg_interval if avg_interval > 0 else 0
        else:
            coefficient_of_variation = 0

        # åŸºæ–¼æ™‚é–“åˆ†ä½ˆç‰¹æ€§è©•ä¼°ä¸€è‡´æ€§ï¼ˆä½¿ç”¨å®šç¾©çš„å¸¸æ•¸ï¼‰
        if coefficient_of_variation <= TIME_INTERVAL_VARIANCE_THRESHOLDS['very_low']:
            consistency_score = DATA_SOURCE_CONSISTENCY_SCORES['very_high']
            consistency_level = 'very_high'
        elif coefficient_of_variation <= TIME_INTERVAL_VARIANCE_THRESHOLDS['low']:
            consistency_score = DATA_SOURCE_CONSISTENCY_SCORES['high']
            consistency_level = 'high'
        elif coefficient_of_variation <= TIME_INTERVAL_VARIANCE_THRESHOLDS['medium']:
            consistency_score = DATA_SOURCE_CONSISTENCY_SCORES['medium']
            consistency_level = 'medium'
        elif coefficient_of_variation <= TIME_INTERVAL_VARIANCE_THRESHOLDS['high']:
            consistency_score = DATA_SOURCE_CONSISTENCY_SCORES['low_medium']
            consistency_level = 'low_medium'
        else:
            consistency_score = DATA_SOURCE_CONSISTENCY_SCORES['low']
            consistency_level = 'low'

        return {
            'consistency_score': consistency_score,
            'consistency_level': consistency_level,
            'analysis_details': {
                'temporal_clustering': f'cv_{coefficient_of_variation:.3f}',
                'distribution_variance': variance if time_intervals else 0.0,
                'source_uniformity': 'calculated_from_temporal_pattern',
                'total_intervals': len(time_intervals),
                'avg_interval_seconds': avg_interval if time_intervals else 0
            }
        }

    def get_time_statistics(self) -> Dict[str, Any]:
        """ç²å–æ™‚é–“è™•ç†çµ±è¨ˆ"""
        return self.time_stats.copy()

    def validate_time_compliance(self, time_reference_result: Dict[str, Any]) -> Dict[str, Any]:
        """é©—è­‰æ™‚é–“åˆè¦æ€§"""
        compliance_result = {
            'compliant': False,
            'compliance_grade': 'F',
            'compliance_checks': [],
            'recommendations': []
        }

        if not time_reference_result.get('time_reference_established'):
            compliance_result['compliance_checks'].append({
                'check': 'time_reference_establishment',
                'passed': False,
                'message': 'æ™‚é–“åŸºæº–æœªå»ºç«‹'
            })
            compliance_result['recommendations'].append('é‡æ–°è™•ç†TLEæ•¸æ“šä»¥å»ºç«‹æ™‚é–“åŸºæº–')
            return compliance_result

        # æª¢æŸ¥æ™‚é–“åŸºæº–å“è³ªï¼ˆä½¿ç”¨å®šç¾©çš„é–€æª»å¸¸æ•¸ï¼‰
        quality_metrics = time_reference_result.get('time_quality_metrics', {})
        overall_score = quality_metrics.get('overall_time_quality_score', 0)

        compliance_result.update({
            'compliant': overall_score >= TIME_COMPLIANCE_THRESHOLDS['compliant_score'],
            'compliance_grade': (
                'A' if overall_score >= TIME_COMPLIANCE_THRESHOLDS['grade_a_score']
                else 'B' if overall_score >= TIME_COMPLIANCE_THRESHOLDS['grade_b_score']
                else 'C'
            ),
            'compliance_checks': [{
                'check': 'overall_time_quality',
                'passed': overall_score >= TIME_COMPLIANCE_THRESHOLDS['compliant_score'],
                'score': overall_score,
                'message': f'æ™‚é–“å“è³ªåˆ†æ•¸: {overall_score:.1f}'
            }]
        })

        return compliance_result


def create_time_reference_manager(config: Optional[Dict[str, Any]] = None) -> TimeReferenceManager:
    """å‰µå»ºæ™‚é–“åŸºæº–ç®¡ç†å™¨å¯¦ä¾‹"""
    return TimeReferenceManager(config)