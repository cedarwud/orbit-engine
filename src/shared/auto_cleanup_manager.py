#!/usr/bin/env python3
# ğŸ§¹ è‡ªå‹•æ¸…ç†ç®¡ç†å™¨
"""
Auto Cleanup Manager
åŠŸèƒ½: æ™ºèƒ½æ¸…ç†èˆŠæ•¸æ“šæ–‡ä»¶ï¼Œæ”¯æŒå¤šç¨®æ¸…ç†ç­–ç•¥
"""

import os
import glob
import time
from pathlib import Path
from typing import List, Dict, Optional
import logging

class AutoCleanupManager:
    """è‡ªå‹•æ¸…ç†ç®¡ç†å™¨"""
    
    def __init__(self, output_dir: Optional[str] = None):
        self.output_dir = Path(output_dir) if output_dir else Path("data/processing_outputs")
        self.logger = logging.getLogger('AutoCleanup')
        
        # ğŸ›¡ï¸ å—ä¿è­·è·¯å¾‘ - ä¸æ¸…ç† RL è¨“ç·´æ•¸æ“šè³‡æ–™å¤¾
        self.protected_paths = [
            '/home/sat/ntn-stack/netstack/tle_data/',
            '/netstack/tle_data/',
            'data/tle_data/'
        ]
        
        # æ¸…ç†æ¨¡å¼é…ç½®
        self.cleanup_patterns = {
            # é–‹ç™¼éšæ®µè¼¸å‡º
            'dev_outputs': [
                'data/dev_processing_outputs/*.json',
                'data/processing_outputs/*.json',
                str(self.output_dir / "*.json"),
                str(self.output_dir / "*.pkl"),
                str(self.output_dir / "*.cache")
            ],
            # å®¹å™¨æ•¸æ“š (æ’é™¤ RL è¨“ç·´æ•¸æ“š)
            'container_data': [
                'data/stage*.json',
                'data/*_results.json',
                'temp/*'  # åƒ…è‡¨æ™‚æ•¸æ“š
            ],
            # è‡¨æ™‚å¿«å– (åƒ…å¤–éƒ¨ç·©å­˜ï¼Œä¸åŒ…å« RL æ•¸æ“š)
            'temp_cache': [
                'data/cache/tle_cache/*.tle',          # å¤–éƒ¨è‡¨æ™‚ç·©å­˜
                'data/cache/sgp4_cache/*.pkl',         # SGP4 è¨ˆç®—ç·©å­˜
                'data/cache/leo_*.tmp',                # LEO è‡¨æ™‚æ–‡ä»¶
                'data/cache/orbit_cache/*.pkl'         # è»Œé“è¨ˆç®—ç·©å­˜
            ],
            # å…¨éƒ¨æ¸…ç†
            'all': []  # å°‡åœ¨initä¸­è¨­ç½®
        }
        
        # åˆä½µæ‰€æœ‰æ¨¡å¼
        self.cleanup_patterns['all'] = []
        for patterns in self.cleanup_patterns.values():
            if isinstance(patterns, list):
                self.cleanup_patterns['all'].extend(patterns)
    
    def cleanup_before_run(self, mode: str = 'dev_outputs') -> int:
        """åŸ·è¡Œå‰æ™ºèƒ½æ¸…ç†"""
        self.logger.info(f"ğŸ§¹ é–‹å§‹æ¸…ç†æ¨¡å¼: {mode}")
        
        patterns = self.cleanup_patterns.get(mode, self.cleanup_patterns['dev_outputs'])
        cleaned_files = 0
        
        for pattern in patterns:
            try:
                for file_path in glob.glob(pattern):
                    if self._safe_to_delete(file_path):
                        os.remove(file_path)
                        cleaned_files += 1
                        self.logger.debug(f"ğŸ§¹ å·²æ¸…ç†: {file_path}")
            except Exception as e:
                self.logger.warning(f"âš ï¸ æ¸…ç†å¤±æ•—: {pattern} - {e}")
        
        if cleaned_files > 0:
            self.logger.info(f"âœ… æ¸…ç†å®Œæˆï¼Œå…±æ¸…ç† {cleaned_files} å€‹æª”æ¡ˆ")
        else:
            self.logger.info("ğŸ“ ç„¡èˆŠæª”æ¡ˆéœ€è¦æ¸…ç†")
        
        # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        return cleaned_files
    
    def cleanup_by_age(self, hours: int = 24, mode: str = 'dev_outputs') -> int:
        """æŒ‰æ™‚é–“æ¸…ç†èˆŠæª”æ¡ˆ"""
        self.logger.info(f"ğŸ•’ æ¸…ç† {hours} å°æ™‚å‰çš„æª”æ¡ˆ...")
        
        cutoff_time = time.time() - (hours * 3600)
        patterns = self.cleanup_patterns.get(mode, self.cleanup_patterns['dev_outputs'])
        cleaned_files = 0
        
        for pattern in patterns:
            try:
                for file_path in glob.glob(pattern):
                    if os.path.getmtime(file_path) < cutoff_time and self._safe_to_delete(file_path):
                        os.remove(file_path)
                        cleaned_files += 1
                        self.logger.debug(f"ğŸ•’ æ¸…ç†éæœŸæª”æ¡ˆ: {file_path}")
            except Exception as e:
                self.logger.warning(f"âš ï¸ æ¸…ç†å¤±æ•—: {pattern} - {e}")
        
        self.logger.info(f"âœ… æŒ‰æ™‚é–“æ¸…ç†å®Œæˆï¼Œå…±æ¸…ç† {cleaned_files} å€‹æª”æ¡ˆ")
        return cleaned_files
    
    def cleanup_by_size(self, max_total_mb: int = 1024, mode: str = 'dev_outputs') -> int:
        """æŒ‰å¤§å°æ¸…ç†æª”æ¡ˆï¼Œä¿æŒç¸½å¤§å°åœ¨é™åˆ¶å…§"""
        self.logger.info(f"ğŸ“ æ¸…ç†ä»¥ä¿æŒç¸½å¤§å° < {max_total_mb}MB...")
        
        patterns = self.cleanup_patterns.get(mode, self.cleanup_patterns['dev_outputs'])
        
        # æ”¶é›†æ‰€æœ‰æª”æ¡ˆå’Œå¤§å°
        files_info = []
        for pattern in patterns:
            try:
                for file_path in glob.glob(pattern):
                    if self._safe_to_delete(file_path):
                        file_size = os.path.getsize(file_path)
                        file_mtime = os.path.getmtime(file_path)
                        files_info.append({
                            'path': file_path,
                            'size': file_size,
                            'mtime': file_mtime
                        })
            except Exception as e:
                self.logger.warning(f"âš ï¸ æª”æ¡ˆä¿¡æ¯æ”¶é›†å¤±æ•—: {pattern} - {e}")
        
        # æŒ‰æ™‚é–“æ’åºï¼Œå„ªå…ˆåˆªé™¤èˆŠæª”æ¡ˆ
        files_info.sort(key=lambda x: x['mtime'])
        
        # è¨ˆç®—ç¸½å¤§å°
        total_size_mb = sum(f['size'] for f in files_info) / 1024 / 1024
        max_total_bytes = max_total_mb * 1024 * 1024
        
        cleaned_files = 0
        current_size = sum(f['size'] for f in files_info)
        
        if current_size <= max_total_bytes:
            self.logger.info(f"ğŸ“ ç•¶å‰ç¸½å¤§å° {total_size_mb:.1f}MB åœ¨é™åˆ¶å…§ï¼Œç„¡éœ€æ¸…ç†")
            return 0
        
        # åˆªé™¤èˆŠæª”æ¡ˆç›´åˆ°å¤§å°ç¬¦åˆè¦æ±‚
        for file_info in files_info:
            if current_size <= max_total_bytes:
                break
            
            try:
                os.remove(file_info['path'])
                current_size -= file_info['size']
                cleaned_files += 1
                self.logger.debug(f"ğŸ“ æ¸…ç†å¤§æª”æ¡ˆ: {file_info['path']}")
            except Exception as e:
                self.logger.warning(f"âš ï¸ æ¸…ç†å¤±æ•—: {file_info['path']} - {e}")
        
        final_size_mb = current_size / 1024 / 1024
        self.logger.info(f"âœ… æŒ‰å¤§å°æ¸…ç†å®Œæˆï¼Œå…±æ¸…ç† {cleaned_files} å€‹æª”æ¡ˆï¼Œç¾åœ¨ç¸½å¤§å° {final_size_mb:.1f}MB")
        return cleaned_files
    
    def _safe_to_delete(self, file_path: str) -> bool:
        """æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å¯ä»¥å®‰å…¨åˆªé™¤"""
        try:
            # æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å­˜åœ¨ä¸”æ˜¯æª”æ¡ˆ
            if not os.path.isfile(file_path):
                return False
            
            # ğŸ›¡ï¸ ç°¡å–®ä¿è­· RL è¨“ç·´æ•¸æ“šè³‡æ–™å¤¾
            for protected_path in self.protected_paths:
                if file_path.startswith(protected_path):
                    self.logger.warning(f"ğŸ›¡ï¸ å—ä¿è­·è·¯å¾‘ï¼Œè·³é: {file_path}")
                    return False
            
            # æª¢æŸ¥æª”æ¡ˆæ˜¯å¦æ­£åœ¨è¢«ä½¿ç”¨ï¼ˆç°¡å–®æª¢æŸ¥ï¼‰
            try:
                with open(file_path, 'a'):
                    pass
                return True
            except IOError:
                self.logger.warning(f"âš ï¸ æª”æ¡ˆå¯èƒ½æ­£åœ¨ä½¿ç”¨: {file_path}")
                return False
                
        except Exception:
            return False
    
    def get_cleanup_stats(self, mode: str = 'dev_outputs') -> Dict:
        """ç²å–æ¸…ç†çµ±è¨ˆä¿¡æ¯"""
        patterns = self.cleanup_patterns.get(mode, self.cleanup_patterns['dev_outputs'])
        
        stats = {
            'total_files': 0,
            'total_size_mb': 0.0,
            'oldest_file': None,
            'newest_file': None,
            'file_types': {}
        }
        
        oldest_time = float('inf')
        newest_time = 0
        
        for pattern in patterns:
            try:
                for file_path in glob.glob(pattern):
                    if os.path.isfile(file_path):
                        stats['total_files'] += 1
                        
                        # å¤§å°çµ±è¨ˆ
                        file_size = os.path.getsize(file_path)
                        stats['total_size_mb'] += file_size / 1024 / 1024
                        
                        # æ™‚é–“çµ±è¨ˆ
                        file_mtime = os.path.getmtime(file_path)
                        if file_mtime < oldest_time:
                            oldest_time = file_mtime
                            stats['oldest_file'] = file_path
                        if file_mtime > newest_time:
                            newest_time = file_mtime
                            stats['newest_file'] = file_path
                        
                        # æª”æ¡ˆé¡å‹çµ±è¨ˆ
                        file_ext = Path(file_path).suffix.lower()
                        stats['file_types'][file_ext] = stats['file_types'].get(file_ext, 0) + 1
                        
            except Exception as e:
                self.logger.warning(f"âš ï¸ çµ±è¨ˆå¤±æ•—: {pattern} - {e}")
        
        return stats
    
    def schedule_cleanup(self, interval_hours: int = 6, max_age_hours: int = 24) -> None:
        """èª¿åº¦å®šæœŸæ¸…ç†ï¼ˆé©åˆèˆ‡Cronæ•´åˆï¼‰"""
        self.logger.info(f"ğŸ“… èª¿åº¦æ¸…ç†: æ¯{interval_hours}å°æ™‚ï¼Œæ¸…ç†{max_age_hours}å°æ™‚å‰æª”æ¡ˆ")
        
        # é€™å€‹æ–¹æ³•ä¸»è¦ç”¨æ–¼ç”ŸæˆCroné…ç½®ï¼Œå¯¦éš›èª¿åº¦ç”±ç³»çµ±Cronå®Œæˆ
        cron_command = f"""
# LEOé‡æ§‹ç³»çµ±è‡ªå‹•æ¸…ç†
# æ¯{interval_hours}å°æ™‚æ¸…ç†ä¸€æ¬¡
0 */{interval_hours} * * * cd /home/sat/ntn-stack/leo_restructure && python -c "
from shared.auto_cleanup_manager import AutoCleanupManager
manager = AutoCleanupManager()
manager.cleanup_by_age({max_age_hours})
" >> data/logs/auto_cleanup.log 2>&1
"""
        
        self.logger.info(f"ğŸ“‹ å»ºè­°çš„Croné…ç½®:\n{cron_command}")


def create_auto_cleanup_manager(output_dir: Optional[str] = None) -> AutoCleanupManager:
    """å‰µå»ºè‡ªå‹•æ¸…ç†ç®¡ç†å™¨å¯¦ä¾‹"""
    return AutoCleanupManager(output_dir)


# å‘½ä»¤è¡Œæ¥å£
if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="LEOé‡æ§‹ç³»çµ±è‡ªå‹•æ¸…ç†ç®¡ç†å™¨")
    parser.add_argument('--mode', default='dev_outputs', choices=['dev_outputs', 'container_data', 'temp_cache', 'all'])
    parser.add_argument('--age-hours', type=int, default=24, help='æ¸…ç†å¤šå°‘å°æ™‚å‰çš„æª”æ¡ˆ')
    parser.add_argument('--max-size-mb', type=int, default=1024, help='æœ€å¤§ç¸½å¤§å°(MB)')
    parser.add_argument('--output-dir', default='data/processing_outputs', help='è¼¸å‡ºç›®éŒ„')
    parser.add_argument('--stats-only', action='store_true', help='åƒ…é¡¯ç¤ºçµ±è¨ˆä¿¡æ¯')
    parser.add_argument('--verbose', action='store_true', help='è©³ç´°è¼¸å‡º')
    
    args = parser.parse_args()
    
    # è¨­ç½®æ—¥èªŒ
    log_level = logging.DEBUG if args.verbose else logging.INFO
    logging.basicConfig(level=log_level, format='%(asctime)s - %(levelname)s - %(message)s')
    
    # å‰µå»ºç®¡ç†å™¨
    manager = create_auto_cleanup_manager(args.output_dir)
    
    if args.stats_only:
        # åƒ…é¡¯ç¤ºçµ±è¨ˆä¿¡æ¯
        print("ğŸ“Š æ¸…ç†çµ±è¨ˆä¿¡æ¯:")
        stats = manager.get_cleanup_stats(args.mode)
        print(f"  ç¸½æª”æ¡ˆæ•¸: {stats['total_files']}")
        print(f"  ç¸½å¤§å°: {stats['total_size_mb']:.1f}MB")
        print(f"  æœ€èˆŠæª”æ¡ˆ: {stats['oldest_file']}")
        print(f"  æœ€æ–°æª”æ¡ˆ: {stats['newest_file']}")
        print(f"  æª”æ¡ˆé¡å‹: {stats['file_types']}")
    else:
        # åŸ·è¡Œæ¸…ç†
        print(f"ğŸ§¹ é–‹å§‹è‡ªå‹•æ¸…ç† (æ¨¡å¼: {args.mode})...")
        cleaned_before = manager.cleanup_before_run(args.mode)
        cleaned_age = manager.cleanup_by_age(args.age_hours, args.mode)
        cleaned_size = manager.cleanup_by_size(args.max_size_mb, args.mode)
        
        total_cleaned = cleaned_before + cleaned_age + cleaned_size
        print(f"âœ… æ¸…ç†å®Œæˆï¼Œç¸½å…±æ¸…ç† {total_cleaned} å€‹æª”æ¡ˆ")