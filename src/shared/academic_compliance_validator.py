"""
å­¸è¡“ç´šæ•¸æ“šåˆè¦æ€§é©—è­‰å™¨
Academic Data Compliance Validator

é©—è­‰æ‰€æœ‰å…­å€‹éšæ®µæ˜¯å¦ç¬¦åˆå­¸è¡“ç´šæ•¸æ“šæ¨™æº–
æª¢æŸ¥æ˜¯å¦å­˜åœ¨ç¡¬ç·¨ç¢¼ã€éš¨æ©Ÿæ•¸ç”Ÿæˆç­‰é•è¦è¡Œç‚º
"""

import os
import re
import json
import logging
from pathlib import Path
from typing import Dict, Any, List, Tuple
from datetime import datetime, timezone

class AcademicComplianceValidator:
    """å­¸è¡“ç´šæ•¸æ“šåˆè¦æ€§é©—è­‰å™¨"""

    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.validation_results = {
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "total_files_scanned": 0,
            "total_violations": 0,  # ğŸ”§ ä¿®å¾©ï¼šæ·»åŠ ç¼ºå°‘çš„å­—æ®µ
            "overall_grade": "Unknown",  # ğŸ”§ ä¿®å¾©ï¼šæ·»åŠ æ•´é«”ç­‰ç´šå­—æ®µ
            "overall_compliance_score": 0.0,  # ğŸ”§ ä¿®å¾©ï¼šæ·»åŠ åˆè¦åˆ†æ•¸å­—æ®µ
            "compliance_summary": {
                "grade_a_files": 0,
                "grade_b_files": 0,
                "grade_c_violations": 0,
                "overall_compliance": "Unknown"
            },
            "stage_results": {},
            "violation_details": []
        }

        # Grade C ç¦æ­¢é …ç›®æ¨¡å¼ - ğŸ”§ ä¿®å¾©ï¼šæ”¹é€²mockæª¢æ¸¬é‚è¼¯
        self.prohibited_patterns = {
            "hardcoded_rsrp": {
                "patterns": [
                    r'rsrp.*-85\b', r'RSRP.*-85\b', r'= -85\b.*rsrp', r'= -85\b.*RSRP',
                    r'rsrp.*-88\b', r'RSRP.*-88\b', r'= -88\b.*rsrp', r'= -88\b.*RSRP',
                    r'rsrp.*-90\b', r'RSRP.*-90\b', r'= -90\b.*rsrp', r'= -90\b.*RSRP',
                    r'threshold.*-85\b', r'threshold.*-88\b', r'threshold.*-90\b',
                    r'-85.*dBm', r'-88.*dBm', r'-90.*dBm',
                    r'base_rsrp.*-85', r'base_rsrp.*-88', r'base_rsrp.*-90'
                ],
                "context": "RSRPç¡¬ç·¨ç¢¼å€¼",
                "severity": "Critical"
            },
            "hardcoded_elevation": {
                # ğŸ”§ ä¿®å¾©ï¼šæ›´ç²¾ç¢ºçš„ä»°è§’ç¡¬ç·¨ç¢¼æª¢æ¸¬ï¼Œæ’é™¤åˆç†çš„sentinelå€¼
                "patterns": [
                    r'elevation_threshold.*=.*\d+',     # ä»°è§’é–€æª»ç¡¬ç·¨ç¢¼
                    r'min_elevation.*=.*\d+',           # æœ€å°ä»°è§’ç¡¬ç·¨ç¢¼
                    r'max_elevation.*=.*\d+',           # æœ€å¤§ä»°è§’ç¡¬ç·¨ç¢¼
                    r'default_elevation.*=.*\d+',       # é è¨­ä»°è§’ç¡¬ç·¨ç¢¼
                    r'get\("elevation_deg",\s*-90\)'    # å­—å…¸å–å€¼ç¡¬ç·¨ç¢¼
                ],
                "context": "ä»°è§’ç¡¬ç·¨ç¢¼é è¨­å€¼",
                "severity": "High"
            },
            "random_generation": {
                "patterns": [r'random\.uniform', r'random\.choice', r'random\.sample', r'random\.randint', r'np\.random'],
                "context": "éš¨æ©Ÿæ•¸ç”Ÿæˆ",
                "severity": "Critical"
            },
            "mock_data": {
                # ğŸ”§ ä¿®å¾©ï¼šæ›´ç²¾ç¢ºçš„mockæª¢æ¸¬æ¨¡å¼ï¼Œæ’é™¤æª¢æ¸¬é‚è¼¯
                "patterns": [
                    r'MockRepository\(\)',      # Mocké¡å¯¦ä¾‹åŒ–
                    r'MockPrediction\(\)',      # Mocké¡å¯¦ä¾‹åŒ–
                    r'mock_repository\s*=',     # Mockå°è±¡è³¦å€¼
                    r'mock_prediction\s*=',     # Mockå°è±¡è³¦å€¼
                    r'å‡è¨­.*å€¼\s*=',            # ä¸­æ–‡å‡è¨­å€¼
                    r'æ¨¡æ“¬.*å€¼\s*='             # ä¸­æ–‡æ¨¡æ“¬å€¼
                ],
                "context": "æ¨¡æ“¬æ•¸æ“šä½¿ç”¨",
                "severity": "Critical"
            },
            "hardcoded_coordinates": {
                "patterns": [r'25\.0.*121\.5', r'longitude.*121\.5', r'latitude.*25\.0'],
                "context": "ç¡¬ç·¨ç¢¼åº§æ¨™",
                "severity": "Medium"
            }
        }

        # Grade A/B è‰¯å¥½å¯¦è¸æ¨¡å¼
        self.good_practices = {
            "standard_references": [
                "ITU-R", "3GPP", "IEEE", "SpaceX_Technical_Specs", "OneWeb_Technical_Specs"
            ],
            "physics_based": [
                "Friis", "path_loss", "atmospheric_attenuation", "SGP4", "orbital_mechanics"
            ],
            "academic_compliance": [
                "academic_standards_config", "elevation_standards"
            ]
        }

    def validate_all_stages(self, stages_dir: str = None) -> Dict[str, Any]:
        """é©—è­‰æ‰€æœ‰å…­å€‹éšæ®µçš„åˆè¦æ€§"""
        if stages_dir is None:
            stages_dir = "/home/sat/ntn-stack/orbit-engine-system/src/stages"

        stages_path = Path(stages_dir)
        if not stages_path.exists():
            self.logger.error(f"éšæ®µç›®éŒ„ä¸å­˜åœ¨: {stages_path}")
            return self.validation_results

        self.logger.info("ğŸ” é–‹å§‹å­¸è¡“ç´šæ•¸æ“šåˆè¦æ€§é©—è­‰...")

        # é©—è­‰æ¯å€‹éšæ®µ
        for stage_num in range(1, 7):
            stage_dir = stages_path / f"stage{stage_num}_*"
            stage_dirs = list(stages_path.glob(f"stage{stage_num}_*"))

            if stage_dirs:
                stage_path = stage_dirs[0]  # å–ç¬¬ä¸€å€‹åŒ¹é…çš„ç›®éŒ„
                self.logger.info(f"ğŸ” é©—è­‰ Stage {stage_num}: {stage_path.name}")
                stage_result = self._validate_stage(stage_path, stage_num)
                self.validation_results["stage_results"][f"stage_{stage_num}"] = stage_result
            else:
                self.logger.warning(f"æœªæ‰¾åˆ° Stage {stage_num} ç›®éŒ„")

        # è¨ˆç®—ç¸½é«”åˆè¦æ€§
        self._calculate_overall_compliance()

        self.logger.info("âœ… å­¸è¡“ç´šæ•¸æ“šåˆè¦æ€§é©—è­‰å®Œæˆ")
        return self.validation_results

    def _validate_stage(self, stage_path: Path, stage_num: int) -> Dict[str, Any]:
        """é©—è­‰å–®å€‹éšæ®µçš„åˆè¦æ€§"""
        stage_result = {
            "stage_number": stage_num,
            "stage_path": str(stage_path),
            "files_scanned": 0,
            "violations": [],
            "good_practices": [],
            "compliance_grade": "Unknown",
            "critical_issues": 0,
            "high_issues": 0,
            "medium_issues": 0
        }

        # æƒæPythonæ–‡ä»¶
        python_files = list(stage_path.glob("*.py"))
        for py_file in python_files:
            file_result = self._validate_file(py_file)
            stage_result["files_scanned"] += 1
            stage_result["violations"].extend(file_result["violations"])
            stage_result["good_practices"].extend(file_result["good_practices"])

        # çµ±è¨ˆå•é¡Œåš´é‡åº¦
        for violation in stage_result["violations"]:
            severity = violation.get("severity", "Medium")
            if severity == "Critical":
                stage_result["critical_issues"] += 1
            elif severity == "High":
                stage_result["high_issues"] += 1
            else:
                stage_result["medium_issues"] += 1

        # æ±ºå®šéšæ®µåˆè¦ç­‰ç´š
        stage_result["compliance_grade"] = self._determine_compliance_grade(stage_result)

        return stage_result

    def _validate_file(self, file_path: Path) -> Dict[str, Any]:
        """
        é©—è­‰å–®å€‹æ–‡ä»¶çš„åˆè¦æ€§
        ğŸ“ Grade Aå­¸è¡“æ¨™æº–ï¼šæ·±åº¦æª¢æŸ¥ç®—æ³•å¯¦ç¾ã€æ•¸æ“šä¾†æºã€æ™‚é–“åŸºæº–ä½¿ç”¨
        """
        file_result = {
            "file_path": str(file_path),
            "violations": [],
            "good_practices": [],
            "academic_grade": "A+",
            "critical_issues": [],
            "detailed_analysis": {}
        }

        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            self.validation_results["total_files_scanned"] += 1

            # ğŸš¨ Grade A é—œéµæª¢æŸ¥ï¼šæ™‚é–“åŸºæº–åˆè¦æ€§
            self._check_time_reference_compliance(content, file_path, file_result)
            
            # ğŸš¨ Grade A é—œéµæª¢æŸ¥ï¼šMockæ•¸æ“šæª¢æ¸¬
            self._check_mock_data_usage(content, file_path, file_result)
            
            # ğŸš¨ Grade A é—œéµæª¢æŸ¥ï¼šç°¡åŒ–ç®—æ³•æª¢æ¸¬
            self._check_simplified_algorithms(content, file_path, file_result)
            
            # ğŸš¨ Grade A é—œéµæª¢æŸ¥ï¼šç¡¬ç·¨ç¢¼åƒæ•¸æª¢æ¸¬
            self._check_hardcoded_parameters(content, file_path, file_result)
            
            # ğŸš¨ Grade A é—œéµæª¢æŸ¥ï¼šæ•¸æ“šä¾†æºé©—è­‰
            self._check_data_source_compliance(content, file_path, file_result)

            # æª¢æŸ¥åŸæœ‰ç¦æ­¢é …ç›®
            for violation_type, config in self.prohibited_patterns.items():
                for pattern in config["patterns"]:
                    matches = re.finditer(pattern, content, re.IGNORECASE)
                    for match in matches:
                        line_num = content[:match.start()].count('\n') + 1
                        violation = {
                            "type": violation_type,
                            "file": file_path.name,
                            "line": line_num,
                            "context": config["context"],
                            "severity": config["severity"],
                            "matched_text": match.group(),
                            "description": f"åœ¨æ–‡ä»¶ {file_path.name}:{line_num} ç™¼ç¾{config['context']}"
                        }
                        file_result["violations"].append(violation)
                        self.validation_results["violation_details"].append(violation)

            # æª¢æŸ¥è‰¯å¥½å¯¦è¸
            for practice_type, keywords in self.good_practices.items():
                for keyword in keywords:
                    if keyword in content:
                        file_result["good_practices"].append({
                            "type": practice_type,
                            "keyword": keyword,
                            "file": file_path.name
                        })
            
            # è¨ˆç®—æ–‡ä»¶çš„å­¸è¡“ç­‰ç´š
            file_result["academic_grade"] = self._calculate_file_academic_grade(file_result)

        except Exception as e:
            self.logger.warning(f"ç„¡æ³•è®€å–æ–‡ä»¶ {file_path}: {e}")

        return file_result

    def _check_time_reference_compliance(self, content: str, file_path: Path, file_result: Dict) -> None:
        """
        æª¢æŸ¥æ™‚é–“åŸºæº–åˆè¦æ€§
        ğŸ“ Grade Aæ¨™æº–ï¼šç¦æ­¢ä½¿ç”¨ç•¶å‰æ™‚é–“ä½œç‚ºè»Œé“è¨ˆç®—åŸºæº–
        """
        time_violations = []
        
        # æª¢æŸ¥datetime.now()çš„ä½¿ç”¨
        datetime_now_pattern = r'datetime\.now\s*\(\s*\)'
        matches = re.finditer(datetime_now_pattern, content)
        for match in matches:
            line_num = content[:match.start()].count('\n') + 1
            context_lines = content.split('\n')[max(0, line_num-2):line_num+1]
            
            # æª¢æŸ¥æ˜¯å¦ç”¨æ–¼è»Œé“è¨ˆç®—æˆ–æ•¸æ“šè©•ä¼°
            context_text = '\n'.join(context_lines).lower()
            if any(keyword in context_text for keyword in ['age_days', 'epoch', 'orbital', 'calculation', 'freshness', 'quality']):
                time_violations.append({
                    "type": "time_reference_violation",
                    "severity": "CRITICAL",
                    "line": line_num,
                    "matched_text": match.group(),
                    "context": "ä½¿ç”¨ç•¶å‰æ™‚é–“é€²è¡Œè»Œé“è¨ˆç®—æˆ–æ•¸æ“šè©•ä¼°",
                    "description": f"âŒ åœ¨ {file_path.name}:{line_num} ç™¼ç¾ä½¿ç”¨datetime.now()é€²è¡Œè»Œé“ç›¸é—œè¨ˆç®—",
                    "fix_suggestion": "ä½¿ç”¨TLE epochæ™‚é–“ä½œç‚ºè¨ˆç®—åŸºæº–"
                })
        
        if time_violations:
            file_result["critical_issues"].extend(time_violations)
            file_result["violations"].extend(time_violations)

    def _check_mock_data_usage(self, content: str, file_path: Path, file_result: Dict) -> None:
        """
        æª¢æŸ¥Mockæ•¸æ“šä½¿ç”¨
        ğŸ“ Grade Aæ¨™æº–ï¼šç¦æ­¢ä½¿ç”¨æ¨¡æ“¬æˆ–å‡æ•¸æ“šï¼Œä½†å…è¨±æª¢æ¸¬å’Œé˜²æ­¢mockæ•¸æ“š
        """
        mock_violations = []
        
        # æª¢æŸ¥çœŸæ­£çš„Mockæ•¸æ“šä½¿ç”¨æ¨¡å¼
        mock_usage_patterns = [
            r'class\s+Mock\w+.*:',  # Mocké¡å®šç¾©
            r'def\s+mock_\w+.*:',   # Mockå‡½æ•¸å®šç¾©
            r'MockPrediction\(\)',  # Mocké¡å¯¦ä¾‹åŒ–
            r'MockRepository\(\)',  # Mocké¡å¯¦ä¾‹åŒ–
            r'fake_data\s*=',       # å‡æ•¸æ“šè³¦å€¼
            r'simulated_data\s*=',  # æ¨¡æ“¬æ•¸æ“šè³¦å€¼
            r'random\.normal\(',    # éš¨æ©Ÿæ•¸æ“šç”Ÿæˆ
            r'np\.random\.',        # NumPyéš¨æ©Ÿæ•¸æ“š
            r'å‡è¨­.*=\s*\d+',       # ä¸­æ–‡å‡è¨­è³¦å€¼ (æ•´æ•¸)
            # ğŸ”§ ä¿®å¾©ï¼šç§»é™¤éæ–¼å¯¬æ³›çš„estimatedæ¨¡å¼ï¼Œé¿å…èª¤å ±ç²¾åº¦å¸¸æ•¸
            # r'estimated.*=\s*\d',   # é€™æœƒèª¤å ±estimated_accuracy = 1e-6
            r'assumed.*=\s*\d+'     # å‡å®šå€¼è³¦å€¼ (æ•´æ•¸)
        ]
        
        # æª¢æ¸¬é‚è¼¯çš„è±å…æ¨¡å¼ (é€™äº›æ˜¯å…è¨±çš„ï¼Œå› ç‚ºæ˜¯åœ¨æª¢æ¸¬mockæ•¸æ“š)
        detection_patterns = [
            r'mock_data_count',     # è¨ˆç®—mockæ•¸æ“šæ•¸é‡
            r'check.*mock',         # æª¢æŸ¥mock
            r'detect.*mock',        # æª¢æ¸¬mock
            r'validate.*mock',      # é©—è­‰mock
            r'if.*mock.*in',        # æª¢æŸ¥æ˜¯å¦åŒ…å«mock
            r'mock.*ratio',         # mockæ¯”ä¾‹è¨ˆç®—
            r'authentic_data_ratio' # çœŸå¯¦æ•¸æ“šæ¯”ä¾‹
        ]
        
        lines = content.split('\n')
        
        for pattern in mock_usage_patterns:
            matches = re.finditer(pattern, content, re.IGNORECASE)
            for match in matches:
                line_num = content[:match.start()].count('\n') + 1
                line_content = lines[line_num - 1] if line_num <= len(lines) else ""
                
                # æª¢æŸ¥æ˜¯å¦æ˜¯æª¢æ¸¬é‚è¼¯ï¼ˆè±å…ï¼‰
                is_detection_logic = any(
                    re.search(exempt_pattern, line_content, re.IGNORECASE) 
                    for exempt_pattern in detection_patterns
                )
                
                # æª¢æŸ¥ä¸Šä¸‹æ–‡æ˜¯å¦æ˜¯é©—è­‰æˆ–æª¢æ¸¬ç›¸é—œ
                context_lines = lines[max(0, line_num-3):min(len(lines), line_num+3)]
                context_text = ' '.join(context_lines).lower()
                is_validation_context = any(keyword in context_text for keyword in [
                    'validate', 'check', 'detect', 'verify', 'authenticity', 
                    'compliance', 'validation', 'é©—è­‰', 'æª¢æŸ¥', 'æª¢æ¸¬'
                ])
                
                # åªæœ‰åœ¨éæª¢æ¸¬é‚è¼¯ä¸”éé©—è­‰ä¸Šä¸‹æ–‡æ™‚æ‰å ±å‘Šé•å
                if not is_detection_logic and not is_validation_context:
                    mock_violations.append({
                        "type": "mock_data_violation",
                        "severity": "CRITICAL",
                        "line": line_num,
                        "matched_text": match.group(),
                        "context": "ä½¿ç”¨æ¨¡æ“¬æˆ–å‡æ•¸æ“š",
                        "description": f"âŒ åœ¨ {file_path.name}:{line_num} ç™¼ç¾Mockæ•¸æ“šä½¿ç”¨",
                        "fix_suggestion": "ä½¿ç”¨çœŸå¯¦æ•¸æ“šæºï¼ˆTLEã€3GPPæ¨™æº–ã€ITU-Ræ¨¡å‹ï¼‰"
                    })
        
        if mock_violations:
            file_result["critical_issues"].extend(mock_violations)
            file_result["violations"].extend(mock_violations)

    def _check_simplified_algorithms(self, content: str, file_path: Path, file_result: Dict) -> None:
        """
        æª¢æŸ¥ç°¡åŒ–ç®—æ³•ä½¿ç”¨
        ğŸ“ Grade Aæ¨™æº–ï¼šç¦æ­¢ä½¿ç”¨ç°¡åŒ–æˆ–è¿‘ä¼¼ç®—æ³•
        """
        simplified_violations = []
        
        # æª¢æŸ¥ç°¡åŒ–ç®—æ³•é—œéµè©
        simplified_patterns = [
            r'ä½¿ç”¨.*ç°¡åŒ–.*ç®—æ³•',         # å¯¦éš›ä½¿ç”¨ç°¡åŒ–ç®—æ³•
            r'æ¡ç”¨.*ç°¡åŒ–.*ç®—æ³•',         # æ¡ç”¨ç°¡åŒ–ç®—æ³•
            r'implemented.*simplified.*algorithm',  # è‹±æ–‡ï¼šå¯¦ç¾äº†ç°¡åŒ–ç®—æ³•
            r'using.*simplified.*algorithm',       # è‹±æ–‡ï¼šä½¿ç”¨ç°¡åŒ–ç®—æ³•
            r'approximate.*calculation',
            r'rough.*estimate',
            r'åŸºæœ¬æ¨¡å‹',
            r'basic.*model',
            r'linear.*approximation',
            r'å‡è¨­.*ç‚ºå¸¸æ•¸',
            r'å›ºå®š.*å€¼'
        ]
        
        # è±å…æ¨¡å¼ï¼šé€™äº›æ˜¯èªªæ˜æ–‡æª”æˆ–ç¦æ­¢æ¢æ¬¾ï¼Œä¸æ˜¯å¯¦éš›ä½¿ç”¨
        exemption_patterns = [
            r'ç¦æ­¢.*ç°¡åŒ–.*ç®—æ³•',         # ç¦æ­¢ä½¿ç”¨ç°¡åŒ–ç®—æ³•
            r'âŒ.*ç°¡åŒ–.*ç®—æ³•',          # âŒ ç¦æ­¢ä»»ä½•ç°¡åŒ–ç®—æ³•
            r'ä¸å¾—.*ç°¡åŒ–.*ç®—æ³•',         # ä¸å¾—ä½¿ç”¨ç°¡åŒ–ç®—æ³•
            r'avoid.*simplified.*algorithm',  # é¿å…ç°¡åŒ–ç®—æ³•
            r'prohibit.*simplified.*algorithm', # ç¦æ­¢ç°¡åŒ–ç®—æ³•
            r'forbidden.*models',        # ç¦æ­¢æ¨¡å‹åˆ—è¡¨
            r'ç¦ç”¨çš„.*æ¨¡å¼',             # ç¦ç”¨çš„æ¨¡å¼åˆ—è¡¨
            r'".*approximation"',        # å­—ç¬¦ä¸²å­—é¢å€¼å®šç¾©
            r'\[.*approximation.*\]'     # åˆ—è¡¨ä¸­çš„é …ç›®
        ]
        
        lines = content.split('\n')
        
        for pattern in simplified_patterns:
            matches = re.finditer(pattern, content, re.IGNORECASE)
            for match in matches:
                line_num = content[:match.start()].count('\n') + 1
                line_content = lines[line_num - 1] if line_num <= len(lines) else ""
                
                # æª¢æŸ¥æ˜¯å¦æ˜¯è±å…æƒ…æ³ï¼ˆæ–‡æª”èªªæ˜æˆ–ç¦æ­¢æ¢æ¬¾ï¼‰
                is_exemption = any(
                    re.search(exempt_pattern, line_content, re.IGNORECASE) 
                    for exempt_pattern in exemption_patterns
                )
                
                # åªæœ‰åœ¨éè±å…æƒ…æ³ä¸‹æ‰å ±å‘Šé•å
                if not is_exemption:
                    simplified_violations.append({
                        "type": "simplified_algorithm_violation",
                        "severity": "HIGH",
                        "line": line_num,
                        "matched_text": match.group(),
                        "context": "ä½¿ç”¨ç°¡åŒ–ç®—æ³•",
                        "description": f"âš ï¸ åœ¨ {file_path.name}:{line_num} ç™¼ç¾ç°¡åŒ–ç®—æ³•",
                        "fix_suggestion": "ä½¿ç”¨å®Œæ•´çš„æ¨™æº–ç®—æ³•å¯¦ç¾ï¼ˆSGP4ã€ITU-Rã€3GPPï¼‰"
                    })
        
        if simplified_violations:
            file_result["violations"].extend(simplified_violations)

    def _check_hardcoded_parameters(self, content: str, file_path: Path, file_result: Dict) -> None:
        """
        æª¢æŸ¥ç¡¬ç·¨ç¢¼åƒæ•¸
        ğŸ“ Grade Aæ¨™æº–ï¼šåƒæ•¸æ‡‰ä¾†è‡ªå®˜æ–¹æ¨™æº–æˆ–é…ç½®æ–‡ä»¶
        """
        hardcoded_violations = []
        
        # æª¢æŸ¥å¯ç–‘çš„ç¡¬ç·¨ç¢¼æ•¸å€¼
        hardcoded_patterns = [
            r'= -\d+\.?\d*\s*#.*dBm',  # ç¡¬ç·¨ç¢¼çš„åŠŸç‡å€¼
            r'= \d+\.?\d*\s*#.*elevation',  # ç¡¬ç·¨ç¢¼çš„ä»°è§’
            r'= \d+\.?\d*\s*#.*frequency',  # ç¡¬ç·¨ç¢¼çš„é »ç‡
            r'é è¨­å€¼.*=',
            r'default.*= \d',
            r'å›ºå®š.*= \d'
        ]
        
        for pattern in hardcoded_patterns:
            matches = re.finditer(pattern, content, re.IGNORECASE)
            for match in matches:
                line_num = content[:match.start()].count('\n') + 1
                
                # æ’é™¤é…ç½®æ–‡ä»¶ä¸­çš„åˆç†é è¨­å€¼
                if not any(keyword in file_path.name.lower() for keyword in ['config', 'settings', 'constants']):
                    hardcoded_violations.append({
                        "type": "hardcoded_parameter_violation",
                        "severity": "MEDIUM",
                        "line": line_num,
                        "matched_text": match.group(),
                        "context": "ä½¿ç”¨ç¡¬ç·¨ç¢¼åƒæ•¸",
                        "description": f"âš ï¸ åœ¨ {file_path.name}:{line_num} ç™¼ç¾ç¡¬ç·¨ç¢¼åƒæ•¸",
                        "fix_suggestion": "å¾é…ç½®æ–‡ä»¶æˆ–æ¨™æº–æ–‡æª”ä¸­è¼‰å…¥åƒæ•¸"
                    })
        
        if hardcoded_violations:
            file_result["violations"].extend(hardcoded_violations)

    def _check_data_source_compliance(self, content: str, file_path: Path, file_result: Dict) -> None:
        """
        æª¢æŸ¥æ•¸æ“šä¾†æºåˆè¦æ€§
        ğŸ“ Grade Aæ¨™æº–ï¼šå¿…é ˆä½¿ç”¨å®˜æ–¹æˆ–æ¬Šå¨æ•¸æ“šæº
        """
        data_source_issues = []
        
        # æª¢æŸ¥è‰¯å¥½çš„æ•¸æ“šæº
        good_sources = [
            'space-track.org', 'itu-r', '3gpp', 'ieee', 'skyfield',
            'sgp4', 'noaa', 'ntp.org', 'celestrak', 'norad'
        ]
        
        # æª¢æŸ¥å¯ç–‘çš„æ•¸æ“šæº
        suspicious_sources = [
            'random', 'fake', 'mock', 'test_data', 'dummy',
            'éš¨æ©Ÿ', 'æ¨¡æ“¬', 'æ¸¬è©¦æ•¸æ“š'
        ]
        
        good_source_found = any(source in content.lower() for source in good_sources)
        suspicious_source_found = any(source in content.lower() for source in suspicious_sources)
        
        if suspicious_source_found and not good_source_found:
            data_source_issues.append({
                "type": "data_source_violation",
                "severity": "HIGH",
                "line": 0,
                "matched_text": "suspicious data sources",
                "context": "ä½¿ç”¨å¯ç–‘æ•¸æ“šæº",
                "description": f"âš ï¸ {file_path.name} å¯èƒ½ä½¿ç”¨éå®˜æ–¹æ•¸æ“šæº",
                "fix_suggestion": "ä½¿ç”¨å®˜æ–¹æ•¸æ“šæºï¼ˆSpace-Trackã€ITU-Rã€3GPPç­‰ï¼‰"
            })
        
        if data_source_issues:
            file_result["violations"].extend(data_source_issues)

    def _calculate_file_academic_grade(self, file_result: Dict) -> str:
        """
        è¨ˆç®—æ–‡ä»¶çš„å­¸è¡“ç­‰ç´š
        ğŸ“ åŸºæ–¼é•åé¡å‹å’Œåš´é‡ç¨‹åº¦è©•åˆ†
        """
        critical_count = len(file_result["critical_issues"])
        total_violations = len(file_result["violations"])
        
        if critical_count > 0:
            return "F"  # æœ‰é—œéµå•é¡Œç›´æ¥ä¸åˆæ ¼
        elif total_violations == 0:
            return "A+"
        elif total_violations <= 2:
            return "A"
        elif total_violations <= 5:
            return "B"
        else:
            return "C"

    def _determine_compliance_grade(self, stage_result: Dict[str, Any]) -> str:
        """ç¢ºå®šéšæ®µçš„åˆè¦ç­‰ç´š"""
        critical_issues = stage_result["critical_issues"]
        high_issues = stage_result["high_issues"]
        medium_issues = stage_result["medium_issues"]
        good_practices = len(stage_result["good_practices"])

        # Critical å•é¡Œç›´æ¥å®šç‚º Grade C
        if critical_issues > 0:
            return "C"

        # High å•é¡Œè¼ƒå¤šä¹Ÿæ˜¯ Grade C
        if high_issues > 2:
            return "C"

        # ğŸš¨ ä¿®å¾©ï¼šæ·»åŠ  Grade A è©•åˆ¤é‚è¼¯
        # Grade A: ç„¡ä»»ä½•å•é¡Œ + è±å¯Œçš„è‰¯å¥½å¯¦è¸
        if critical_issues == 0 and high_issues == 0 and medium_issues == 0 and good_practices >= 2:
            return "A"

        # Grade B: ç„¡Critical/Highå•é¡Œ + æœ‰è‰¯å¥½å¯¦è¸
        if critical_issues == 0 and high_issues == 0 and good_practices > 0:
            return "B"

        # ä¸­ç­‰å•é¡Œä½†æœ‰è‰¯å¥½å¯¦è¸å¯èƒ½æ˜¯ Grade B
        if medium_issues > 0 and good_practices > 0 and critical_issues == 0 and high_issues == 0:
            return "B"

        # å…¶ä»–æƒ…æ³
        return "C"

    def _calculate_overall_compliance(self):
        """
        è¨ˆç®—ç¸½é«”åˆè¦æ€§
        ğŸ“ Grade A å­¸è¡“æ¨™æº–ï¼šåŸºæ–¼é—œéµå•é¡Œæ•¸é‡å’Œæ•´é«”é•åæƒ…æ³è©•ä¼°
        """
        total_critical = 0
        total_high = 0
        total_medium = 0
        grade_counts = {"A+": 0, "A": 0, "B": 0, "C": 0, "F": 0}
        
        # è¨ˆç®—ç¸½é•åæ•¸
        self.validation_results["total_violations"] = len(self.validation_results["violation_details"])
        
        # çµ±è¨ˆå„éšæ®µçµæœ
        stage_count = 0
        for stage_name, stage_result in self.validation_results["stage_results"].items():
            # è™•ç†ä¸åŒçš„æ•¸æ“šçµæ§‹
            if isinstance(stage_result, dict):
                stage_count += 1
                critical_issues = stage_result.get("critical_issues", 0)
                high_issues = stage_result.get("high_issues", 0) 
                medium_issues = stage_result.get("medium_issues", 0)
                
                # å¦‚æœæ˜¯åˆ—è¡¨å½¢å¼çš„critical_issues
                if isinstance(critical_issues, list):
                    critical_issues = len(critical_issues)
                if isinstance(high_issues, list):
                    high_issues = len(high_issues)
                if isinstance(medium_issues, list):
                    medium_issues = len(medium_issues)
                
                total_critical += critical_issues
                total_high += high_issues
                total_medium += medium_issues

                grade = stage_result.get("compliance_grade", "C")
                if grade in grade_counts:
                    grade_counts[grade] += 1
                else:
                    grade_counts["C"] += 1

        # æ›´æ–°åˆè¦æ€§æ‘˜è¦
        self.validation_results["compliance_summary"].update({
            "grade_a_files": grade_counts["A"] + grade_counts["A+"],
            "grade_b_files": grade_counts["B"], 
            "grade_c_violations": grade_counts["C"],
            "grade_f_violations": grade_counts["F"],
            "total_critical_issues": total_critical,
            "total_high_issues": total_high,
            "total_medium_issues": total_medium
        })

        # ğŸ“ è¨ˆç®—æ•´é«”å­¸è¡“ç­‰ç´š - ä¿®å¾©å–®éšæ®µæª¢æŸ¥é‚è¼¯
        if total_critical > 0:
            overall_grade = "F"  # æœ‰é—œéµå•é¡Œç›´æ¥ä¸åˆæ ¼
            compliance_score = max(0, 60 - total_critical * 10)
        elif grade_counts["F"] > 0:
            overall_grade = "F"
            compliance_score = 50
        elif total_high > 5:
            overall_grade = "C"
            compliance_score = max(60, 80 - total_high * 3)
        elif stage_count == 1:  # ğŸ”§ ä¿®å¾©ï¼šå–®éšæ®µæª¢æŸ¥é‚è¼¯
            # å°æ–¼å–®éšæ®µæª¢æŸ¥ï¼Œç›´æ¥åŸºæ–¼è©²éšæ®µçš„è¡¨ç¾
            if grade_counts["A+"] > 0:
                overall_grade = "A+"
                compliance_score = max(95, 100 - total_medium)
            elif grade_counts["A"] > 0:
                overall_grade = "A"
                compliance_score = max(90, 95 - total_medium)
            elif grade_counts["B"] > 0:
                overall_grade = "B"
                compliance_score = max(80, 90 - total_high * 2 - total_medium)
            else:
                overall_grade = "C"
                compliance_score = 65
        elif grade_counts["C"] > stage_count / 2:  # è¶…éä¸€åŠéšæ®µCç­‰ç´š
            overall_grade = "C"
            compliance_score = 70
        elif grade_counts["A"] + grade_counts["A+"] >= stage_count * 0.7:  # 70%ä»¥ä¸Šéšæ®µé”åˆ°Aç´š
            overall_grade = "A"
            compliance_score = max(85, 100 - total_high - total_medium)
        elif grade_counts["A"] + grade_counts["A+"] + grade_counts["B"] >= stage_count * 0.8:  # 80%ä»¥ä¸Šéšæ®µé”åˆ°A/Bç´š
            overall_grade = "B"
            compliance_score = max(75, 90 - total_high * 2 - total_medium)
        else:
            overall_grade = "C"
            compliance_score = 65

        # æ›´æ–°çµæœ
        self.validation_results["overall_grade"] = overall_grade
        self.validation_results["overall_compliance_score"] = compliance_score
        self.validation_results["compliance_summary"]["overall_compliance"] = overall_grade

        self.logger.info(f"ğŸ“Š å­¸è¡“æ¨™æº–è©•ä¼°å®Œæˆ: {overall_grade} ({compliance_score:.1f}/100)")
        self.logger.info(f"ğŸš¨ é—œéµå•é¡Œ: {total_critical}, âš ï¸ é«˜é¢¨éšª: {total_high}, ğŸ“ ä¸­ç­‰: {total_medium}")

    def generate_compliance_report(self) -> str:
        """ç”Ÿæˆåˆè¦æ€§å ±å‘Š"""
        report = []
        report.append("# å­¸è¡“ç´šæ•¸æ“šåˆè¦æ€§é©—è­‰å ±å‘Š")
        report.append("# Academic Data Compliance Validation Report")
        report.append("")
        report.append(f"**é©—è­‰æ™‚é–“**: {self.validation_results['timestamp']}")
        report.append(f"**æƒææ–‡ä»¶æ•¸**: {self.validation_results['total_files_scanned']}")
        report.append("")

        # ç¸½é«”åˆè¦æ€§
        summary = self.validation_results["compliance_summary"]
        report.append("## ç¸½é«”åˆè¦æ€§æ‘˜è¦")
        report.append(f"- **ç¸½é«”åˆè¦ç­‰ç´š**: {summary['overall_compliance']}")
        report.append(f"- Grade A æª”æ¡ˆ: {summary['grade_a_files']}")
        report.append(f"- Grade B æª”æ¡ˆ: {summary['grade_b_files']}")
        report.append(f"- Grade C é•è¦: {summary['grade_c_violations']}")
        report.append(f"- åš´é‡å•é¡Œç¸½æ•¸: {summary.get('total_critical_issues', 0)}")
        report.append("")

        # å„éšæ®µè©³æƒ…
        report.append("## å„éšæ®µåˆè¦æ€§è©³æƒ…")
        for stage_name, stage_result in self.validation_results["stage_results"].items():
            report.append(f"### {stage_name.upper()}")
            report.append(f"- **åˆè¦ç­‰ç´š**: {stage_result['compliance_grade']}")
            report.append(f"- **æƒææ–‡ä»¶æ•¸**: {stage_result['files_scanned']}")
            report.append(f"- **åš´é‡å•é¡Œ**: {stage_result['critical_issues']}")
            report.append(f"- **é«˜ç´šå•é¡Œ**: {stage_result['high_issues']}")
            report.append(f"- **ä¸­ç­‰å•é¡Œ**: {stage_result['medium_issues']}")
            report.append("")

        # ä¸»è¦é•è¦è©³æƒ…
        if self.validation_results["violation_details"]:
            report.append("## ä¸»è¦é•è¦è©³æƒ…")
            for violation in self.validation_results["violation_details"][:20]:  # åªé¡¯ç¤ºå‰20å€‹
                report.append(f"- **{violation['severity']}**: {violation['description']}")
                report.append(f"  - æ–‡ä»¶: {violation['file']}")
                report.append(f"  - è¡Œè™Ÿ: {violation['line']}")
                report.append(f"  - åŒ¹é…æ–‡æœ¬: `{violation['matched_text']}`")
                report.append("")

        return "\n".join(report)

    def save_validation_results(self, output_path: str = None):
        """ä¿å­˜é©—è­‰çµæœ"""
        if output_path is None:
            output_path = "/home/sat/ntn-stack/orbit-engine-system/academic_compliance_validation_report.json"

        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(self.validation_results, f, indent=2, ensure_ascii=False)

            self.logger.info(f"é©—è­‰çµæœå·²ä¿å­˜è‡³: {output_path}")

            # åŒæ™‚ä¿å­˜å ±å‘Š
            report_path = output_path.replace('.json', '.md')
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write(self.generate_compliance_report())

            self.logger.info(f"åˆè¦æ€§å ±å‘Šå·²ä¿å­˜è‡³: {report_path}")

        except Exception as e:
            self.logger.error(f"ä¿å­˜é©—è­‰çµæœå¤±æ•—: {e}")

# å…¨åŸŸå¯¦ä¾‹
COMPLIANCE_VALIDATOR = AcademicComplianceValidator()