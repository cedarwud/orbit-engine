# ML è¨“ç·´æ•¸æ“šç”Ÿæˆå™¨è¦æ ¼ - Stage 6 æ ¸å¿ƒçµ„ä»¶

**æª”æ¡ˆ**: `src/stages/stage6_research_optimization/ml_training_data_generator.py`
**ä¾æ“š**: `docs/stages/stage6-research-optimization.md` Line 91-96, 597-623
**ç›®æ¨™**: ç”Ÿæˆ 50,000+ é«˜å“è³ª ML è¨“ç·´æ¨£æœ¬

---

## ğŸ¯ æ ¸å¿ƒè·è²¬

ç”Ÿæˆç”¨æ–¼å¼·åŒ–å­¸ç¿’æ›æ‰‹å„ªåŒ–çš„è¨“ç·´æ•¸æ“šï¼š
1. **ç‹€æ…‹ç©ºé–“æ§‹å»º**: 7ç¶­+ è¡›æ˜Ÿç‹€æ…‹å‘é‡
2. **å‹•ä½œç©ºé–“ç·¨ç¢¼**: æ›æ‰‹æ±ºç­– (ä¿æŒ/åˆ‡æ›è‡³å€™é¸1/2/3...)
3. **çå‹µå‡½æ•¸è¨ˆç®—**: åŸºæ–¼ QoSã€ä¸­æ–·æ™‚é–“ã€ä¿¡è™Ÿå“è³ªçš„è¤‡åˆçå‹µ
4. **ç¶“é©—å›æ”¾**: å¤§é‡çœŸå¯¦æ›æ‰‹å ´æ™¯å­˜å„²ä¾›ç®—æ³•å­¸ç¿’

æ”¯æ´ç®—æ³•ï¼š
- **DQN** (Deep Q-Network)
- **A3C** (Asynchronous Advantage Actor-Critic)
- **PPO** (Proximal Policy Optimization)
- **SAC** (Soft Actor-Critic)

---

## ğŸ—ï¸ é¡åˆ¥è¨­è¨ˆ

```python
class MLTrainingDataGenerator:
    """ML è¨“ç·´æ•¸æ“šç”Ÿæˆå™¨"""

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """åˆå§‹åŒ–ç”Ÿæˆå™¨

        Args:
            config: é…ç½®åƒæ•¸
                - state_space_dimensions: ç‹€æ…‹ç©ºé–“ç¶­åº¦ (é è¨­ 7)
                - action_space_size: å‹•ä½œç©ºé–“å¤§å° (é è¨­ 5)
                - reward_function_type: çå‹µå‡½æ•¸é¡å‹
                - experience_replay_size: ç¶“é©—å›æ”¾ç·©è¡å€å¤§å°
        """
        self.config = self._load_config(config)
        self.logger = logging.getLogger(__name__)

        # è¨“ç·´æ¨£æœ¬ç·©è¡å€
        self.training_buffer = {
            'dqn_samples': [],
            'a3c_samples': [],
            'ppo_samples': [],
            'sac_samples': []
        }

        # çµ±è¨ˆ
        self.generation_stats = {
            'total_samples': 0,
            'dqn_samples': 0,
            'a3c_samples': 0,
            'ppo_samples': 0,
            'sac_samples': 0
        }

    def generate_all_training_data(
        self,
        signal_analysis: Dict[str, Any],
        gpp_events: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ç”Ÿæˆæ‰€æœ‰ç®—æ³•çš„è¨“ç·´æ•¸æ“š

        Args:
            signal_analysis: Stage 5 ä¿¡è™Ÿåˆ†ææ•¸æ“š
            gpp_events: 3GPP äº‹ä»¶æ•¸æ“š

        Returns:
            {
                'dqn_dataset': {...},
                'a3c_dataset': {...},
                'ppo_dataset': {...},
                'sac_dataset': {...},
                'dataset_summary': {...}
            }
        """
        pass

    def generate_dqn_dataset(
        self,
        signal_analysis: Dict[str, Any],
        gpp_events: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ç”Ÿæˆ DQN è¨“ç·´æ•¸æ“šé›†

        Returns:
            {
                'state_vectors': List[List[float]],  # (N, 7) ç‹€æ…‹å‘é‡
                'action_space': List[str],            # å‹•ä½œåç¨±
                'q_values': List[List[float]],        # (N, action_space_size) Qå€¼
                'reward_values': List[float],         # (N,) çå‹µå€¼
                'next_state_vectors': List[List[float]], # (N, 7) ä¸‹ä¸€ç‹€æ…‹
                'done_flags': List[bool],             # (N,) çµ‚æ­¢æ¨™è¨˜
                'dataset_size': int
            }
        """
        pass

    def generate_a3c_dataset(
        self,
        signal_analysis: Dict[str, Any],
        gpp_events: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ç”Ÿæˆ A3C è¨“ç·´æ•¸æ“šé›†

        Returns:
            {
                'state_vectors': List[List[float]],
                'action_probs': List[List[float]],    # ç­–ç•¥æ¦‚ç‡
                'value_estimates': List[float],        # åƒ¹å€¼ä¼°è¨ˆ
                'advantage_functions': List[float],    # å„ªå‹¢å‡½æ•¸
                'policy_gradients': List[List[float]], # ç­–ç•¥æ¢¯åº¦
                'dataset_size': int
            }
        """
        pass

    def generate_ppo_dataset(
        self,
        signal_analysis: Dict[str, Any],
        gpp_events: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ç”Ÿæˆ PPO è¨“ç·´æ•¸æ“šé›†

        Returns:
            {
                'state_vectors': List[List[float]],
                'actions_taken': List[int],            # å¯¦éš›å‹•ä½œ
                'old_policy_probs': List[float],       # èˆŠç­–ç•¥æ¦‚ç‡
                'new_policy_probs': List[float],       # æ–°ç­–ç•¥æ¦‚ç‡
                'advantages': List[float],             # å„ªå‹¢å‡½æ•¸
                'returns': List[float],                # å›å ±
                'clipped_ratios': List[float],         # è£å‰ªæ¯”ç‡
                'dataset_size': int
            }
        """
        pass

    def generate_sac_dataset(
        self,
        signal_analysis: Dict[str, Any],
        gpp_events: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ç”Ÿæˆ SAC è¨“ç·´æ•¸æ“šé›†

        Returns:
            {
                'state_vectors': List[List[float]],
                'continuous_actions': List[List[float]], # é€£çºŒå‹•ä½œ
                'rewards': List[float],
                'next_state_vectors': List[List[float]],
                'done_flags': List[bool],
                'soft_q_values': List[List[float]],    # è»Ÿ Q å€¼
                'policy_entropy': List[float],          # ç­–ç•¥ç†µ
                'dataset_size': int
            }
        """
        pass

    def build_state_vector(
        self,
        satellite_data: Dict[str, Any]
    ) -> List[float]:
        """æ§‹å»ºç‹€æ…‹å‘é‡ (7ç¶­æ¨™æº–)

        Args:
            satellite_data: å–®é¡†è¡›æ˜Ÿçš„ä¿¡è™Ÿåˆ†ææ•¸æ“š

        Returns:
            [latitude, longitude, altitude, rsrp, elevation, distance, sinr]
        """
        state_vector = [
            satellite_data['current_position']['latitude_deg'],
            satellite_data['current_position']['longitude_deg'],
            satellite_data['current_position']['altitude_km'],
            satellite_data['signal_quality']['rsrp_dbm'],
            satellite_data['visibility_metrics']['elevation_deg'],
            satellite_data['physical_parameters']['distance_km'],
            satellite_data['signal_quality']['rs_sinr_db']
        ]

        return state_vector

    def encode_action(
        self,
        action_type: str,
        candidate_satellite_id: Optional[str] = None
    ) -> Tuple[int, List[int]]:
        """ç·¨ç¢¼å‹•ä½œ

        Args:
            action_type: 'maintain' æˆ– 'handover'
            candidate_satellite_id: ç›®æ¨™è¡›æ˜Ÿ ID (æ›æ‰‹æ™‚)

        Returns:
            (action_index, one_hot_encoding)
        """
        pass

    def calculate_reward(
        self,
        current_state: Dict[str, Any],
        action: str,
        next_state: Dict[str, Any]
    ) -> Tuple[float, Dict[str, float]]:
        """è¨ˆç®—è¤‡åˆçå‹µå‡½æ•¸

        çå‹µçµ„æˆ:
        1. QoS æ”¹å–„çå‹µ (+0.0 ~ +1.0)
        2. ä¸­æ–·æ‡²ç½° (-0.5 ~ 0.0)
        3. ä¿¡è™Ÿå“è³ªçå‹µ (+0.0 ~ +1.0)
        4. æ›æ‰‹æˆæœ¬æ‡²ç½° (-0.1 ~ 0.0)

        Returns:
            (total_reward, reward_components)
        """
        reward_components = {}

        # 1. QoS æ”¹å–„
        current_rsrp = current_state['signal_quality']['rsrp_dbm']
        next_rsrp = next_state['signal_quality']['rsrp_dbm']
        rsrp_improvement = (next_rsrp - current_rsrp) / 20.0  # æ¨™æº–åŒ–åˆ° [-1, 1]
        reward_components['qos_improvement'] = max(0, rsrp_improvement)

        # 2. ä¸­æ–·æ‡²ç½°
        if next_state['quality_assessment']['is_usable'] == False:
            reward_components['interruption_penalty'] = -0.5
        else:
            reward_components['interruption_penalty'] = 0.0

        # 3. ä¿¡è™Ÿå“è³ªçå‹µ
        quality_score = next_state['quality_assessment']['quality_score']
        reward_components['signal_quality_gain'] = quality_score

        # 4. æ›æ‰‹æˆæœ¬
        if action == 'handover':
            reward_components['handover_cost'] = -0.1
        else:
            reward_components['handover_cost'] = 0.0

        total_reward = sum(reward_components.values())

        return total_reward, reward_components

    def _load_config(self, config: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        """è¼‰å…¥ä¸¦åˆä½µé…ç½®åƒæ•¸"""
        default_config = {
            'state_space_dimensions': 7,
            'action_space_size': 5,
            'reward_function_type': 'composite_qos_interruption_quality',
            'experience_replay_size': 100000,
            'batch_size': 256,
            'learning_rate': 0.001,
            'discount_factor': 0.99
        }

        if config:
            default_config.update(config)

        return default_config
```

---

## ğŸ“Š è¼¸å‡ºæ•¸æ“šæ ¼å¼

### DQN æ•¸æ“šé›†ç¯„ä¾‹
```python
{
    'state_vectors': [
        [25.1234, 121.5678, 550.123, -85.2, 15.5, 750.2, 12.8],  # è¡›æ˜Ÿ1ç‹€æ…‹
        [25.2345, 121.6789, 555.234, -88.1, 18.3, 820.5, 10.2],  # è¡›æ˜Ÿ2ç‹€æ…‹
        # ... 50,000+ æ¨£æœ¬
    ],
    'action_space': ['maintain', 'handover_to_candidate_1', 'handover_to_candidate_2',
                     'handover_to_candidate_3', 'handover_to_candidate_4'],
    'q_values': [
        [0.89, -0.12, 0.76, 0.45, 0.23],  # 5å€‹å‹•ä½œçš„Qå€¼
        [0.92, -0.08, 0.71, 0.38, 0.15],
        # ... å°æ‡‰æ¯å€‹ç‹€æ…‹
    ],
    'reward_values': [0.89, 0.76, 0.45, ...],  # å¯¦éš›çå‹µ
    'next_state_vectors': [
        [25.1235, 121.5679, 550.124, -84.8, 15.6, 748.5, 13.1],
        # ... ä¸‹ä¸€ç‹€æ…‹
    ],
    'done_flags': [False, False, True, ...],  # çµ‚æ­¢æ¨™è¨˜
    'dataset_size': 50000
}
```

### ML è¨“ç·´æ¨£æœ¬è©³ç´°æ ¼å¼
```python
{
    'sample_id': 'SAMPLE_1695024000789',
    'timestamp': '2025-09-27T08:00:00.789123+00:00',
    'state_vector': {
        'serving_satellite': {
            'latitude_deg': 25.1234,
            'longitude_deg': 121.5678,
            'altitude_km': 550.123,
            'rsrp_dbm': -85.2,
            'elevation_deg': 15.5,
            'distance_km': 750.2,
            'sinr_db': 12.8
        },
        'candidate_satellites': [
            # å€™é¸è¡›æ˜Ÿç‹€æ…‹å‘é‡
        ],
        'system_state': {
            'current_qos': 0.89,
            'handover_count_last_hour': 3,
            'coverage_gap_risk': 0.12
        }
    },
    'action_taken': 'handover_to_candidate_1',
    'action_encoding': [0, 1, 0, 0, 0],  # one-hot encoding
    'reward_received': 0.76,
    'reward_components': {
        'qos_improvement': 0.15,
        'interruption_penalty': -0.02,
        'signal_quality_gain': 0.63,
        'handover_cost': 0.0
    },
    'next_state_vector': {
        # ä¸‹ä¸€ç‹€æ…‹å‘é‡
    },
    'algorithm_metadata': {
        'dqn_q_value': 0.76,
        'a3c_policy_prob': 0.83,
        'ppo_advantage': 0.21,
        'sac_entropy': 0.67
    }
}
```

---

## âœ… å¯¦ç¾æª¢æŸ¥æ¸…å–®

### åŠŸèƒ½å®Œæ•´æ€§
- [ ] DQN æ•¸æ“šé›†ç”Ÿæˆ
- [ ] A3C æ•¸æ“šé›†ç”Ÿæˆ
- [ ] PPO æ•¸æ“šé›†ç”Ÿæˆ
- [ ] SAC æ•¸æ“šé›†ç”Ÿæˆ
- [ ] ç‹€æ…‹å‘é‡æ§‹å»º (7ç¶­)
- [ ] å‹•ä½œç·¨ç¢¼ (one-hot)
- [ ] è¤‡åˆçå‹µå‡½æ•¸è¨ˆç®—
- [ ] ç¶“é©—å›æ”¾ç·©è¡å€ç®¡ç†

### æ•¸æ“šå“è³ª
- [ ] 50,000+ æ¨£æœ¬ç”Ÿæˆèƒ½åŠ›
- [ ] ç‹€æ…‹å‘é‡å®Œæ•´æ€§æª¢æŸ¥
- [ ] å‹•ä½œç©ºé–“åˆç†æ€§é©—è­‰
- [ ] çå‹µå‡½æ•¸è¨­è¨ˆæ­£ç¢ºæ€§
- [ ] æ•¸æ“šæ¨™æº–åŒ–è™•ç†

### ç®—æ³•æ”¯æ´
- [ ] DQN æ ¼å¼æ­£ç¢º
- [ ] A3C æ ¼å¼æ­£ç¢º
- [ ] PPO æ ¼å¼æ­£ç¢º
- [ ] SAC æ ¼å¼æ­£ç¢º
- [ ] è·¨ç®—æ³•æ•¸æ“šä¸€è‡´æ€§

### æ€§èƒ½è¦æ±‚
- [ ] < 0.1ç§’ ç”Ÿæˆ 1000 æ¨£æœ¬
- [ ] è¨˜æ†¶é«”æ•ˆç‡å„ªåŒ–
- [ ] å¤§è¦æ¨¡æ•¸æ“šé›†æ”¯æ´

### å–®å…ƒæ¸¬è©¦
- [ ] ç‹€æ…‹å‘é‡æ§‹å»ºæ¸¬è©¦
- [ ] å‹•ä½œç·¨ç¢¼æ¸¬è©¦
- [ ] çå‹µå‡½æ•¸æ¸¬è©¦
- [ ] å„ç®—æ³•æ•¸æ“šé›†æ¸¬è©¦
- [ ] é‚Šç•Œæ¢ä»¶æ¸¬è©¦

---

## ğŸ“š å­¸è¡“åƒè€ƒ

1. Mnih, V., et al. (2015). Human-level control through deep reinforcement learning. *Nature*, 518(7540), 529-533.

2. Mnih, V., et al. (2016). Asynchronous methods for deep reinforcement learning. *ICML*.

3. Schulman, J., et al. (2017). Proximal policy optimization algorithms. *arXiv preprint arXiv:1707.06347*.

4. Haarnoja, T., et al. (2018). Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. *ICML*.

---

**è¦æ ¼ç‰ˆæœ¬**: v1.0
**å‰µå»ºæ—¥æœŸ**: 2025-09-30
**ç‹€æ…‹**: å¾…å¯¦ç¾