# P2: HDF5 å„²å­˜ç­–ç•¥çµ±ä¸€

**å„ªå…ˆç´š**: ğŸŸ  P2 - ä¸­ç­‰å„ªå…ˆç´šï¼ˆæ‡‰è©²åŸ·è¡Œï¼‰
**é ä¼°æ™‚é–“**: 0.5å¤©
**ä¾è³´**: Phase 3 å®Œæˆï¼ˆBaseResultManager å·²å»ºç«‹ï¼‰
**å½±éŸ¿ç¯„åœ**: Stage 2ï¼ˆé›™æ ¼å¼è¼¸å‡ºï¼‰, Stage 3ï¼ˆç·©å­˜ç­–ç•¥ï¼‰

---

## ğŸ“Š å•é¡Œåˆ†æ

### ç•¶å‰ç‹€æ…‹

**HDF5 ä½¿ç”¨ç¾æ³**:

| Stage | HDF5ç”¨é€” | å¯¦ä½œæ–¹å¼ | æ•¸æ“šè¦æ¨¡ | å•é¡Œ |
|-------|---------|---------|---------|------|
| **Stage 2** | é›™æ ¼å¼è¼¸å‡ºï¼ˆJSON + HDF5ï¼‰ | `save_results(output_format='both')` | ~1.7M åæ¨™é» | å¯¦ä½œèˆ‡å…¶ä»–éšæ®µä¸ä¸€è‡´ |
| **Stage 3** | åæ¨™ç·©å­˜ï¼ˆåŠ é€Ÿé‡ç®—ï¼‰ | `Stage3HDF5Cache` é¡åˆ¥ | 154MB ç·©å­˜æª” | ç·©å­˜é‚è¼¯æ•£è½åœ¨å¤šè™• |
| **Stage 1,4,5,6** | ä¸ä½¿ç”¨ HDF5 | åƒ… JSON è¼¸å‡º | - | - |

---

### Stage 2: é›™æ ¼å¼è¼¸å‡ºæ¨¡å¼

**æª”æ¡ˆä½ç½®**: `src/stages/stage2_orbital_computing/stage2_result_manager.py`

**å¯¦ä½œæ–¹å¼**:

```python
def save_results(self, results, output_format='json', custom_filename=None):
    """
    è¦†å¯«åŸºé¡ save_results() æ”¯æ´ HDF5 è¼¸å‡º

    Args:
        output_format: 'json' | 'hdf5' | 'both'
    """
    # Step 1: å…ˆå‘¼å«åŸºé¡ä¿å­˜ JSON
    json_path = super().save_results(results, 'json', custom_filename)

    # Step 2: å¦‚æœéœ€è¦ HDF5ï¼Œé¡å¤–ä¿å­˜
    if output_format in ['hdf5', 'both']:
        hdf5_path = self._save_hdf5(results, custom_filename)

        if output_format == 'hdf5':
            return hdf5_path
        else:  # 'both'
            return json_path, hdf5_path

    return json_path
```

**è¨­è¨ˆè€ƒé‡**:
- âœ… **å„ªé»**: å®Œå…¨å‘å¾Œç›¸å®¹ï¼ˆé è¨­ä»æ˜¯ JSONï¼‰
- âœ… **å„ªé»**: HDF5 ä½œç‚ºå¯é¸è¼¸å‡ºæ ¼å¼
- âš ï¸ **å•é¡Œ**: å¯¦ä½œé‚è¼¯èˆ‡ BaseResultManager è¨­è¨ˆæ¨¡å¼ä¸ä¸€è‡´ï¼ˆè¦†å¯« template methodï¼‰

---

### Stage 3: HDF5 ç·©å­˜æ¨¡å¼

**æª”æ¡ˆä½ç½®**: `src/stages/stage3_coordinate_transformation/stage3_hdf5_cache.py`

**å¯¦ä½œæ–¹å¼**:

```python
class Stage3HDF5Cache:
    """
    Stage 3 åæ¨™è½‰æ› HDF5 ç·©å­˜ç®¡ç†å™¨

    åŠŸèƒ½:
    1. æª¢æŸ¥ç·©å­˜æ˜¯å¦å­˜åœ¨ä¸”æœ‰æ•ˆï¼ˆåŸºæ–¼ Stage 2 è¼¸å‡ºæ™‚é–“æˆ³ï¼‰
    2. å¾ HDF5 ç·©å­˜è®€å–åæ¨™æ•¸æ“š
    3. å°‡æ–°è¨ˆç®—çš„åæ¨™æ•¸æ“šå¯«å…¥ HDF5 ç·©å­˜
    """

    def __init__(self, cache_dir='data/cache/stage3'):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)

    def is_cache_valid(self, stage2_timestamp):
        """
        æª¢æŸ¥ç·©å­˜æ˜¯å¦æœ‰æ•ˆ

        é‚è¼¯: HDF5 æª”æ¡ˆä¿®æ”¹æ™‚é–“ > Stage 2 è¼¸å‡ºæª”æ¡ˆä¿®æ”¹æ™‚é–“
        """
        cache_file = self.cache_dir / 'coordinate_cache.h5'
        if not cache_file.exists():
            return False

        cache_mtime = cache_file.stat().st_mtime
        return cache_mtime > stage2_timestamp

    def load_from_cache(self, satellite_id):
        """å¾ HDF5 ç·©å­˜è®€å–å–®å€‹è¡›æ˜Ÿçš„åæ¨™æ•¸æ“š"""
        with h5py.File(self.cache_dir / 'coordinate_cache.h5', 'r') as f:
            if satellite_id in f:
                return f[satellite_id][:]
            return None

    def save_to_cache(self, satellite_data_dict):
        """å°‡æ‰€æœ‰è¡›æ˜Ÿåæ¨™æ•¸æ“šå¯«å…¥ HDF5 ç·©å­˜"""
        with h5py.File(self.cache_dir / 'coordinate_cache.h5', 'w') as f:
            for sat_id, coord_array in satellite_data_dict.items():
                f.create_dataset(sat_id, data=coord_array, compression='gzip')
```

**ä½¿ç”¨å ´æ™¯**:

```python
# Stage 3 Processor ä¸­çš„ä½¿ç”¨

cache = Stage3HDF5Cache()

# æª¢æŸ¥ç·©å­˜
if cache.is_cache_valid(stage2_output_timestamp):
    logger.info("âœ… ä½¿ç”¨ HDF5 ç·©å­˜ï¼ˆè·³éåæ¨™è½‰æ›è¨ˆç®—ï¼‰")
    for sat_id in satellite_ids:
        coordinates = cache.load_from_cache(sat_id)
        results[sat_id] = coordinates
else:
    logger.info("ğŸ”„ åŸ·è¡Œå®Œæ•´åæ¨™è½‰æ›ï¼ˆç„¡æœ‰æ•ˆç·©å­˜ï¼‰")
    for sat_id in satellite_ids:
        coordinates = transform_coordinates(sat_id)
        results[sat_id] = coordinates

    # è¨ˆç®—å®Œæˆå¾Œä¿å­˜ç·©å­˜
    cache.save_to_cache(results)
```

**è¨­è¨ˆè€ƒé‡**:
- âœ… **å„ªé»**: é¡¯è‘—åŠ é€Ÿ Stage 3 é‡ç®—ï¼ˆ25min â†’ 2minï¼‰
- âœ… **å„ªé»**: åŸºæ–¼æ™‚é–“æˆ³çš„æ™ºèƒ½ç·©å­˜å¤±æ•ˆæª¢æ¸¬
- âš ï¸ **å•é¡Œ**: ç·©å­˜é‚è¼¯èˆ‡ ResultManager è§£è€¦ï¼ˆç¨ç«‹é¡åˆ¥ï¼‰
- âš ï¸ **å•é¡Œ**: ç·©å­˜æœ‰æ•ˆæ€§æª¢æŸ¥æ•£è½åœ¨ Processor ä¸­

---

### æ ¸å¿ƒå•é¡Œ

**1. å¯¦ä½œæ¨¡å¼ä¸çµ±ä¸€**:
- Stage 2: HDF5 é‚è¼¯åµŒå…¥ ResultManager
- Stage 3: HDF5 é‚è¼¯ç¨ç«‹ç‚º Cache é¡åˆ¥
- å…©ç¨®æ¨¡å¼ç¼ºå°‘çµ±ä¸€æŠ½è±¡

**2. ä»£ç¢¼é‡è¤‡**:
```python
# Stage 2 å’Œ Stage 3 éƒ½æœ‰é¡ä¼¼çš„ HDF5 å¯«å…¥é‚è¼¯
with h5py.File(output_path, 'w') as f:
    for sat_id, data in satellite_data.items():
        f.create_dataset(sat_id, data=data, compression='gzip')
```

**3. ç·©å­˜ç­–ç•¥ä¸æ˜ç¢º**:
- ä½•æ™‚æ‡‰è©²ä½¿ç”¨ HDF5ï¼Ÿï¼ˆæ•¸æ“šè¦æ¨¡ > 1GBï¼Ÿï¼‰
- ä½•æ™‚æ‡‰è©²å•Ÿç”¨ç·©å­˜ï¼Ÿï¼ˆè¨ˆç®—æ™‚é–“ > 10minï¼Ÿï¼‰
- ç¼ºå°‘çµ±ä¸€çš„æ±ºç­–æŒ‡å¼•

**4. æ¸¬è©¦è¦†è“‹ä¸è¶³**:
- Stage 2 HDF5 è¼¸å‡ºï¼šæœ‰åŸºæœ¬æ¸¬è©¦
- Stage 3 HDF5 ç·©å­˜ï¼šç¼ºå°‘å–®å…ƒæ¸¬è©¦ï¼ˆåƒ…é›†æˆæ¸¬è©¦ï¼‰

---

## ğŸ¯ è¨­è¨ˆç›®æ¨™

### ä¸»è¦ç›®æ¨™

1. **æŠ½å–å…±åŒæ¨¡å¼** - å»ºç«‹ `BaseHDF5Handler` çµ±ä¸€ HDF5 æ“ä½œ
2. **ç­–ç•¥æ¨¡å¼åˆ†é›¢** - HDF5 è¼¸å‡º vs ç·©å­˜ä½¿ç”¨ä¸åŒç­–ç•¥
3. **ä¿æŒç¾æœ‰å„ªå‹¢** - Stage 2 é›™æ ¼å¼è¼¸å‡ºã€Stage 3 å¿«é€Ÿç·©å­˜ä¸å—å½±éŸ¿
4. **å‘å¾Œç›¸å®¹** - æ‰€æœ‰ç¾æœ‰æ¥å£ä¿æŒä¸è®Š
5. **æ¸¬è©¦è¦†è“‹** - è£œå…… HDF5 ç›¸é—œå–®å…ƒæ¸¬è©¦

### æˆåŠŸæŒ‡æ¨™

- âœ… HDF5 æ“ä½œä»£ç¢¼æ¸›å°‘ ~50è¡Œï¼ˆæŠ½å–å…±åŒé‚è¼¯ï¼‰
- âœ… Stage 2/3 ä½¿ç”¨çµ±ä¸€ HDF5 handler
- âœ… HDF5 ç›¸é—œå–®å…ƒæ¸¬è©¦è¦†è“‹ç‡ â‰¥ 80%
- âœ… æ€§èƒ½ç„¡é€€åŒ–ï¼ˆStage 3 ç·©å­˜åŠ é€Ÿ â‰¥ 90%ï¼‰
- âœ… å‘å¾Œç›¸å®¹æ€§ 100%

---

## ğŸ—ï¸ è¨­è¨ˆæ–¹æ¡ˆ

### Strategy Pattern æ‡‰ç”¨

**æ ¸å¿ƒè¨­è¨ˆ**: ä½¿ç”¨ Strategy Pattern åˆ†é›¢ã€ŒHDF5 è¼¸å‡ºã€å’Œã€ŒHDF5 ç·©å­˜ã€å…©ç¨®å ´æ™¯

```
BaseHDF5Handler (æŠ½è±¡åŸºé¡)
â”œâ”€â”€ HDF5OutputStrategy (Stage 2: é›™æ ¼å¼è¼¸å‡º)
â””â”€â”€ HDF5CacheStrategy (Stage 3: ç·©å­˜åŠ é€Ÿ)
```

---

## ğŸ’» å¯¦ä½œç´°ç¯€

### 1. BaseHDF5Handler åŸºé¡

**æª”æ¡ˆä½ç½®**: `src/shared/hdf5_handler.py` (æ–°å»º)

```python
"""
ğŸ’¾ HDF5 Handler - Strategy Pattern for HDF5 Operations

Unified HDF5 handling for output and caching strategies.
Eliminates code duplication between Stage 2 (output) and Stage 3 (cache).

Author: Orbit Engine Refactoring Team
Date: 2025-10-15 (Phase 4 Refactoring)

Design Philosophy:
-----------------
1. **Strategy Pattern**: Separate output vs cache strategies
2. **Template Method**: Common HDF5 operations in base class
3. **Fail-Fast**: Validate HDF5 data integrity
4. **Performance**: gzip compression for large datasets

Usage Example:
--------------
# Stage 2: Output strategy
handler = HDF5OutputStrategy(output_dir='data/outputs/stage2')
handler.save(satellite_data, filename='stage2_orbital_propagation.h5')

# Stage 3: Cache strategy
cache = HDF5CacheStrategy(cache_dir='data/cache/stage3')
if cache.is_valid(upstream_timestamp):
    data = cache.load(satellite_id)
else:
    data = compute_coordinates()
    cache.save({satellite_id: data})
"""

import logging
from abc import ABC, abstractmethod
from pathlib import Path
from typing import Dict, Any, Optional, Union
import h5py
import numpy as np


class BaseHDF5Handler(ABC):
    """
    Base class for HDF5 operations

    Provides common HDF5 read/write operations with compression and validation.
    Subclasses implement specific strategies (output vs cache).

    Template Methods (implemented here):
    ------------------------------------
    - save(): Save data to HDF5 file
    - load(): Load data from HDF5 file
    - _write_hdf5_dataset(): Write single dataset with compression
    - _read_hdf5_dataset(): Read single dataset

    Abstract Methods (subclass must implement):
    -------------------------------------------
    - get_hdf5_path(): Return HDF5 file path for given context
    - is_valid(): Check if HDF5 data is valid (for cache strategy)
    """

    def __init__(
        self,
        base_dir: Union[str, Path],
        compression: str = 'gzip',
        compression_opts: int = 4,
        logger_instance: Optional[logging.Logger] = None
    ):
        """
        Initialize HDF5 handler

        Args:
            base_dir: Base directory for HDF5 files
            compression: Compression algorithm ('gzip', 'lzf', or None)
            compression_opts: Compression level (1-9 for gzip)
            logger_instance: Optional logger instance
        """
        self.base_dir = Path(base_dir)
        self.base_dir.mkdir(parents=True, exist_ok=True)

        self.compression = compression
        self.compression_opts = compression_opts

        self.logger = logger_instance or logging.getLogger(
            f"{__name__}.{self.__class__.__name__}"
        )

    # ==================== Abstract Methods ====================

    @abstractmethod
    def get_hdf5_path(self, identifier: str) -> Path:
        """
        Get HDF5 file path for given identifier

        Args:
            identifier: Context-specific identifier (filename or cache key)

        Returns:
            HDF5 file path

        Example (Output Strategy):
            identifier='stage2_output_20251015_120000'
            returns: Path('data/outputs/stage2/stage2_output_20251015_120000.h5')

        Example (Cache Strategy):
            identifier='coordinate_cache'
            returns: Path('data/cache/stage3/coordinate_cache.h5')
        """
        pass

    # ==================== Template Methods ====================

    def save(
        self,
        data: Dict[str, Union[np.ndarray, Dict[str, Any]]],
        identifier: str,
        metadata: Optional[Dict[str, Any]] = None
    ) -> Path:
        """
        Template method: Save data to HDF5 file

        Standard workflow:
        1. Get HDF5 file path from subclass
        2. Write all datasets with compression
        3. Optionally write metadata as attributes
        4. Validate written data
        5. Log success

        Args:
            data: Dictionary mapping dataset names to numpy arrays
            identifier: Context-specific identifier for file path
            metadata: Optional metadata to store as HDF5 attributes

        Returns:
            Path to saved HDF5 file

        Example:
            handler.save(
                data={'SAT-12345': coordinates_array},
                identifier='stage2_output_20251015_120000',
                metadata={'stage': 2, 'timestamp': '...'}
            )
        """
        hdf5_path = self.get_hdf5_path(identifier)

        try:
            with h5py.File(hdf5_path, 'w') as f:
                # Write datasets
                for dataset_name, dataset_data in data.items():
                    self._write_hdf5_dataset(f, dataset_name, dataset_data)

                # Write metadata as root attributes
                if metadata:
                    for key, value in metadata.items():
                        f.attrs[key] = str(value)  # Convert to string for safety

            # Validate written data
            if not self._validate_hdf5_file(hdf5_path, list(data.keys())):
                raise IOError(f"HDF5 validation failed: {hdf5_path}")

            file_size_mb = hdf5_path.stat().st_size / (1024 * 1024)
            self.logger.info(
                f"ğŸ’¾ HDF5 æ–‡ä»¶å·²ä¿å­˜: {hdf5_path} ({file_size_mb:.1f} MB)"
            )

            return hdf5_path

        except Exception as e:
            self.logger.error(f"âŒ HDF5 ä¿å­˜å¤±æ•—: {hdf5_path} - {e}")
            raise IOError(f"ç„¡æ³•ä¿å­˜ HDF5 æ–‡ä»¶: {e}")

    def load(
        self,
        identifier: str,
        dataset_name: Optional[str] = None
    ) -> Union[Dict[str, np.ndarray], np.ndarray]:
        """
        Template method: Load data from HDF5 file

        Args:
            identifier: Context-specific identifier for file path
            dataset_name: Optional specific dataset name to load
                         If None, loads all datasets

        Returns:
            - If dataset_name provided: numpy array
            - If dataset_name is None: dict mapping names to arrays

        Example:
            # Load all datasets
            all_data = handler.load('stage2_output_20251015_120000')

            # Load specific satellite
            sat_data = handler.load('coordinate_cache', 'SAT-12345')
        """
        hdf5_path = self.get_hdf5_path(identifier)

        if not hdf5_path.exists():
            raise FileNotFoundError(f"HDF5 æ–‡ä»¶ä¸å­˜åœ¨: {hdf5_path}")

        try:
            with h5py.File(hdf5_path, 'r') as f:
                if dataset_name:
                    # Load specific dataset
                    if dataset_name not in f:
                        raise KeyError(f"Dataset '{dataset_name}' not found in {hdf5_path}")
                    return self._read_hdf5_dataset(f, dataset_name)
                else:
                    # Load all datasets
                    data = {}
                    for name in f.keys():
                        data[name] = self._read_hdf5_dataset(f, name)
                    return data

        except Exception as e:
            self.logger.error(f"âŒ HDF5 è®€å–å¤±æ•—: {hdf5_path} - {e}")
            raise IOError(f"ç„¡æ³•è®€å– HDF5 æ–‡ä»¶: {e}")

    def exists(self, identifier: str) -> bool:
        """
        Check if HDF5 file exists

        Args:
            identifier: Context-specific identifier

        Returns:
            True if file exists
        """
        return self.get_hdf5_path(identifier).exists()

    def get_metadata(self, identifier: str) -> Dict[str, Any]:
        """
        Load metadata from HDF5 file attributes

        Args:
            identifier: Context-specific identifier

        Returns:
            Metadata dictionary
        """
        hdf5_path = self.get_hdf5_path(identifier)

        if not hdf5_path.exists():
            return {}

        try:
            with h5py.File(hdf5_path, 'r') as f:
                return dict(f.attrs)
        except Exception as e:
            self.logger.warning(f"âš ï¸ ç„¡æ³•è®€å– HDF5 metadata: {e}")
            return {}

    # ==================== Helper Methods ====================

    def _write_hdf5_dataset(
        self,
        hdf5_file: h5py.File,
        dataset_name: str,
        data: np.ndarray
    ) -> None:
        """
        Write single dataset with compression

        Args:
            hdf5_file: Opened HDF5 file handle
            dataset_name: Dataset name
            data: Numpy array data
        """
        if not isinstance(data, np.ndarray):
            data = np.array(data)

        hdf5_file.create_dataset(
            dataset_name,
            data=data,
            compression=self.compression,
            compression_opts=self.compression_opts
        )

    def _read_hdf5_dataset(
        self,
        hdf5_file: h5py.File,
        dataset_name: str
    ) -> np.ndarray:
        """
        Read single dataset

        Args:
            hdf5_file: Opened HDF5 file handle
            dataset_name: Dataset name

        Returns:
            Numpy array data
        """
        return hdf5_file[dataset_name][:]

    def _validate_hdf5_file(
        self,
        hdf5_path: Path,
        expected_datasets: list
    ) -> bool:
        """
        Validate HDF5 file integrity

        Args:
            hdf5_path: HDF5 file path
            expected_datasets: List of expected dataset names

        Returns:
            True if valid
        """
        try:
            with h5py.File(hdf5_path, 'r') as f:
                for dataset_name in expected_datasets:
                    if dataset_name not in f:
                        self.logger.error(f"âŒ Dataset '{dataset_name}' ç¼ºå¤±")
                        return False
            return True
        except Exception as e:
            self.logger.error(f"âŒ HDF5 é©—è­‰å¤±æ•—: {e}")
            return False


# ==================== Strategy 1: Output Strategy ====================

class HDF5OutputStrategy(BaseHDF5Handler):
    """
    HDF5 Output Strategy - For dual-format output (JSON + HDF5)

    Used by Stage 2 to save orbital propagation results in both formats.

    Usage Example:
        handler = HDF5OutputStrategy(output_dir='data/outputs/stage2')

        # Save orbital propagation results
        hdf5_path = handler.save(
            data={'SAT-12345': teme_coordinates_array},
            identifier='stage2_orbital_propagation_20251015_120000',
            metadata={'stage': 2, 'satellite_count': 9015}
        )
    """

    def get_hdf5_path(self, identifier: str) -> Path:
        """
        Get HDF5 output file path

        Args:
            identifier: Filename without extension

        Returns:
            Full HDF5 file path

        Example:
            identifier='stage2_output_20251015_120000'
            returns: Path('data/outputs/stage2/stage2_output_20251015_120000.h5')
        """
        if not identifier.endswith('.h5'):
            identifier = f"{identifier}.h5"

        return self.base_dir / identifier


# ==================== Strategy 2: Cache Strategy ====================

class HDF5CacheStrategy(BaseHDF5Handler):
    """
    HDF5 Cache Strategy - For computation result caching

    Used by Stage 3 to cache coordinate transformation results.
    Provides cache validation based on upstream timestamp.

    Usage Example:
        cache = HDF5CacheStrategy(cache_dir='data/cache/stage3')

        # Check cache validity
        if cache.is_valid('coordinate_cache', upstream_timestamp):
            coordinates = cache.load('coordinate_cache', 'SAT-12345')
        else:
            coordinates = transform_coordinates()
            cache.save(
                data={'SAT-12345': coordinates},
                identifier='coordinate_cache',
                metadata={'upstream_timestamp': upstream_timestamp}
            )
    """

    def get_hdf5_path(self, identifier: str) -> Path:
        """
        Get HDF5 cache file path

        Args:
            identifier: Cache identifier (typically 'coordinate_cache')

        Returns:
            Full HDF5 cache file path

        Example:
            identifier='coordinate_cache'
            returns: Path('data/cache/stage3/coordinate_cache.h5')
        """
        if not identifier.endswith('.h5'):
            identifier = f"{identifier}.h5"

        return self.base_dir / identifier

    def is_valid(self, identifier: str, upstream_timestamp: float) -> bool:
        """
        Check if cache is valid based on upstream timestamp

        Cache is valid if:
        1. Cache file exists
        2. Cache file modification time > upstream timestamp

        Args:
            identifier: Cache identifier
            upstream_timestamp: Upstream file timestamp (Unix epoch)

        Returns:
            True if cache is valid and can be used

        Example:
            stage2_timestamp = Path('data/outputs/stage2/output.json').stat().st_mtime
            if cache.is_valid('coordinate_cache', stage2_timestamp):
                # Use cache
            else:
                # Recompute
        """
        cache_path = self.get_hdf5_path(identifier)

        if not cache_path.exists():
            self.logger.info(f"ğŸ”„ HDF5 ç·©å­˜ä¸å­˜åœ¨: {cache_path}")
            return False

        cache_mtime = cache_path.stat().st_mtime

        if cache_mtime > upstream_timestamp:
            self.logger.info(f"âœ… HDF5 ç·©å­˜æœ‰æ•ˆ: {cache_path}")
            return True
        else:
            self.logger.info(
                f"ğŸ”„ HDF5 ç·©å­˜éæœŸ: cache_mtime={cache_mtime}, "
                f"upstream_timestamp={upstream_timestamp}"
            )
            return False

    def invalidate(self, identifier: str) -> bool:
        """
        Invalidate (delete) cache file

        Args:
            identifier: Cache identifier

        Returns:
            True if cache was deleted
        """
        cache_path = self.get_hdf5_path(identifier)

        if cache_path.exists():
            cache_path.unlink()
            self.logger.info(f"ğŸ—‘ï¸ HDF5 ç·©å­˜å·²æ¸…é™¤: {cache_path}")
            return True

        return False
```

---

### 2. Stage 2 é·ç§»

**æª”æ¡ˆ**: `src/stages/stage2_orbital_computing/stage2_result_manager.py`

**ä¿®æ”¹å‰**:
```python
def _save_hdf5(self, results, custom_filename=None):
    """è‡ªå®šç¾© HDF5 ä¿å­˜é‚è¼¯"""
    # ç´„ 40 è¡Œ HDF5 æ“ä½œä»£ç¢¼...
    with h5py.File(hdf5_path, 'w') as f:
        for sat_id, coords in satellite_coords.items():
            f.create_dataset(sat_id, data=coords, compression='gzip')
    # ...
```

**ä¿®æ”¹å¾Œ**:
```python
from shared.hdf5_handler import HDF5OutputStrategy

class Stage2ResultManager(BaseResultManager):
    def __init__(self, logger_instance=None):
        super().__init__(logger_instance)
        # åˆå§‹åŒ– HDF5 handler
        self.hdf5_handler = HDF5OutputStrategy(
            output_dir='data/outputs/stage2',
            compression='gzip',
            compression_opts=4,
            logger_instance=self.logger
        )

    def save_results(self, results, output_format='json', custom_filename=None):
        """è¦†å¯«åŸºé¡æ”¯æ´é›™æ ¼å¼è¼¸å‡º"""
        # Step 1: ä¿å­˜ JSONï¼ˆå‘¼å«åŸºé¡ï¼‰
        json_path = super().save_results(results, 'json', custom_filename)

        # Step 2: å¦‚æœéœ€è¦ HDF5ï¼Œä½¿ç”¨ handler ä¿å­˜
        if output_format in ['hdf5', 'both']:
            # æå–è¡›æ˜Ÿåæ¨™æ•¸æ“š
            satellite_coords = self._extract_coordinate_arrays(results)

            # ä½¿ç”¨ HDF5 handler ä¿å­˜
            hdf5_identifier = custom_filename or Path(json_path).stem
            hdf5_path = self.hdf5_handler.save(
                data=satellite_coords,
                identifier=hdf5_identifier,
                metadata={
                    'stage': 2,
                    'timestamp': results['metadata']['processing_start_time'],
                    'satellite_count': len(satellite_coords)
                }
            )

            if output_format == 'hdf5':
                return str(hdf5_path)
            else:  # 'both'
                return json_path, str(hdf5_path)

        return json_path
```

**ä»£ç¢¼æ¸›å°‘**: ~30è¡Œï¼ˆHDF5 æ“ä½œé‚è¼¯ç§»è‡³ handlerï¼‰

---

### 3. Stage 3 é·ç§»

**æª”æ¡ˆ**: `src/stages/stage3_coordinate_transformation/stage3_hdf5_cache.py`

**ä¿®æ”¹å‰**:
```python
class Stage3HDF5Cache:
    """ç´„ 80 è¡Œè‡ªå®šç¾© HDF5 ç·©å­˜é‚è¼¯"""

    def is_cache_valid(self, stage2_timestamp):
        # è‡ªå®šç¾©æª¢æŸ¥é‚è¼¯...
        pass

    def load_from_cache(self, satellite_id):
        # è‡ªå®šç¾©è®€å–é‚è¼¯...
        with h5py.File(...) as f:
            return f[satellite_id][:]

    def save_to_cache(self, satellite_data_dict):
        # è‡ªå®šç¾©å¯«å…¥é‚è¼¯...
        with h5py.File(...) as f:
            for sat_id, data in satellite_data_dict.items():
                f.create_dataset(sat_id, data=data, compression='gzip')
```

**ä¿®æ”¹å¾Œ**:
```python
from shared.hdf5_handler import HDF5CacheStrategy

class Stage3HDF5Cache:
    """ç°¡åŒ–ç‚º HDF5CacheStrategy çš„è–„åŒ…è£"""

    def __init__(self, cache_dir='data/cache/stage3', logger_instance=None):
        self.cache_strategy = HDF5CacheStrategy(
            base_dir=cache_dir,
            compression='gzip',
            compression_opts=4,
            logger_instance=logger_instance
        )
        self.cache_identifier = 'coordinate_cache'

    def is_cache_valid(self, stage2_timestamp: float) -> bool:
        """æª¢æŸ¥ç·©å­˜æ˜¯å¦æœ‰æ•ˆï¼ˆå§”è¨—çµ¦ strategyï¼‰"""
        return self.cache_strategy.is_valid(
            self.cache_identifier,
            stage2_timestamp
        )

    def load_from_cache(self, satellite_id: str) -> np.ndarray:
        """å¾ç·©å­˜è®€å–å–®å€‹è¡›æ˜Ÿæ•¸æ“š"""
        return self.cache_strategy.load(
            self.cache_identifier,
            dataset_name=satellite_id
        )

    def save_to_cache(self, satellite_data_dict: Dict[str, np.ndarray]) -> Path:
        """ä¿å­˜æ‰€æœ‰è¡›æ˜Ÿæ•¸æ“šåˆ°ç·©å­˜"""
        return self.cache_strategy.save(
            data=satellite_data_dict,
            identifier=self.cache_identifier,
            metadata={'cache_version': '1.0'}
        )

    def invalidate_cache(self) -> bool:
        """æ¸…é™¤ç·©å­˜"""
        return self.cache_strategy.invalidate(self.cache_identifier)
```

**ä»£ç¢¼æ¸›å°‘**: ~50è¡Œï¼ˆHDF5 æ“ä½œé‚è¼¯ç§»è‡³ strategyï¼‰

---

## ğŸ“‹ å¯¦æ–½è¨ˆåŠƒ

### éšæ®µåŠƒåˆ†

| éšæ®µ | ä»»å‹™ | é ä¼°æ™‚é–“ | ä¾è³´ |
|-----|------|---------|------|
| **Step 1** | å‰µå»º `src/shared/hdf5_handler.py` | 1.5å°æ™‚ | ç„¡ |
| **Step 2** | ç·¨å¯« HDF5 handler å–®å…ƒæ¸¬è©¦ | 1å°æ™‚ | Step 1 |
| **Step 3** | é·ç§» Stage 2 ä½¿ç”¨ HDF5OutputStrategy | 0.5å°æ™‚ | Step 1 |
| **Step 4** | é·ç§» Stage 3 ä½¿ç”¨ HDF5CacheStrategy | 0.5å°æ™‚ | Step 1 |
| **Step 5** | åŸ·è¡Œå…¨é‡æ¸¬è©¦é©—è­‰æ€§èƒ½å’Œç›¸å®¹æ€§ | 0.5å°æ™‚ | Step 3-4 |

**ç¸½è¨ˆ**: 4å°æ™‚ï¼ˆç´„ 0.5å¤©ï¼‰

---

### æ¸¬è©¦ç­–ç•¥

**å–®å…ƒæ¸¬è©¦**: `tests/unit/shared/test_hdf5_handler.py`

```python
import pytest
import numpy as np
from pathlib import Path
from src.shared.hdf5_handler import (
    HDF5OutputStrategy,
    HDF5CacheStrategy
)


def test_output_strategy_save_load(tmp_path):
    """æ¸¬è©¦ HDF5OutputStrategy ä¿å­˜å’Œè®€å–"""
    handler = HDF5OutputStrategy(base_dir=tmp_path)

    # æº–å‚™æ¸¬è©¦æ•¸æ“š
    test_data = {
        'SAT-12345': np.array([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]),
        'SAT-67890': np.array([[7.0, 8.0, 9.0]])
    }

    # ä¿å­˜
    hdf5_path = handler.save(
        data=test_data,
        identifier='test_output',
        metadata={'stage': 2, 'count': 2}
    )

    assert hdf5_path.exists()

    # è®€å–æ‰€æœ‰æ•¸æ“š
    loaded_data = handler.load('test_output')
    assert 'SAT-12345' in loaded_data
    np.testing.assert_array_equal(loaded_data['SAT-12345'], test_data['SAT-12345'])

    # è®€å–å–®å€‹æ•¸æ“šé›†
    sat_data = handler.load('test_output', 'SAT-12345')
    np.testing.assert_array_equal(sat_data, test_data['SAT-12345'])


def test_cache_strategy_validation(tmp_path):
    """æ¸¬è©¦ HDF5CacheStrategy ç·©å­˜é©—è­‰"""
    cache = HDF5CacheStrategy(base_dir=tmp_path)

    import time
    current_time = time.time()

    # ç·©å­˜ä¸å­˜åœ¨ï¼Œæ‡‰è©²ç„¡æ•ˆ
    assert not cache.is_valid('test_cache', current_time)

    # ä¿å­˜ç·©å­˜
    test_data = {'SAT-123': np.array([1, 2, 3])}
    cache.save(test_data, 'test_cache', metadata={'timestamp': current_time})

    # ç·©å­˜æ‡‰è©²æœ‰æ•ˆï¼ˆç·©å­˜æ™‚é–“ > upstreamæ™‚é–“ï¼‰
    assert cache.is_valid('test_cache', current_time - 10)

    # å¦‚æœ upstream æ›´æ–°ï¼Œç·©å­˜æ‡‰è©²ç„¡æ•ˆ
    future_time = time.time() + 100
    assert not cache.is_valid('test_cache', future_time)


def test_cache_invalidation(tmp_path):
    """æ¸¬è©¦ç·©å­˜æ¸…é™¤"""
    cache = HDF5CacheStrategy(base_dir=tmp_path)

    # ä¿å­˜ç·©å­˜
    test_data = {'SAT-123': np.array([1, 2, 3])}
    cache.save(test_data, 'test_cache')

    assert cache.exists('test_cache')

    # æ¸…é™¤ç·©å­˜
    assert cache.invalidate('test_cache')
    assert not cache.exists('test_cache')
```

**é›†æˆæ¸¬è©¦**: é©—è­‰ Stage 2/3 å¯¦éš›ä½¿ç”¨

```bash
# æ¸¬è©¦ Stage 2 é›™æ ¼å¼è¼¸å‡º
./run.sh --stage 2
ls data/outputs/stage2/*.json  # JSON æ‡‰è©²å­˜åœ¨
ls data/outputs/stage2/*.h5    # HDF5 æ‡‰è©²å­˜åœ¨ï¼ˆå¦‚æœé…ç½® output_format='both'ï¼‰

# æ¸¬è©¦ Stage 3 ç·©å­˜åŠ é€Ÿ
rm -rf data/cache/stage3/       # æ¸…é™¤ç·©å­˜
time ./run.sh --stage 3         # ç¬¬ä¸€æ¬¡é‹è¡Œï¼ˆ~25åˆ†é˜ï¼‰
time ./run.sh --stage 3         # ç¬¬äºŒæ¬¡é‹è¡Œï¼ˆ~2åˆ†é˜ï¼Œä½¿ç”¨ç·©å­˜ï¼‰
```

---

## âœ… é©—æ”¶æ¨™æº–

### åŠŸèƒ½æ€§æ¨™æº–

- âœ… **HDF5 handler å®Œæ•´**: BaseHDF5Handler + 2ç¨®ç­–ç•¥å¯¦ä½œ
- âœ… **Stage 2 é·ç§»**: ä½¿ç”¨ HDF5OutputStrategyï¼Œä¿æŒé›™æ ¼å¼è¼¸å‡º
- âœ… **Stage 3 é·ç§»**: ä½¿ç”¨ HDF5CacheStrategyï¼Œä¿æŒç·©å­˜åŠ é€Ÿ
- âœ… **å‘å¾Œç›¸å®¹**: Stage 2/3 ç¾æœ‰æ¥å£ä¸è®Š
- âœ… **æ¸¬è©¦è¦†è“‹**: HDF5 handler å–®å…ƒæ¸¬è©¦è¦†è“‹ç‡ â‰¥ 80%

### æ€§èƒ½æ¨™æº–

- âœ… **Stage 2 è¼¸å‡ºæ€§èƒ½**: JSON + HDF5 ä¿å­˜æ™‚é–“ â‰¤ åŸå¯¦ä½œ +5%
- âœ… **Stage 3 ç·©å­˜åŠ é€Ÿ**: ç·©å­˜å‘½ä¸­æ™‚åŸ·è¡Œæ™‚é–“ â‰¤ 3åˆ†é˜ï¼ˆåŸ 2åˆ†é˜åŸºæº–ï¼‰
- âœ… **HDF5 å£“ç¸®ç‡**: gzip å£“ç¸®å¾Œæª”æ¡ˆå¤§å° â‰¤ åŸå¯¦ä½œ +10%

### ä»£ç¢¼è³ªé‡æ¨™æº–

- âœ… **ä»£ç¢¼æ¸›å°‘**: Stage 2/3 HDF5 æ“ä½œä»£ç¢¼æ¸›å°‘ ~50è¡Œ
- âœ… **é‡è¤‡æ¶ˆé™¤**: HDF5 read/write é‚è¼¯çµ±ä¸€æŠ½å–
- âœ… **æ–‡æª”å®Œæ•´**: æ‰€æœ‰ HDF5 strategy åŒ…å«ä½¿ç”¨ç¯„ä¾‹

---

## ğŸ“Š é æœŸæ”¶ç›Š

### é‡åŒ–æ”¶ç›Š

| ç¶­åº¦ | æ”¹é€²å‰ | æ”¹é€²å¾Œ | æå‡å¹…åº¦ |
|-----|-------|-------|---------|
| **HDF5 æ“ä½œä»£ç¢¼è¡Œæ•¸** | ~120è¡Œ | ~70è¡Œ | -42% |
| **ä»£ç¢¼é‡è¤‡** | 2è™•å¯¦ä½œ | çµ±ä¸€ç­–ç•¥ | -50% |
| **æ¸¬è©¦è¦†è“‹ç‡** | ~30% | â‰¥80% | +167% |
| **æ–°éšæ®µ HDF5 æ·»åŠ æ™‚é–“** | ~2å°æ™‚ | ~30åˆ†é˜ | -75% |

### è³ªåŒ–æ”¶ç›Š

1. **æ“´å±•æ€§**:
   - æœªä¾†æ–°éšæ®µéœ€è¦ HDF5 æ™‚ï¼Œç›´æ¥ä½¿ç”¨ç¾æœ‰ç­–ç•¥
   - å¯è¼•é¬†æ·»åŠ æ–°ç­–ç•¥ï¼ˆå¦‚ HDF5ReadOnlyStrategyï¼‰

2. **ç¶­è­·æ€§**:
   - HDF5 æ“ä½œé‚è¼¯é›†ä¸­ç®¡ç†
   - Bug ä¿®å¾©åªéœ€ä¿®æ”¹ä¸€è™•

3. **æ¸¬è©¦æ€§**:
   - çµ±ä¸€çš„ HDF5 æ¸¬è©¦æ¡†æ¶
   - Mock HDF5 æ“ä½œæ›´å®¹æ˜“

---

## âš ï¸ é¢¨éšªèˆ‡ç·©è§£

| é¢¨éšª | ç­‰ç´š | ç·©è§£æªæ–½ |
|-----|------|---------|
| **HDF5 é‡æ§‹å½±éŸ¿æ€§èƒ½** | ğŸŸ¢ ä½ | ä¿æŒåŸæœ‰å£“ç¸®åƒæ•¸ï¼ˆgzip level 4ï¼‰ |
| **ç·©å­˜ç­–ç•¥è®Šæ›´ç ´å£ç¾æœ‰ç·©å­˜** | ğŸŸ¡ ä¸­ | ä¿ç•™ Stage3HDF5Cache æ¥å£ï¼Œå…§éƒ¨å§”è¨—çµ¦ strategy |
| **HDF5 æª”æ¡ˆæ ¼å¼ä¸ç›¸å®¹** | ğŸŸ¢ ä½ | ä½¿ç”¨ç›¸åŒçš„ dataset å‘½åå’Œçµæ§‹ |

---

## ğŸ”— ç›¸é—œæ–‡ä»¶

- [00_OVERVIEW.md](00_OVERVIEW.md) - Phase 4 ç¸½è¦½
- [01_CURRENT_STATUS.md](01_CURRENT_STATUS.md) - ç•¶å‰æ¶æ§‹ç‹€æ…‹
- [Phase 3 Progress Report](../phase3_result_manager_refactor/PHASE3_PROGRESS_REPORT.md) - BaseResultManager è¨­è¨ˆ

---

**ä¸‹ä¸€æ­¥**: é–±è®€ [05_P2_TESTING_FRAMEWORK.md](05_P2_TESTING_FRAMEWORK.md) äº†è§£æ¸¬è©¦æ¡†æ¶å®Œå–„æ–¹æ¡ˆ
